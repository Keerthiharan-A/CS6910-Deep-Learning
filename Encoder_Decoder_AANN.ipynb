{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "QykNBnQ05Cgo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.utils import plot_model\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(path): # Function for preprocessing the data\n",
        "    Data = pd.read_csv(path, header=None)\n",
        "    columns = [f'feature_{i+1}' for i in range(Data.shape[1])]\n",
        "    Data.columns = columns\n",
        "    return Data"
      ],
      "metadata": {
        "id": "HNObrSRxcsJ0"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = preprocess(\"/content/drive/MyDrive/Colab Notebooks/CS6910_ASSIGNMENT1/testing_data_set_24.csv\")\n",
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLd79XYN5yMf",
        "outputId": "de047893-0cce-44e0-c6b6-cfd30f2c84f2"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 37)"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labelled_train_data = preprocess(\"/content/drive/MyDrive/Colab Notebooks/CS6910_ASSIGNMENT1/training_data_set_24_labeled.csv\")\n",
        "labelled_train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRQY_z7g6Bw5",
        "outputId": "b41fa04e-437d-498d-d51e-868fb0d5f820"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(750, 37)"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = preprocess(\"/content/drive/MyDrive/Colab Notebooks/CS6910_ASSIGNMENT1/validation_data_set_24.csv\")\n",
        "val_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHKLLWEa6hdq",
        "outputId": "7e914bc0-1f47-47ad-bbc3-bd65d0165b7d"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250, 37)"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#labelled_train_data = labelled_train_data.sample(frac = 1)\n",
        "#val_data = val_data.sample(frac = 1)"
      ],
      "metadata": {
        "id": "bg5FHBlkaI8h"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = labelled_train_data.iloc[:,:-1]\n",
        "Y_train = labelled_train_data.iloc[:,-1]\n",
        "X_val = val_data.iloc[:,:-1]\n",
        "Y_val = val_data.iloc[:,-1]\n",
        "X_test = test_data.iloc[:,:-1]\n",
        "Y_test = test_data.iloc[:,-1]\n",
        "Y_train.hist()\n",
        "plt.title(\"Training data labels\")\n",
        "plt.show()\n",
        "print()\n",
        "Y_val.hist()\n",
        "plt.title(\"Validation data labels\")\n",
        "plt.show()\n",
        "Y_test.hist()\n",
        "plt.title(\"Testing data labels\")\n",
        "plt.show()\n",
        "print()"
      ],
      "metadata": {
        "id": "46D1e2wc7IDY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d064b7a4-57cf-49f2-aa35-8f6f1d15b2a7"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGzCAYAAAAFROyYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1XklEQVR4nO3df3RU1aH+/2cShgkRhhgEkmgQRFQQJZYIDVhBCQSkYJB7EUGLaEUFLDGfYqFLIEEF5XoVwSjSWsBqKvUH2KKAEQRqBYQAFiiXgga0UpIikAApw5A53z9cmS9DIMzEc8gefL/WmrU4e/bZs3/MTh7OzGRclmVZAgAAMEhMfXcAAADgdAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQgSt17771q3bp1nc7Ny8uTy+Wyt0PfU8+ePdWzZ8/67kbQnj175HK5NH/+/IjPrZ7fAwcO2Naf77PeQDQioAA2c7lcYd1WrVpV3129IFRWViovL4/5BC4wDeq7A8CF5ve//33I8WuvvaaioqIa5e3bt/9ej/Ob3/xGgUCgTuc+/vjjmjBhwvd6fFNUVlYqPz9fkoy6AgPg+yGgADa7++67Q47XrVunoqKiGuWnq6ysVHx8fNiP43a769Q/SWrQoIEaNGD7AzAXL/EA9aBnz57q2LGjiouLdfPNNys+Pl6//vWvJUnvvfee+vfvr5SUFHk8HrVt21ZPPPGEqqqqQto4/T0J1e+ZePbZZzV37ly1bdtWHo9HN954ozZs2BBy7pneg+JyuTR27FgtXrxYHTt2lMfj0bXXXqtly5bV6P+qVauUnp6uuLg4tW3bVq+88kpE72up7l+jRo3UpUsX/eUvf6lR58SJE5o8ebI6d+6spk2b6qKLLtJPfvITffzxxyFjbt68uSQpPz8/+PJZXl6eJOlvf/ub7r33Xl1xxRWKi4tTUlKS7rvvPn377bdh9fN0kbZ34MABDRkyRF6vV82aNdO4ceN0/PjxGvVef/11de7cWY0aNVJiYqKGDh2qr7/++pz9efPNN9W5c2c1adJEXq9X1113nV544YU6jQ0wDf+FAurJt99+q379+mno0KG6++671bJlS0nS/Pnz1bhxY+Xm5qpx48ZauXKlJk+erIqKCv3P//zPOdstLCzUkSNH9OCDD8rlcmnGjBm644479OWXX57zqssnn3yid999V6NHj1aTJk00a9YsDR48WF999ZWaNWsmSdq8ebP69u2r5ORk5efnq6qqSlOnTg0GhXN59dVX9eCDD6pbt27KycnRl19+qYEDByoxMVGpqanBehUVFfrtb3+ru+66Sw888ICOHDmiV199VVlZWfrss8+Ulpam5s2b6+WXX9bDDz+sQYMG6Y477pAkXX/99ZKkoqIiffnllxo5cqSSkpK0fft2zZ07V9u3b9e6desifqNwpO0NGTJErVu31vTp07Vu3TrNmjVLhw4d0muvvRas89RTT2nSpEkaMmSIfv7zn+vf//63Zs+erZtvvlmbN29WQkLCWfty1113qVevXnrmmWckSTt27NBf//pXjRs3LqJxAUayADhqzJgx1ulbrUePHpYka86cOTXqV1ZW1ih78MEHrfj4eOv48ePBshEjRliXX3558LikpMSSZDVr1sw6ePBgsPy9996zJFl//vOfg2VTpkyp0SdJVsOGDa3du3cHyz7//HNLkjV79uxg2YABA6z4+Hjrm2++CZbt2rXLatCgQY02T3fixAmrRYsWVlpamuXz+YLlc+fOtSRZPXr0CJadPHkypI5lWdahQ4esli1bWvfdd1+w7N///rclyZoyZUqNxzvTXP7hD3+wJFlr1qypta/V8zlv3ryI26ue34EDB4bUHT16tCXJ+vzzzy3Lsqw9e/ZYsbGx1lNPPRVSb+vWrVaDBg1Cyk9f73Hjxller9c6efJkreMAohUv8QD1xOPxaOTIkTXKGzVqFPz3kSNHdODAAf3kJz9RZWWl/u///u+c7d555526+OKLg8c/+clPJElffvnlOc/NzMxU27Ztg8fXX3+9vF5v8Nyqqip99NFHys7OVkpKSrDelVdeqX79+p2z/Y0bN6qsrEwPPfSQGjZsGCy/99571bRp05C6sbGxwTqBQEAHDx7UyZMnlZ6erk2bNp3zsaTQuTx+/LgOHDigH//4x5IUdhvfp70xY8aEHD/yyCOSpA8++ECS9O677yoQCGjIkCE6cOBA8JaUlKR27dqFvJx1uoSEBB07dkxFRUURjwOIBgQUoJ5ceumlIb+kq23fvl2DBg1S06ZN5fV61bx58+AbbMvLy8/ZbqtWrUKOq8PKoUOHIj63+vzqc8vKyvSf//xHV155ZY16Zyo73d69eyVJ7dq1Cyl3u9264ooratRfsGCBrr/+esXFxalZs2Zq3ry53n///bDmQZIOHjyocePGqWXLlmrUqJGaN2+uNm3aSApvLr9ve6ePs23btoqJidGePXskSbt27ZJlWWrXrp2aN28ectuxY4fKysrO2pfRo0frqquuUr9+/XTZZZfpvvvuO+P7hYBoxXtQgHpy6v/Gqx0+fFg9evSQ1+vV1KlT1bZtW8XFxWnTpk361a9+FdbHimNjY89YblmWo+fa7fXXX9e9996r7OxsjR8/Xi1atFBsbKymT5+uL774Iqw2hgwZok8//VTjx49XWlqaGjdurEAgoL59+9bpI9rft73T36MSCATkcrm0dOnSM85948aNz9pWixYttGXLFi1fvlxLly7V0qVLNW/ePP3sZz/TggULIh4bYBoCCmCQVatW6dtvv9W7776rm2++OVheUlJSj736/7Vo0UJxcXHavXt3jfvOVHa6yy+/XNJ3Vw5uvfXWYLnf71dJSYk6deoULHv77bd1xRVX6N133w35xT5lypSQNs/2RtdDhw5pxYoVys/P1+TJk4Plu3btOmc/7Wpv165dwSss0ndzFAgEgp++atu2rSzLUps2bXTVVVdF3KeGDRtqwIABGjBggAKBgEaPHq1XXnlFkyZNCuuKFmAyXuIBDFL9v+hTr1icOHFCL730Un11KURsbKwyMzO1ePFi7du3L1i+e/duLV269Jznp6enq3nz5pozZ45OnDgRLJ8/f74OHz5c47Gk0LlYv3691q5dG1Kv+m/HhHO+JM2cOfOc/TyTurRXUFAQcjx79mxJCr5f54477lBsbKzy8/NrtGtZVq0fhz79vpiYmOCnl3w+Xy0jAaIDV1AAg3Tr1k0XX3yxRowYoV/84hdyuVz6/e9/Xy8vsZxNXl6ePvzwQ3Xv3l0PP/ywqqqq9OKLL6pjx47asmVLree63W49+eSTevDBB3XrrbfqzjvvVElJiebNm1fjPSg//elP9e6772rQoEHq37+/SkpKNGfOHHXo0EFHjx4N1mvUqJE6dOighQsX6qqrrlJiYqI6duyojh076uabb9aMGTPk9/t16aWX6sMPP6zz1Siv1xtxeyUlJRo4cKD69u2rtWvX6vXXX9ewYcOCV4ratm2rJ598UhMnTtSePXuUnZ2tJk2aqKSkRIsWLdKoUaP0y1/+8oxt//znP9fBgwd166236rLLLtPevXs1e/ZspaWlfe+/UgyYgCsogEGaNWumJUuWKDk5WY8//rieffZZ9e7dWzNmzKjvrgV17txZS5cu1cUXX6xJkybp1Vdf1dSpU9WrVy/FxcWd8/xRo0bppZde0r59+zR+/Hj95S9/0Z/+9KeQv4EifffJnmnTpunzzz/XL37xCy1fvlyvv/660tPTa7T529/+VpdeeqkeffRR3XXXXXr77bclffc3YbKyslRQUKCJEyfK7XaHdaXnbCJtb+HChfJ4PJowYYLef/99jR07Vq+++mpInQkTJuidd95RTEyM8vPz9ctf/lJ/+tOf1KdPHw0cOPCsbd99992Ki4vTSy+9pNGjR2vBggW68847tXTpUsXE8KMd0c9lmfRfMwBRKzs7W9u3b6/zezwA4FTEbAAR+89//hNyvGvXLn3wwQd8WR8A23AFBUDEkpOTg99Js3fvXr388svy+XzavHlzjb/9AQB1wZtkAUSsb9+++sMf/qD9+/fL4/EoIyND06ZNI5wAsA1XUAAAgHF4DwoAADAOAQUAABgnKt+DEggEtG/fPjVp0uSsf+YaAACYxbIsHTlyRCkpKef8ez1RGVD27dtX4486AQCA6PD111/rsssuq7VOVAaUJk2aSPpugF6v19a2/X6/PvzwQ/Xp00dut9vWtk3A+KLfhT5Gxhf9LvQxXujjk5wbY0VFhVJTU4O/x2sTlQGl+mUdr9frSECJj4+X1+u9IJ94jC/6XehjZHzR70If44U+Psn5MYbz9gzeJAsAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgnAb13QFTdcxbLl/Vub8O2hR7nu5f312IWOsJ79vepifW0owuzq1fNM5ztGIPOs+JPSg5uw+Z5/Ojeg3rE1dQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYJyIA8qaNWs0YMAApaSkyOVyafHixWet+9BDD8nlcmnmzJkh5QcPHtTw4cPl9XqVkJCg+++/X0ePHo20KwAA4AIVcUA5duyYOnXqpIKCglrrLVq0SOvWrVNKSkqN+4YPH67t27erqKhIS5Ys0Zo1azRq1KhIuwIAAC5QDSI9oV+/furXr1+tdb755hs98sgjWr58ufr37x9y344dO7Rs2TJt2LBB6enpkqTZs2frtttu07PPPnvGQAMAAH5YIg4o5xIIBHTPPfdo/Pjxuvbaa2vcv3btWiUkJATDiSRlZmYqJiZG69ev16BBg2qc4/P55PP5gscVFRWSJL/fL7/fb2v/q9vzxFi2tuu0cOehup7d81YXnlj757h63ZxaPxPmzaQ1dAJ78PxxYg9Kzu5DE+Yt0jV0ap6dVL12Tv2ODYfLsqw6z5zL5dKiRYuUnZ0dLJs+fbo+/vhjLV++XC6XS61bt1ZOTo5ycnIkSdOmTdOCBQu0c+fOkLZatGih/Px8PfzwwzUeJy8vT/n5+TXKCwsLFR8fX9fuAwCA86iyslLDhg1TeXm5vF5vrXVtvYJSXFysF154QZs2bZLL5bKt3YkTJyo3Nzd4XFFRodTUVPXp0+ecA4yU3+9XUVGRJm2MkS9g3xicti0vK6x61ePr3bu33G63w72qXce85ba36Ymx9ER6wLH1C3eenWTSGjqBPXj+OLEHJWf3YTTuQafm2UnVa2j387T6FZBw2BpQ/vKXv6isrEytWrUKllVVVen//b//p5kzZ2rPnj1KSkpSWVlZyHknT57UwYMHlZSUdMZ2PR6PPB5PjXK32+3YBvcFXPJVRc8Px0jnwcm5C5eT8+vU+tX3nJ3KhDV0EnvQeU7PrxNrWN9zdqpw1zCansens/t5GklbtgaUe+65R5mZmSFlWVlZuueeezRy5EhJUkZGhg4fPqzi4mJ17txZkrRy5UoFAgF17drVzu4AAIAoFXFAOXr0qHbv3h08Likp0ZYtW5SYmKhWrVqpWbNmIfXdbreSkpJ09dVXS5Lat2+vvn376oEHHtCcOXPk9/s1duxYDR06lE/wAAAASXX4OygbN27UDTfcoBtuuEGSlJubqxtuuEGTJ08Ou4033nhD11xzjXr16qXbbrtNN910k+bOnRtpVwAAwAUq4isoPXv2VCQf/NmzZ0+NssTERBUWFkb60AAA4AeC7+IBAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgRB5Q1a9ZowIABSklJkcvl0uLFi4P3+f1+/epXv9J1112niy66SCkpKfrZz36mffv2hbRx8OBBDR8+XF6vVwkJCbr//vt19OjR7z0YAABwYYg4oBw7dkydOnVSQUFBjfsqKyu1adMmTZo0SZs2bdK7776rnTt3auDAgSH1hg8fru3bt6uoqEhLlizRmjVrNGrUqLqPAgAAXFAaRHpCv3791K9fvzPe17RpUxUVFYWUvfjii+rSpYu++uortWrVSjt27NCyZcu0YcMGpaenS5Jmz56t2267Tc8++6xSUlLqMAwAAHAhiTigRKq8vFwul0sJCQmSpLVr1yohISEYTiQpMzNTMTExWr9+vQYNGlSjDZ/PJ5/PFzyuqKiQ9N1LSn6/39b+VrfnibFsbddp4c5DdT27560uPLH2z3H1ujm1fibMm0lr6AT24PnjxB6UnN2HJsxbpGvo1Dw7qXrtnPodGw6XZVl1njmXy6VFixYpOzv7jPcfP35c3bt31zXXXKM33nhDkjRt2jQtWLBAO3fuDKnbokUL5efn6+GHH67RTl5envLz82uUFxYWKj4+vq7dBwAA51FlZaWGDRum8vJyeb3eWus6dgXF7/dryJAhsixLL7/88vdqa+LEicrNzQ0eV1RUKDU1VX369DnnACPl9/tVVFSkSRtj5Au4bG3bSdvyssKqVz2+3r17y+12O9yr2nXMW257m54YS0+kBxxbv3Dn2UkmraET2IPnjxN7UHJ2H0bjHnRqnp1UvYZ2P0+rXwEJhyMBpTqc7N27VytXrgwJEUlJSSorKwupf/LkSR08eFBJSUlnbM/j8cjj8dQod7vdjm1wX8AlX1X0/HCMdB6cnLtwOTm/Tq1ffc/ZqUxYQyexB53n9Pw6sYb1PWenCncNo+l5fDq7n6eRtGX730GpDie7du3SRx99pGbNmoXcn5GRocOHD6u4uDhYtnLlSgUCAXXt2tXu7gAAgCgU8RWUo0ePavfu3cHjkpISbdmyRYmJiUpOTtZ//dd/adOmTVqyZImqqqq0f/9+SVJiYqIaNmyo9u3bq2/fvnrggQc0Z84c+f1+jR07VkOHDuUTPAAAQFIdAsrGjRt1yy23BI+r3xsyYsQI5eXl6U9/+pMkKS0tLeS8jz/+WD179pQkvfHGGxo7dqx69eqlmJgYDR48WLNmzarjEAAAwIUm4oDSs2dP1fbBn3A+FJSYmKjCwsJIHxoAAPxA8F08AADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABgn4oCyZs0aDRgwQCkpKXK5XFq8eHHI/ZZlafLkyUpOTlajRo2UmZmpXbt2hdQ5ePCghg8fLq/Xq4SEBN1///06evTo9xoIAAC4cEQcUI4dO6ZOnTqpoKDgjPfPmDFDs2bN0pw5c7R+/XpddNFFysrK0vHjx4N1hg8fru3bt6uoqEhLlizRmjVrNGrUqLqPAgAAXFAaRHpCv3791K9fvzPeZ1mWZs6cqccff1y33367JOm1115Ty5YttXjxYg0dOlQ7duzQsmXLtGHDBqWnp0uSZs+erdtuu03PPvusUlJSvsdwAADAhSDigFKbkpIS7d+/X5mZmcGypk2bqmvXrlq7dq2GDh2qtWvXKiEhIRhOJCkzM1MxMTFav369Bg0aVKNdn88nn88XPK6oqJAk+f1++f1+O4cQbM8TY9nartPCnYfqenbPW114Yu2f4+p1c2r9TJg3k9bQCezB88eJPSg5uw9NmLdI19CpeXZS9do59Ts2HC7Lsuo8cy6XS4sWLVJ2drYk6dNPP1X37t21b98+JScnB+sNGTJELpdLCxcu1LRp07RgwQLt3LkzpK0WLVooPz9fDz/8cI3HycvLU35+fo3ywsJCxcfH17X7AADgPKqsrNSwYcNUXl4ur9dba11br6A4ZeLEicrNzQ0eV1RUKDU1VX369DnnACPl9/tVVFSkSRtj5Au4bG3bSdvyssKqVz2+3r17y+12O9yr2nXMW257m54YS0+kBxxbv3Dn2UkmraET2IPnjxN7UHJ2H0bjHnRqnp1UvYZ2P0+rXwEJh60BJSkpSZJUWloacgWltLRUaWlpwTplZWUh5508eVIHDx4Mnn86j8cjj8dTo9ztdju2wX0Bl3xV0fPDMdJ5cHLuwuXk/Dq1fvU9Z6cyYQ2dxB50ntPz68Qa1vecnSrcNYym5/Hp7H6eRtKWrX8HpU2bNkpKStKKFSuCZRUVFVq/fr0yMjIkSRkZGTp8+LCKi4uDdVauXKlAIKCuXbva2R0AABClIr6CcvToUe3evTt4XFJSoi1btigxMVGtWrVSTk6OnnzySbVr105t2rTRpEmTlJKSEnyfSvv27dW3b1898MADmjNnjvx+v8aOHauhQ4fyCR4AACCpDgFl48aNuuWWW4LH1e8NGTFihObPn6/HHntMx44d06hRo3T48GHddNNNWrZsmeLi4oLnvPHGGxo7dqx69eqlmJgYDR48WLNmzbJhOAAA4EIQcUDp2bOnavvgj8vl0tSpUzV16tSz1klMTFRhYWGkDw0AAH4g+C4eAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxje0CpqqrSpEmT1KZNGzVq1Eht27bVE088IcuygnUsy9LkyZOVnJysRo0aKTMzU7t27bK7KwAAIErZHlCeeeYZvfzyy3rxxRe1Y8cOPfPMM5oxY4Zmz54drDNjxgzNmjVLc+bM0fr163XRRRcpKytLx48ft7s7AAAgCjWwu8FPP/1Ut99+u/r37y9Jat26tf7whz/os88+k/Td1ZOZM2fq8ccf1+233y5Jeu2119SyZUstXrxYQ4cOtbtLAAAgytgeULp166a5c+fqH//4h6666ip9/vnn+uSTT/Tcc89JkkpKSrR//35lZmYGz2natKm6du2qtWvXnjGg+Hw++Xy+4HFFRYUkye/3y+/329r/6vY8MdY5apol3Hmormf3vNWFJ9b+Oa5eN6fWz4R5M2kNncAePH+c2IOSs/vQhHmLdA2dmmcnVa+dU79jw+GyTn1ziA0CgYB+/etfa8aMGYqNjVVVVZWeeuopTZw4UdJ3V1i6d++uffv2KTk5OXjekCFD5HK5tHDhwhpt5uXlKT8/v0Z5YWGh4uPj7ew+AABwSGVlpYYNG6by8nJ5vd5a69p+BeWPf/yj3njjDRUWFuraa6/Vli1blJOTo5SUFI0YMaJObU6cOFG5ubnB44qKCqWmpqpPnz7nHGCk/H6/ioqKNGljjHwBl61tO2lbXlZY9arH17t3b7ndbod7VbuOecttb9MTY+mJ9IBj6xfuPDvJpDV0Anvw/HFiD0rO7sNo3INOzbOTqtfQ7udp9Ssg4bA9oIwfP14TJkwIvlRz3XXXae/evZo+fbpGjBihpKQkSVJpaWnIFZTS0lKlpaWdsU2PxyOPx1Oj3O12O7bBfQGXfFXR88Mx0nlwcu7C5eT8OrV+9T1npzJhDZ3EHnSe0/PrxBrW95ydKtw1jKbn8ensfp5G0pbtn+KprKxUTExos7GxsQoEApKkNm3aKCkpSStWrAjeX1FRofXr1ysjI8Pu7gAAgChk+xWUAQMG6KmnnlKrVq107bXXavPmzXruued03333SZJcLpdycnL05JNPql27dmrTpo0mTZqklJQUZWdn290dAAAQhWwPKLNnz9akSZM0evRolZWVKSUlRQ8++KAmT54crPPYY4/p2LFjGjVqlA4fPqybbrpJy5YtU1xcnN3dAQAAUcj2gNKkSRPNnDlTM2fOPGsdl8ulqVOnaurUqXY/PAAAuADwXTwAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGMeRgPLNN9/o7rvvVrNmzdSoUSNdd9112rhxY/B+y7I0efJkJScnq1GjRsrMzNSuXbuc6AoAAIhCtgeUQ4cOqXv37nK73Vq6dKn+/ve/63//93918cUXB+vMmDFDs2bN0pw5c7R+/XpddNFFysrK0vHjx+3uDgAAiEIN7G7wmWeeUWpqqubNmxcsa9OmTfDflmVp5syZevzxx3X77bdLkl577TW1bNlSixcv1tChQ2u06fP55PP5gscVFRWSJL/fL7/fb2v/q9vzxFi2tuu0cOehup7d81YXnlj757h63ZxaPxPmzaQ1dAJ78PxxYg9Kzu5DE+Yt0jV0ap6dVL12Tv2ODYfLsixbZ65Dhw7KysrSP//5T61evVqXXnqpRo8erQceeECS9OWXX6pt27bavHmz0tLSguf16NFDaWlpeuGFF2q0mZeXp/z8/BrlhYWFio+Pt7P7AADAIZWVlRo2bJjKy8vl9XprrWt7QImLi5Mk5ebm6r//+7+1YcMGjRs3TnPmzNGIESP06aefqnv37tq3b5+Sk5OD5w0ZMkQul0sLFy6s0eaZrqCkpqbqwIED5xxgpPx+v4qKijRpY4x8AZetbTtpW15WWPWqx9e7d2+53W6He1W7jnnLbW/TE2PpifSAY+sX7jw7yaQ1dAJ78PxxYg9Kzu7DaNyDTs2zk6rX0O7naUVFhS655JKwAortL/EEAgGlp6dr2rRpkqQbbrhB27ZtCwaUuvB4PPJ4PDXK3W63YxvcF3DJVxU9PxwjnQcn5y5cTs6vU+tX33N2KhPW0EnsQec5Pb9OrGF9z9mpwl3DaHoen87u52kkbdn+Jtnk5GR16NAhpKx9+/b66quvJElJSUmSpNLS0pA6paWlwfsAAMAPm+0BpXv37tq5c2dI2T/+8Q9dfvnlkr57w2xSUpJWrFgRvL+iokLr169XRkaG3d0BAABRyPaXeB599FF169ZN06ZN05AhQ/TZZ59p7ty5mjt3riTJ5XIpJydHTz75pNq1a6c2bdpo0qRJSklJUXZ2tt3dAQAAUcj2gHLjjTdq0aJFmjhxoqZOnao2bdpo5syZGj58eLDOY489pmPHjmnUqFE6fPiwbrrpJi1btiz4BlsAAPDDZntAkaSf/vSn+ulPf3rW+10ul6ZOnaqpU6c68fAAACDK8V08AADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABjH8YDy9NNPy+VyKScnJ1h2/PhxjRkzRs2aNVPjxo01ePBglZaWOt0VAAAQJRwNKBs2bNArr7yi66+/PqT80Ucf1Z///Ge99dZbWr16tfbt26c77rjDya4AAIAo0sCpho8eParhw4frN7/5jZ588slgeXl5uV599VUVFhbq1ltvlSTNmzdP7du317p16/TjH/+4Rls+n08+ny94XFFRIUny+/3y+/229ru6PU+MZWu7Tgt3Hqrr2T1vdeGJtX+Oq9fNqfUzYd5MWkMnsAfPHyf2oOTsPjRh3iJdQ6fm2UnVa+fU79hwuCzLcmTmRowYocTERD3//PPq2bOn0tLSNHPmTK1cuVK9evXSoUOHlJCQEKx/+eWXKycnR48++miNtvLy8pSfn1+jvLCwUPHx8U50HwAA2KyyslLDhg1TeXm5vF5vrXUduYLy5ptvatOmTdqwYUON+/bv36+GDRuGhBNJatmypfbv33/G9iZOnKjc3NzgcUVFhVJTU9WnT59zDjBSfr9fRUVFmrQxRr6Ay9a2nbQtLyusetXj6927t9xut8O9ql3HvOW2t+mJsfREesCx9Qt3np1k0ho6gT14/jixByVn92E07kGn5tlJ1Wto9/O0+hWQcNgeUL7++muNGzdORUVFiouLs6VNj8cjj8dTo9ztdju2wX0Bl3xV0fPDMdJ5cHLuwuXk/Dq1fvU9Z6cyYQ2dxB50ntPz68Qa1vecnSrcNYym5/Hp7H6eRtKW7W+SLS4uVllZmX70ox+pQYMGatCggVavXq1Zs2apQYMGatmypU6cOKHDhw+HnFdaWqqkpCS7uwMAAKKQ7VdQevXqpa1bt4aUjRw5Utdcc41+9atfKTU1VW63WytWrNDgwYMlSTt37tRXX32ljIwMu7sDAACikO0BpUmTJurYsWNI2UUXXaRmzZoFy++//37l5uYqMTFRXq9XjzzyiDIyMs74CR4AAPDD49jHjGvz/PPPKyYmRoMHD5bP51NWVpZeeuml+ugKAAAw0HkJKKtWrQo5jouLU0FBgQoKCs7HwwMAgCjDd/EAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDi2B5Tp06frxhtvVJMmTdSiRQtlZ2dr586dIXWOHz+uMWPGqFmzZmrcuLEGDx6s0tJSu7sCAACilO0BZfXq1RozZozWrVunoqIi+f1+9enTR8eOHQvWefTRR/XnP/9Zb731llavXq19+/bpjjvusLsrAAAgSjWwu8Fly5aFHM+fP18tWrRQcXGxbr75ZpWXl+vVV19VYWGhbr31VknSvHnz1L59e61bt04//vGP7e4SAACIMrYHlNOVl5dLkhITEyVJxcXF8vv9yszMDNa55ppr1KpVK61du/aMAcXn88nn8wWPKyoqJEl+v19+v9/W/la354mxbG3XaeHOQ3U9u+etLjyx9s9x9bo5tX4mzJtJa+gE9uD548QelJzdhybMW6Rr6NQ8O6l67Zz6HRsOl2VZjs1cIBDQwIEDdfjwYX3yySeSpMLCQo0cOTIkcEhSly5ddMstt+iZZ56p0U5eXp7y8/NrlBcWFio+Pt6ZzgMAAFtVVlZq2LBhKi8vl9frrbWuo1dQxowZo23btgXDSV1NnDhRubm5weOKigqlpqaqT58+5xxgpPx+v4qKijRpY4x8AZetbTtpW15WWPWqx9e7d2+53W6He1W7jnnLbW/TE2PpifSAY+sX7jw7yaQ1dAJ78PxxYg9Kzu7DaNyDTs2zk6rX0O7nafUrIOFwLKCMHTtWS5Ys0Zo1a3TZZZcFy5OSknTixAkdPnxYCQkJwfLS0lIlJSWdsS2PxyOPx1Oj3O12O7bBfQGXfFXR88Mx0nlwcu7C5eT8OrV+9T1npzJhDZ3EHnSe0/PrxBrW95ydKtw1jKbn8ensfp5G0pbtn+KxLEtjx47VokWLtHLlSrVp0ybk/s6dO8vtdmvFihXBsp07d+qrr75SRkaG3d0BAABRyPYrKGPGjFFhYaHee+89NWnSRPv375ckNW3aVI0aNVLTpk11//33Kzc3V4mJifJ6vXrkkUeUkZHBJ3gAAIAkBwLKyy+/LEnq2bNnSPm8efN07733SpKef/55xcTEaPDgwfL5fMrKytJLL71kd1cAAECUsj2ghPOhoLi4OBUUFKigoMDuhwcAABcAvosHAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOPUa0ApKChQ69atFRcXp65du+qzzz6rz+4AAABD1FtAWbhwoXJzczVlyhRt2rRJnTp1UlZWlsrKyuqrSwAAwBD1FlCee+45PfDAAxo5cqQ6dOigOXPmKD4+Xr/73e/qq0sAAMAQDerjQU+cOKHi4mJNnDgxWBYTE6PMzEytXbu2Rn2fzyefzxc8Li8vlyQdPHhQfr/f1r75/X5VVlaqgT9GVQGXrW076dtvvw2rXvX4vv32W7ndbod7VbsGJ4/Z32bAUmVlwLH1C3eenWTSGjqBPXj+OLEHJWf3YTTuQafm2UnVa2j38/TIkSOSJMuyzl3ZqgfffPONJcn69NNPQ8rHjx9vdenSpUb9KVOmWJK4cePGjRs3bhfA7euvvz5nVqiXKyiRmjhxonJzc4PHgUBABw8eVLNmzeRy2ZvOKyoqlJqaqq+//lper9fWtk3A+KLfhT5Gxhf9LvQxXujjk5wbo2VZOnLkiFJSUs5Zt14CyiWXXKLY2FiVlpaGlJeWliopKalGfY/HI4/HE1KWkJDgZBfl9Xov2CeexPguBBf6GBlf9LvQx3ihj09yZoxNmzYNq169vEm2YcOG6ty5s1asWBEsCwQCWrFihTIyMuqjSwAAwCD19hJPbm6uRowYofT0dHXp0kUzZ87UsWPHNHLkyPrqEgAAMES9BZQ777xT//73vzV58mTt379faWlpWrZsmVq2bFlfXZL03ctJU6ZMqfGS0oWC8UW/C32MjC/6XehjvNDHJ5kxRpdlhfNZHwAAgPOH7+IBAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGCcH2RAKSgoUOvWrRUXF6euXbvqs88+q7X+W2+9pWuuuUZxcXG67rrr9MEHH5ynntZNJOObP3++XC5XyC0uLu489jYya9as0YABA5SSkiKXy6XFixef85xVq1bpRz/6kTwej6688krNnz/f8X7WVaTjW7VqVY31c7lc2r9///npcISmT5+uG2+8UU2aNFGLFi2UnZ2tnTt3nvO8aNqDdRljNO3Dl19+Wddff33wL4xmZGRo6dKltZ4TTesX6fiiae3O5Omnn5bL5VJOTk6t9epjDX9wAWXhwoXKzc3VlClTtGnTJnXq1ElZWVkqKys7Y/1PP/1Ud911l+6//35t3rxZ2dnZys7O1rZt285zz8MT6fik7/6U8b/+9a/gbe/eveexx5E5duyYOnXqpIKCgrDql5SUqH///rrlllu0ZcsW5eTk6Oc//7mWL1/ucE/rJtLxVdu5c2fIGrZo0cKhHn4/q1ev1pgxY7Ru3ToVFRXJ7/erT58+Onbs7N/2Gm17sC5jlKJnH1522WV6+umnVVxcrI0bN+rWW2/V7bffru3bt5+xfrStX6Tjk6Jn7U63YcMGvfLKK7r++utrrVdva2jP9xNHjy5dulhjxowJHldVVVkpKSnW9OnTz1h/yJAhVv/+/UPKunbtaj344IOO9rOuIh3fvHnzrKZNm56n3tlLkrVo0aJa6zz22GPWtddeG1J25513WllZWQ72zB7hjO/jjz+2JFmHDh06L32yW1lZmSXJWr169VnrRNsePF04Y4zmfWhZlnXxxRdbv/3tb894X7Svn2XVPr5oXbsjR45Y7dq1s4qKiqwePXpY48aNO2vd+lrDH9QVlBMnTqi4uFiZmZnBspiYGGVmZmrt2rVnPGft2rUh9SUpKyvrrPXrU13GJ0lHjx7V5ZdfrtTU1HP+TyHaRNP6fR9paWlKTk5W79699de//rW+uxO28vJySVJiYuJZ60T7GoYzRik692FVVZXefPNNHTt27KzfoxbN6xfO+KToXLsxY8aof//+NdbmTOprDX9QAeXAgQOqqqqq8ef0W7ZsedbX7Pfv3x9R/fpUl/FdffXV+t3vfqf33ntPr7/+ugKBgLp166Z//vOf56PLjjvb+lVUVOg///lPPfXKPsnJyZozZ47eeecdvfPOO0pNTVXPnj21adOm+u7aOQUCAeXk5Kh79+7q2LHjWetF0x48XbhjjLZ9uHXrVjVu3Fgej0cPPfSQFi1apA4dOpyxbjSuXyTji7a1k6Q333xTmzZt0vTp08OqX19rWG/fxQMzZGRkhPzPoFu3bmrfvr1eeeUVPfHEE/XYM4Tj6quv1tVXXx087tatm7744gs9//zz+v3vf1+PPTu3MWPGaNu2bfrkk0/quyuOCXeM0bYPr776am3ZskXl5eV6++23NWLECK1evfqsv8SjTSTji7a1+/rrrzVu3DgVFRUZ/2beH1RAueSSSxQbG6vS0tKQ8tLSUiUlJZ3xnKSkpIjq16e6jO90brdbN9xwg3bv3u1EF8+7s62f1+tVo0aN6qlXzurSpYvxv/THjh2rJUuWaM2aNbrssstqrRtNe/BUkYzxdKbvw4YNG+rKK6+UJHXu3FkbNmzQCy+8oFdeeaVG3Whcv0jGdzrT1664uFhlZWX60Y9+FCyrqqrSmjVr9OKLL8rn8yk2NjbknPpawx/USzwNGzZU586dtWLFimBZIBDQihUrzvr6YkZGRkh9SSoqKqr19cj6Upfxna6qqkpbt25VcnKyU908r6Jp/eyyZcsWY9fPsiyNHTtWixYt0sqVK9WmTZtznhNta1iXMZ4u2vZhIBCQz+c7433Rtn5nUtv4Tmf62vXq1Utbt27Vli1bgrf09HQNHz5cW7ZsqRFOpHpcQ0ffgmugN9980/J4PNb8+fOtv//979aoUaOshIQEa//+/ZZlWdY999xjTZgwIVj/r3/9q9WgQQPr2WeftXbs2GFNmTLFcrvd1tatW+trCLWKdHz5+fnW8uXLrS+++MIqLi62hg4dasXFxVnbt2+vryHU6siRI9bmzZutzZs3W5Ks5557ztq8ebO1d+9ey7Isa8KECdY999wTrP/ll19a8fHx1vjx460dO3ZYBQUFVmxsrLVs2bL6GkKtIh3f888/by1evNjatWuXtXXrVmvcuHFWTEyM9dFHH9XXEGr18MMPW02bNrVWrVpl/etf/wreKisrg3WifQ/WZYzRtA8nTJhgrV692iopKbH+9re/WRMmTLBcLpf14YcfWpYV/esX6fiiae3O5vRP8Ziyhj+4gGJZljV79myrVatWVsOGDa0uXbpY69atC97Xo0cPa8SIESH1//jHP1pXXXWV1bBhQ+vaa6+13n///fPc48hEMr6cnJxg3ZYtW1q33XabtWnTpnrodXiqP1Z7+q16TCNGjLB69OhR45y0tDSrYcOG1hVXXGHNmzfvvPc7XJGO75lnnrHatm1rxcXFWYmJiVbPnj2tlStX1k/nw3CmsUkKWZNo34N1GWM07cP77rvPuvzyy62GDRtazZs3t3r16hX85W1Z0b9+kY4vmtbubE4PKKasocuyLMvZazQAAACR+UG9BwUAAEQHAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGOf/A5gg3d4YUXy9AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(Model):\n",
        "    def __init__(self,hidden_dim,bottle_neck_dim,encoder_name):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        \"\"\"\n",
        "        Initialize the model parameters\n",
        "        \"\"\"\n",
        "        self.hidden=hidden_dim\n",
        "        self.bottle_neck=bottle_neck_dim\n",
        "        self.encoder_name=encoder_name\n",
        "\n",
        "    def Encoder(self, inputs):\n",
        "        \"\"\"\n",
        "        Functional_parameters\n",
        "        \"\"\"\n",
        "        self.shape=inputs.shape[1]\n",
        "        x=Dense(units=self.hidden,name=self.encoder_name+'_encoder_h',activation = 'tanh',kernel_initializer='glorot_uniform')(inputs)\n",
        "        x=Dense(units=self.bottle_neck,name=self.encoder_name+'_bottleneck',kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def Decoder(self, inputs):\n",
        "        x = Dense(units=self.hidden,name=self.encoder_name+'_decoder_h',activation = 'tanh',kernel_initializer='glorot_uniform')(inputs)\n",
        "        x = Dense(units=self.shape,name=self.encoder_name+'_decoder_out',kernel_initializer='glorot_uniform')(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def Full_model(self,inputs):\n",
        "        \"\"\"\n",
        "        Full model is required for training\n",
        "        \"\"\"\n",
        "        bottle_neck = self.Encoder(inputs)\n",
        "        reconstructed = self.Decoder(bottle_neck)\n",
        "\n",
        "        return reconstructed"
      ],
      "metadata": {
        "id": "jcGaY8c7VS2-"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AE1_hidden=32\n",
        "AE1_bottle_neck=28\n",
        "AE2_hidden=24\n",
        "AE2_bottle_neck=16\n",
        "AE3_hidden=12\n",
        "AE3_bottle_neck=8\n",
        "\n",
        "Inputs_AE1 = Input(shape=(36,),name = 'Input_AE1')\n",
        "Inputs_AE2 = Input(shape=(AE1_bottle_neck,),name = 'Input_AE2')\n",
        "Inputs_AE3 = Input(shape=(AE2_bottle_neck,),name = 'Input_AE3')\n",
        "\n",
        "AE1=AutoEncoder(AE1_hidden,AE1_bottle_neck,'AE1')\n",
        "AE2=AutoEncoder(AE2_hidden,AE2_bottle_neck,'AE2')\n",
        "AE3=AutoEncoder(AE3_hidden,AE3_bottle_neck,'AE3')\n",
        "\n",
        "outputs_AE1=AE1.Full_model(Inputs_AE1)\n",
        "Model_AE1 = Model(inputs=Inputs_AE1,outputs=outputs_AE1, name = 'AE1')\n",
        "\n",
        "outputs_AE2=AE2.Full_model(Inputs_AE2)\n",
        "Model_AE2 = Model(inputs=Inputs_AE2,outputs=outputs_AE2, name = 'AE2')\n",
        "\n",
        "outputs_AE3=AE3.Full_model(Inputs_AE3)\n",
        "Model_AE3 = Model(inputs=Inputs_AE3,outputs=outputs_AE3, name = 'AE3')"
      ],
      "metadata": {
        "id": "N_TcW_e7sM3u"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_AE1.compile(optimizer='Adam',loss='mse')\n",
        "Model_AE2.compile(optimizer='Adam',loss='mse')\n",
        "Model_AE3.compile(optimizer='Adam',loss='mse')\n",
        "input_data = X_train\n",
        "AEs=[Model_AE1, Model_AE2, Model_AE3]\n",
        "Encoders = []\n",
        "batch_size=32\n",
        "for i,model in enumerate(AEs):\n",
        "    #print(f\"\\n Training on {i+1}st AE\\n\")\n",
        "    #early_stop = EarlyStopping(monitor = 'loss', mode = 'auto', min_delta = 0.000001, patience = 15, verbose = 0)\n",
        "    #model.fit(input_data,input_data,verbose=1,batch_size=batch_size,epochs=1000,callbacks=[early_stop])\n",
        "    Encoder = Model(inputs=model.input,outputs=model.get_layer(f'AE{i+1}_bottleneck').output,name=f'AE_{i+1}')\n",
        "    #input_data = Encoder.predict(input_data)\n",
        "    Encoders.append(Encoder)"
      ],
      "metadata": {
        "id": "JMYSYYHpKK8O"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(36,),name = 'Input_stacked_AE')\n",
        "# Stack the encoders\n",
        "encoded = inputs\n",
        "for encoder in Encoders:\n",
        "    encoded = encoder(encoded)\n",
        "\n",
        "# Create the stacked encoder model\n",
        "Stacked_encoder_model = Model(inputs=inputs, outputs=encoded, name = 'Stacked_autoencoder')"
      ],
      "metadata": {
        "id": "65KTvt-VfJeh"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(history):\n",
        "    plt.figure(figsize = (10, 8))\n",
        "    plt.title(\"Learning curve\", fontsize=22)\n",
        "    plt.plot(history.history['loss'], label = 'Train loss')\n",
        "    plt.plot(history.history['val_loss'], label = 'Validation loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epochs', fontsize=20)\n",
        "    plt.ylabel('Loss', fontsize=20)\n",
        "    plt.show()\n",
        "\n",
        "def Confusion_matrix(y_true,y_pred, data_name):\n",
        "    labels = np.unique(y_true)\n",
        "    CM = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize = (10, 8))\n",
        "    plt.title(f\"Confusion Matrix for {data_name} data\", fontsize=18)\n",
        "    sns.heatmap(CM, annot = True, cmap = 'Pastel2', fmt = 'd', linewidths = 1.0)\n",
        "    plt.yticks(np.arange(len(labels)) + 0.5, labels, fontsize = 14, rotation = 'horizontal')\n",
        "    plt.xticks(np.arange(len(labels)) + 0.5, labels, fontsize = 14)\n",
        "    plt.xlabel(\"Predicted\", fontsize = 16)\n",
        "    plt.ylabel(\"Actual\", fontsize = 16)\n",
        "    plt.savefig(f\"Conf_{data_name}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BCyA2Pz8wdSR"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the top layer for domain specific downstream applications\n",
        "def classification_block(inputs):\n",
        "    x = Dense(units=12,activation='tanh',name='classifier_dense_1',kernel_initializer='glorot_uniform')(inputs)\n",
        "    x = Dense(units=5,activation='softmax',name='output_layer',kernel_initializer='glorot_uniform')(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "AlKzNAlFDmXm"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(36,))\n",
        "btneck_output = Stacked_encoder_model(inputs)\n",
        "output = classification_block(btneck_output)\n",
        "Fine_tunable_model = Model(inputs=inputs,outputs=output)"
      ],
      "metadata": {
        "id": "d8aXjDd9f6h1"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Fine_tunable_model.compile(optimizer=Adam(learning_rate=0.0001),loss=SparseCategoricalCrossentropy(), metrics=['acc'])\n",
        "early_stop = EarlyStopping(monitor = 'val_loss', mode = 'auto', min_delta = 0.000001, patience = 15, verbose = 0)\n",
        "filepath = 'Best_model_epoch_{epoch}_val_loss_{val_loss:.4f}.keras'\n",
        "chkpt = ModelCheckpoint(filepath = filepath, monitor = 'val_loss', save_best_only = True, verbose = 1)\n",
        "history_wt = Fine_tunable_model.fit(X_train, Y_train, batch_size = 64, validation_data = (X_val, Y_val), validation_batch_size = 64, epochs = 1000, callbacks = [early_stop, chkpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsZmGYM-ELnP",
        "outputId": "32583972-7b09-440a-9414-240627352f74"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            " 1/12 [=>............................] - ETA: 28s - loss: 1.5650 - acc: 0.2344\n",
            "Epoch 1: val_loss improved from inf to 1.60817, saving model to Best_model_epoch_1_val_loss_1.6082.keras\n",
            "12/12 [==============================] - 3s 37ms/step - loss: 1.6061 - acc: 0.2293 - val_loss: 1.6082 - val_acc: 0.2480\n",
            "Epoch 2/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.5954 - acc: 0.2480\n",
            "Epoch 2: val_loss improved from 1.60817 to 1.59836, saving model to Best_model_epoch_2_val_loss_1.5984.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.5954 - acc: 0.2480 - val_loss: 1.5984 - val_acc: 0.2600\n",
            "Epoch 3/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5909 - acc: 0.2656\n",
            "Epoch 3: val_loss improved from 1.59836 to 1.58859, saving model to Best_model_epoch_3_val_loss_1.5886.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.5854 - acc: 0.2760 - val_loss: 1.5886 - val_acc: 0.2800\n",
            "Epoch 4/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5889 - acc: 0.2500\n",
            "Epoch 4: val_loss improved from 1.58859 to 1.57914, saving model to Best_model_epoch_4_val_loss_1.5791.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.5762 - acc: 0.2987 - val_loss: 1.5791 - val_acc: 0.3080\n",
            "Epoch 5/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5833 - acc: 0.2656\n",
            "Epoch 5: val_loss improved from 1.57914 to 1.57023, saving model to Best_model_epoch_5_val_loss_1.5702.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.5668 - acc: 0.3173 - val_loss: 1.5702 - val_acc: 0.3200\n",
            "Epoch 6/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5570 - acc: 0.3281\n",
            "Epoch 6: val_loss improved from 1.57023 to 1.56074, saving model to Best_model_epoch_6_val_loss_1.5607.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.5583 - acc: 0.3373 - val_loss: 1.5607 - val_acc: 0.3400\n",
            "Epoch 7/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.5489 - acc: 0.3587\n",
            "Epoch 7: val_loss improved from 1.56074 to 1.55226, saving model to Best_model_epoch_7_val_loss_1.5523.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.5489 - acc: 0.3587 - val_loss: 1.5523 - val_acc: 0.3520\n",
            "Epoch 8/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5252 - acc: 0.4688\n",
            "Epoch 8: val_loss improved from 1.55226 to 1.54320, saving model to Best_model_epoch_8_val_loss_1.5432.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.5403 - acc: 0.3800 - val_loss: 1.5432 - val_acc: 0.3400\n",
            "Epoch 9/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5530 - acc: 0.3125\n",
            "Epoch 9: val_loss improved from 1.54320 to 1.53446, saving model to Best_model_epoch_9_val_loss_1.5345.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.5315 - acc: 0.3920 - val_loss: 1.5345 - val_acc: 0.3520\n",
            "Epoch 10/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5281 - acc: 0.4219\n",
            "Epoch 10: val_loss improved from 1.53446 to 1.52480, saving model to Best_model_epoch_10_val_loss_1.5248.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.5226 - acc: 0.3920 - val_loss: 1.5248 - val_acc: 0.3560\n",
            "Epoch 11/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4987 - acc: 0.4375\n",
            "Epoch 11: val_loss improved from 1.52480 to 1.51592, saving model to Best_model_epoch_11_val_loss_1.5159.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.5132 - acc: 0.4053 - val_loss: 1.5159 - val_acc: 0.3680\n",
            "Epoch 12/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4904 - acc: 0.4375\n",
            "Epoch 12: val_loss improved from 1.51592 to 1.50726, saving model to Best_model_epoch_12_val_loss_1.5073.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.5039 - acc: 0.4040 - val_loss: 1.5073 - val_acc: 0.3760\n",
            "Epoch 13/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4787 - acc: 0.5156\n",
            "Epoch 13: val_loss improved from 1.50726 to 1.49796, saving model to Best_model_epoch_13_val_loss_1.4980.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.4946 - acc: 0.4120 - val_loss: 1.4980 - val_acc: 0.3760\n",
            "Epoch 14/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4539 - acc: 0.5000\n",
            "Epoch 14: val_loss improved from 1.49796 to 1.48831, saving model to Best_model_epoch_14_val_loss_1.4883.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.4847 - acc: 0.4227 - val_loss: 1.4883 - val_acc: 0.3880\n",
            "Epoch 15/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.5002 - acc: 0.3906\n",
            "Epoch 15: val_loss improved from 1.48831 to 1.47882, saving model to Best_model_epoch_15_val_loss_1.4788.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.4751 - acc: 0.4347 - val_loss: 1.4788 - val_acc: 0.3920\n",
            "Epoch 16/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4925 - acc: 0.4531\n",
            "Epoch 16: val_loss improved from 1.47882 to 1.46909, saving model to Best_model_epoch_16_val_loss_1.4691.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.4653 - acc: 0.4493 - val_loss: 1.4691 - val_acc: 0.3920\n",
            "Epoch 17/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4601 - acc: 0.4844\n",
            "Epoch 17: val_loss improved from 1.46909 to 1.45980, saving model to Best_model_epoch_17_val_loss_1.4598.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.4554 - acc: 0.4547 - val_loss: 1.4598 - val_acc: 0.4040\n",
            "Epoch 18/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4930 - acc: 0.3750\n",
            "Epoch 18: val_loss improved from 1.45980 to 1.44995, saving model to Best_model_epoch_18_val_loss_1.4499.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.4455 - acc: 0.4560 - val_loss: 1.4499 - val_acc: 0.4160\n",
            "Epoch 19/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4313 - acc: 0.5156\n",
            "Epoch 19: val_loss improved from 1.44995 to 1.44102, saving model to Best_model_epoch_19_val_loss_1.4410.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.4353 - acc: 0.4613 - val_loss: 1.4410 - val_acc: 0.4120\n",
            "Epoch 20/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4088 - acc: 0.4844\n",
            "Epoch 20: val_loss improved from 1.44102 to 1.43140, saving model to Best_model_epoch_20_val_loss_1.4314.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.4251 - acc: 0.4627 - val_loss: 1.4314 - val_acc: 0.4160\n",
            "Epoch 21/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4036 - acc: 0.4531\n",
            "Epoch 21: val_loss improved from 1.43140 to 1.42084, saving model to Best_model_epoch_21_val_loss_1.4208.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.4151 - acc: 0.4653 - val_loss: 1.4208 - val_acc: 0.4280\n",
            "Epoch 22/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4448 - acc: 0.4375\n",
            "Epoch 22: val_loss improved from 1.42084 to 1.41225, saving model to Best_model_epoch_22_val_loss_1.4122.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.4045 - acc: 0.4627 - val_loss: 1.4122 - val_acc: 0.4160\n",
            "Epoch 23/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.4279 - acc: 0.4375\n",
            "Epoch 23: val_loss improved from 1.41225 to 1.40233, saving model to Best_model_epoch_23_val_loss_1.4023.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.3944 - acc: 0.4587 - val_loss: 1.4023 - val_acc: 0.4200\n",
            "Epoch 24/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.3782 - acc: 0.4688\n",
            "Epoch 24: val_loss improved from 1.40233 to 1.39263, saving model to Best_model_epoch_24_val_loss_1.3926.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 1.3839 - acc: 0.4693 - val_loss: 1.3926 - val_acc: 0.4280\n",
            "Epoch 25/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 1.3723 - acc: 0.4797\n",
            "Epoch 25: val_loss improved from 1.39263 to 1.38225, saving model to Best_model_epoch_25_val_loss_1.3822.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.3737 - acc: 0.4720 - val_loss: 1.3822 - val_acc: 0.4400\n",
            "Epoch 26/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 1.3576 - acc: 0.4815\n",
            "Epoch 26: val_loss improved from 1.38225 to 1.37282, saving model to Best_model_epoch_26_val_loss_1.3728.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.3637 - acc: 0.4760 - val_loss: 1.3728 - val_acc: 0.4400\n",
            "Epoch 27/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 1.3547 - acc: 0.4875\n",
            "Epoch 27: val_loss improved from 1.37282 to 1.36300, saving model to Best_model_epoch_27_val_loss_1.3630.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 1.3534 - acc: 0.4827 - val_loss: 1.3630 - val_acc: 0.4440\n",
            "Epoch 28/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.3377 - acc: 0.4931\n",
            "Epoch 28: val_loss improved from 1.36300 to 1.35357, saving model to Best_model_epoch_28_val_loss_1.3536.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.3435 - acc: 0.4840 - val_loss: 1.3536 - val_acc: 0.4440\n",
            "Epoch 29/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 1.3304 - acc: 0.4859\n",
            "Epoch 29: val_loss improved from 1.35357 to 1.34513, saving model to Best_model_epoch_29_val_loss_1.3451.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 1.3334 - acc: 0.4773 - val_loss: 1.3451 - val_acc: 0.4400\n",
            "Epoch 30/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 1.3208 - acc: 0.4858\n",
            "Epoch 30: val_loss improved from 1.34513 to 1.33518, saving model to Best_model_epoch_30_val_loss_1.3352.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 1.3234 - acc: 0.4800 - val_loss: 1.3352 - val_acc: 0.4480\n",
            "Epoch 31/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.3152 - acc: 0.4861\n",
            "Epoch 31: val_loss improved from 1.33518 to 1.32517, saving model to Best_model_epoch_31_val_loss_1.3252.keras\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.3134 - acc: 0.4893 - val_loss: 1.3252 - val_acc: 0.4520\n",
            "Epoch 32/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 1.3039 - acc: 0.5000\n",
            "Epoch 32: val_loss improved from 1.32517 to 1.31626, saving model to Best_model_epoch_32_val_loss_1.3163.keras\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3038 - acc: 0.4960 - val_loss: 1.3163 - val_acc: 0.4440\n",
            "Epoch 33/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.2897 - acc: 0.5052\n",
            "Epoch 33: val_loss improved from 1.31626 to 1.30713, saving model to Best_model_epoch_33_val_loss_1.3071.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 1.2941 - acc: 0.4987 - val_loss: 1.3071 - val_acc: 0.4480\n",
            "Epoch 34/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 1.2814 - acc: 0.4984\n",
            "Epoch 34: val_loss improved from 1.30713 to 1.29724, saving model to Best_model_epoch_34_val_loss_1.2972.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 1.2849 - acc: 0.4920 - val_loss: 1.2972 - val_acc: 0.4520\n",
            "Epoch 35/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 1.2675 - acc: 0.5078\n",
            "Epoch 35: val_loss improved from 1.29724 to 1.28955, saving model to Best_model_epoch_35_val_loss_1.2895.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.2757 - acc: 0.5000 - val_loss: 1.2895 - val_acc: 0.4480\n",
            "Epoch 36/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.2755 - acc: 0.4965\n",
            "Epoch 36: val_loss improved from 1.28955 to 1.28008, saving model to Best_model_epoch_36_val_loss_1.2801.keras\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 1.2662 - acc: 0.5027 - val_loss: 1.2801 - val_acc: 0.4560\n",
            "Epoch 37/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.2700 - acc: 0.5000\n",
            "Epoch 37: val_loss improved from 1.28008 to 1.27216, saving model to Best_model_epoch_37_val_loss_1.2722.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 1.2572 - acc: 0.5067 - val_loss: 1.2722 - val_acc: 0.4680\n",
            "Epoch 38/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.2435 - acc: 0.5260\n",
            "Epoch 38: val_loss improved from 1.27216 to 1.26300, saving model to Best_model_epoch_38_val_loss_1.2630.keras\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 1.2487 - acc: 0.5120 - val_loss: 1.2630 - val_acc: 0.4720\n",
            "Epoch 39/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.2323 - acc: 0.5260\n",
            "Epoch 39: val_loss improved from 1.26300 to 1.25546, saving model to Best_model_epoch_39_val_loss_1.2555.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 1.2401 - acc: 0.5120 - val_loss: 1.2555 - val_acc: 0.4760\n",
            "Epoch 40/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 1.2286 - acc: 0.5295\n",
            "Epoch 40: val_loss improved from 1.25546 to 1.24660, saving model to Best_model_epoch_40_val_loss_1.2466.keras\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.2319 - acc: 0.5133 - val_loss: 1.2466 - val_acc: 0.4880\n",
            "Epoch 41/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0980 - acc: 0.6719\n",
            "Epoch 41: val_loss improved from 1.24660 to 1.23848, saving model to Best_model_epoch_41_val_loss_1.2385.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2235 - acc: 0.5213 - val_loss: 1.2385 - val_acc: 0.4880\n",
            "Epoch 42/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.2290 - acc: 0.5469\n",
            "Epoch 42: val_loss improved from 1.23848 to 1.23213, saving model to Best_model_epoch_42_val_loss_1.2321.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2159 - acc: 0.5227 - val_loss: 1.2321 - val_acc: 0.4880\n",
            "Epoch 43/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1323 - acc: 0.5781\n",
            "Epoch 43: val_loss improved from 1.23213 to 1.22429, saving model to Best_model_epoch_43_val_loss_1.2243.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2080 - acc: 0.5253 - val_loss: 1.2243 - val_acc: 0.4920\n",
            "Epoch 44/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.2426 - acc: 0.5312\n",
            "Epoch 44: val_loss improved from 1.22429 to 1.21697, saving model to Best_model_epoch_44_val_loss_1.2170.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.2001 - acc: 0.5293 - val_loss: 1.2170 - val_acc: 0.5080\n",
            "Epoch 45/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.2171 - acc: 0.5156\n",
            "Epoch 45: val_loss improved from 1.21697 to 1.20952, saving model to Best_model_epoch_45_val_loss_1.2095.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.1928 - acc: 0.5293 - val_loss: 1.2095 - val_acc: 0.5160\n",
            "Epoch 46/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1670 - acc: 0.5469\n",
            "Epoch 46: val_loss improved from 1.20952 to 1.20227, saving model to Best_model_epoch_46_val_loss_1.2023.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1859 - acc: 0.5267 - val_loss: 1.2023 - val_acc: 0.5200\n",
            "Epoch 47/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.2495 - acc: 0.4844\n",
            "Epoch 47: val_loss improved from 1.20227 to 1.19737, saving model to Best_model_epoch_47_val_loss_1.1974.keras\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.1807 - acc: 0.5293 - val_loss: 1.1974 - val_acc: 0.5160\n",
            "Epoch 48/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1675 - acc: 0.5312\n",
            "Epoch 48: val_loss improved from 1.19737 to 1.18810, saving model to Best_model_epoch_48_val_loss_1.1881.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1731 - acc: 0.5373 - val_loss: 1.1881 - val_acc: 0.5200\n",
            "Epoch 49/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0907 - acc: 0.5781\n",
            "Epoch 49: val_loss improved from 1.18810 to 1.18275, saving model to Best_model_epoch_49_val_loss_1.1828.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.1656 - acc: 0.5387 - val_loss: 1.1828 - val_acc: 0.5320\n",
            "Epoch 50/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1941 - acc: 0.5312\n",
            "Epoch 50: val_loss improved from 1.18275 to 1.17765, saving model to Best_model_epoch_50_val_loss_1.1777.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.1597 - acc: 0.5413 - val_loss: 1.1777 - val_acc: 0.5400\n",
            "Epoch 51/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0372 - acc: 0.6562\n",
            "Epoch 51: val_loss improved from 1.17765 to 1.17110, saving model to Best_model_epoch_51_val_loss_1.1711.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.1533 - acc: 0.5480 - val_loss: 1.1711 - val_acc: 0.5360\n",
            "Epoch 52/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1238 - acc: 0.5938\n",
            "Epoch 52: val_loss improved from 1.17110 to 1.16387, saving model to Best_model_epoch_52_val_loss_1.1639.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1481 - acc: 0.5493 - val_loss: 1.1639 - val_acc: 0.5440\n",
            "Epoch 53/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.2053 - acc: 0.5625\n",
            "Epoch 53: val_loss improved from 1.16387 to 1.15917, saving model to Best_model_epoch_53_val_loss_1.1592.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.1413 - acc: 0.5533 - val_loss: 1.1592 - val_acc: 0.5480\n",
            "Epoch 54/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1765 - acc: 0.5000\n",
            "Epoch 54: val_loss improved from 1.15917 to 1.15416, saving model to Best_model_epoch_54_val_loss_1.1542.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.1355 - acc: 0.5560 - val_loss: 1.1542 - val_acc: 0.5440\n",
            "Epoch 55/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1795 - acc: 0.4531\n",
            "Epoch 55: val_loss improved from 1.15416 to 1.14731, saving model to Best_model_epoch_55_val_loss_1.1473.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.1302 - acc: 0.5547 - val_loss: 1.1473 - val_acc: 0.5640\n",
            "Epoch 56/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0487 - acc: 0.5781\n",
            "Epoch 56: val_loss improved from 1.14731 to 1.14189, saving model to Best_model_epoch_56_val_loss_1.1419.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.1242 - acc: 0.5627 - val_loss: 1.1419 - val_acc: 0.5560\n",
            "Epoch 57/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1837 - acc: 0.5156\n",
            "Epoch 57: val_loss improved from 1.14189 to 1.13760, saving model to Best_model_epoch_57_val_loss_1.1376.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.1195 - acc: 0.5627 - val_loss: 1.1376 - val_acc: 0.5560\n",
            "Epoch 58/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1071 - acc: 0.6094\n",
            "Epoch 58: val_loss improved from 1.13760 to 1.13224, saving model to Best_model_epoch_58_val_loss_1.1322.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.1141 - acc: 0.5587 - val_loss: 1.1322 - val_acc: 0.5520\n",
            "Epoch 59/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.2069 - acc: 0.5312\n",
            "Epoch 59: val_loss improved from 1.13224 to 1.12668, saving model to Best_model_epoch_59_val_loss_1.1267.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.1102 - acc: 0.5693 - val_loss: 1.1267 - val_acc: 0.5680\n",
            "Epoch 60/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9566 - acc: 0.6875\n",
            "Epoch 60: val_loss improved from 1.12668 to 1.12273, saving model to Best_model_epoch_60_val_loss_1.1227.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.1047 - acc: 0.5707 - val_loss: 1.1227 - val_acc: 0.5680\n",
            "Epoch 61/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1020 - acc: 0.5781\n",
            "Epoch 61: val_loss improved from 1.12273 to 1.11802, saving model to Best_model_epoch_61_val_loss_1.1180.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0997 - acc: 0.5680 - val_loss: 1.1180 - val_acc: 0.5760\n",
            "Epoch 62/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0641 - acc: 0.5625\n",
            "Epoch 62: val_loss improved from 1.11802 to 1.11287, saving model to Best_model_epoch_62_val_loss_1.1129.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0947 - acc: 0.5773 - val_loss: 1.1129 - val_acc: 0.5840\n",
            "Epoch 63/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1512 - acc: 0.6250\n",
            "Epoch 63: val_loss improved from 1.11287 to 1.10895, saving model to Best_model_epoch_63_val_loss_1.1090.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0907 - acc: 0.5760 - val_loss: 1.1090 - val_acc: 0.5760\n",
            "Epoch 64/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1113 - acc: 0.6094\n",
            "Epoch 64: val_loss improved from 1.10895 to 1.10338, saving model to Best_model_epoch_64_val_loss_1.1034.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0867 - acc: 0.5840 - val_loss: 1.1034 - val_acc: 0.5800\n",
            "Epoch 65/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0341 - acc: 0.5781\n",
            "Epoch 65: val_loss improved from 1.10338 to 1.10024, saving model to Best_model_epoch_65_val_loss_1.1002.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0824 - acc: 0.5827 - val_loss: 1.1002 - val_acc: 0.5720\n",
            "Epoch 66/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0543 - acc: 0.6406\n",
            "Epoch 66: val_loss improved from 1.10024 to 1.09563, saving model to Best_model_epoch_66_val_loss_1.0956.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0776 - acc: 0.5907 - val_loss: 1.0956 - val_acc: 0.5840\n",
            "Epoch 67/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0149 - acc: 0.6406\n",
            "Epoch 67: val_loss improved from 1.09563 to 1.09212, saving model to Best_model_epoch_67_val_loss_1.0921.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0734 - acc: 0.5933 - val_loss: 1.0921 - val_acc: 0.5760\n",
            "Epoch 68/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0864 - acc: 0.5625\n",
            "Epoch 68: val_loss improved from 1.09212 to 1.08836, saving model to Best_model_epoch_68_val_loss_1.0884.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0701 - acc: 0.5973 - val_loss: 1.0884 - val_acc: 0.5800\n",
            "Epoch 69/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1265 - acc: 0.5938\n",
            "Epoch 69: val_loss improved from 1.08836 to 1.08433, saving model to Best_model_epoch_69_val_loss_1.0843.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0660 - acc: 0.5973 - val_loss: 1.0843 - val_acc: 0.5800\n",
            "Epoch 70/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1215 - acc: 0.5469\n",
            "Epoch 70: val_loss improved from 1.08433 to 1.07983, saving model to Best_model_epoch_70_val_loss_1.0798.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0623 - acc: 0.6013 - val_loss: 1.0798 - val_acc: 0.5920\n",
            "Epoch 71/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0802 - acc: 0.5938\n",
            "Epoch 71: val_loss improved from 1.07983 to 1.07688, saving model to Best_model_epoch_71_val_loss_1.0769.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0584 - acc: 0.6093 - val_loss: 1.0769 - val_acc: 0.5800\n",
            "Epoch 72/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0225 - acc: 0.5938\n",
            "Epoch 72: val_loss improved from 1.07688 to 1.07295, saving model to Best_model_epoch_72_val_loss_1.0730.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0551 - acc: 0.6107 - val_loss: 1.0730 - val_acc: 0.5840\n",
            "Epoch 73/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0396 - acc: 0.5938\n",
            "Epoch 73: val_loss improved from 1.07295 to 1.06915, saving model to Best_model_epoch_73_val_loss_1.0691.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0510 - acc: 0.6147 - val_loss: 1.0691 - val_acc: 0.5880\n",
            "Epoch 74/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0217 - acc: 0.6562\n",
            "Epoch 74: val_loss improved from 1.06915 to 1.06589, saving model to Best_model_epoch_74_val_loss_1.0659.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0481 - acc: 0.6173 - val_loss: 1.0659 - val_acc: 0.5880\n",
            "Epoch 75/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0685 - acc: 0.6719\n",
            "Epoch 75: val_loss improved from 1.06589 to 1.06224, saving model to Best_model_epoch_75_val_loss_1.0622.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0444 - acc: 0.6213 - val_loss: 1.0622 - val_acc: 0.5840\n",
            "Epoch 76/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0071 - acc: 0.6719\n",
            "Epoch 76: val_loss improved from 1.06224 to 1.05943, saving model to Best_model_epoch_76_val_loss_1.0594.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0409 - acc: 0.6187 - val_loss: 1.0594 - val_acc: 0.5840\n",
            "Epoch 77/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8810 - acc: 0.7344\n",
            "Epoch 77: val_loss improved from 1.05943 to 1.05615, saving model to Best_model_epoch_77_val_loss_1.0561.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0381 - acc: 0.6187 - val_loss: 1.0561 - val_acc: 0.5840\n",
            "Epoch 78/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0844 - acc: 0.5469\n",
            "Epoch 78: val_loss improved from 1.05615 to 1.05267, saving model to Best_model_epoch_78_val_loss_1.0527.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0346 - acc: 0.6240 - val_loss: 1.0527 - val_acc: 0.6000\n",
            "Epoch 79/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0141 - acc: 0.6719\n",
            "Epoch 79: val_loss improved from 1.05267 to 1.04997, saving model to Best_model_epoch_79_val_loss_1.0500.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.0319 - acc: 0.6187 - val_loss: 1.0500 - val_acc: 0.5920\n",
            "Epoch 80/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0188 - acc: 0.6719\n",
            "Epoch 80: val_loss improved from 1.04997 to 1.04679, saving model to Best_model_epoch_80_val_loss_1.0468.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0291 - acc: 0.6253 - val_loss: 1.0468 - val_acc: 0.5800\n",
            "Epoch 81/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0124 - acc: 0.6250\n",
            "Epoch 81: val_loss improved from 1.04679 to 1.04361, saving model to Best_model_epoch_81_val_loss_1.0436.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.0262 - acc: 0.6240 - val_loss: 1.0436 - val_acc: 0.6040\n",
            "Epoch 82/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0051 - acc: 0.6094\n",
            "Epoch 82: val_loss improved from 1.04361 to 1.04090, saving model to Best_model_epoch_82_val_loss_1.0409.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0237 - acc: 0.6267 - val_loss: 1.0409 - val_acc: 0.6120\n",
            "Epoch 83/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0336 - acc: 0.5938\n",
            "Epoch 83: val_loss improved from 1.04090 to 1.03867, saving model to Best_model_epoch_83_val_loss_1.0387.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 1.0203 - acc: 0.6227 - val_loss: 1.0387 - val_acc: 0.5800\n",
            "Epoch 84/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0210 - acc: 0.6250\n",
            "Epoch 84: val_loss improved from 1.03867 to 1.03579, saving model to Best_model_epoch_84_val_loss_1.0358.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0190 - acc: 0.6200 - val_loss: 1.0358 - val_acc: 0.6200\n",
            "Epoch 85/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9667 - acc: 0.6406\n",
            "Epoch 85: val_loss improved from 1.03579 to 1.03394, saving model to Best_model_epoch_85_val_loss_1.0339.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0159 - acc: 0.6160 - val_loss: 1.0339 - val_acc: 0.5800\n",
            "Epoch 86/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0161 - acc: 0.6250\n",
            "Epoch 86: val_loss improved from 1.03394 to 1.03036, saving model to Best_model_epoch_86_val_loss_1.0304.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0122 - acc: 0.6253 - val_loss: 1.0304 - val_acc: 0.6080\n",
            "Epoch 87/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9583 - acc: 0.6719\n",
            "Epoch 87: val_loss improved from 1.03036 to 1.02810, saving model to Best_model_epoch_87_val_loss_1.0281.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.0098 - acc: 0.6267 - val_loss: 1.0281 - val_acc: 0.6240\n",
            "Epoch 88/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0647 - acc: 0.5938\n",
            "Epoch 88: val_loss improved from 1.02810 to 1.02609, saving model to Best_model_epoch_88_val_loss_1.0261.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.0077 - acc: 0.6267 - val_loss: 1.0261 - val_acc: 0.6120\n",
            "Epoch 89/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0756 - acc: 0.5781\n",
            "Epoch 89: val_loss improved from 1.02609 to 1.02322, saving model to Best_model_epoch_89_val_loss_1.0232.keras\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 1.0049 - acc: 0.6280 - val_loss: 1.0232 - val_acc: 0.6240\n",
            "Epoch 90/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.9981 - acc: 0.6321\n",
            "Epoch 90: val_loss improved from 1.02322 to 1.02056, saving model to Best_model_epoch_90_val_loss_1.0206.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 1.0021 - acc: 0.6267 - val_loss: 1.0206 - val_acc: 0.6080\n",
            "Epoch 91/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.9937 - acc: 0.6378\n",
            "Epoch 91: val_loss improved from 1.02056 to 1.01844, saving model to Best_model_epoch_91_val_loss_1.0184.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9994 - acc: 0.6360 - val_loss: 1.0184 - val_acc: 0.6160\n",
            "Epoch 92/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.9946 - acc: 0.6234\n",
            "Epoch 92: val_loss improved from 1.01844 to 1.01606, saving model to Best_model_epoch_92_val_loss_1.0161.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9971 - acc: 0.6320 - val_loss: 1.0161 - val_acc: 0.6200\n",
            "Epoch 93/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 1.0011 - acc: 0.6203\n",
            "Epoch 93: val_loss improved from 1.01606 to 1.01449, saving model to Best_model_epoch_93_val_loss_1.0145.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.9952 - acc: 0.6280 - val_loss: 1.0145 - val_acc: 0.6160\n",
            "Epoch 94/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.9860 - acc: 0.6281\n",
            "Epoch 94: val_loss improved from 1.01449 to 1.01125, saving model to Best_model_epoch_94_val_loss_1.0113.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.9932 - acc: 0.6320 - val_loss: 1.0113 - val_acc: 0.6240\n",
            "Epoch 95/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9902 - acc: 0.6320\n",
            "Epoch 95: val_loss improved from 1.01125 to 1.00939, saving model to Best_model_epoch_95_val_loss_1.0094.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9902 - acc: 0.6320 - val_loss: 1.0094 - val_acc: 0.6240\n",
            "Epoch 96/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.9890 - acc: 0.6266\n",
            "Epoch 96: val_loss improved from 1.00939 to 1.00712, saving model to Best_model_epoch_96_val_loss_1.0071.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9883 - acc: 0.6320 - val_loss: 1.0071 - val_acc: 0.6360\n",
            "Epoch 97/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9861 - acc: 0.6320\n",
            "Epoch 97: val_loss improved from 1.00712 to 1.00488, saving model to Best_model_epoch_97_val_loss_1.0049.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.9861 - acc: 0.6320 - val_loss: 1.0049 - val_acc: 0.6400\n",
            "Epoch 98/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.9764 - acc: 0.6349\n",
            "Epoch 98: val_loss improved from 1.00488 to 1.00316, saving model to Best_model_epoch_98_val_loss_1.0032.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.9842 - acc: 0.6320 - val_loss: 1.0032 - val_acc: 0.6080\n",
            "Epoch 99/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 1.0015 - acc: 0.6348\n",
            "Epoch 99: val_loss improved from 1.00316 to 1.00098, saving model to Best_model_epoch_99_val_loss_1.0010.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.9819 - acc: 0.6333 - val_loss: 1.0010 - val_acc: 0.6360\n",
            "Epoch 100/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.9806 - acc: 0.6313\n",
            "Epoch 100: val_loss improved from 1.00098 to 0.99846, saving model to Best_model_epoch_100_val_loss_0.9985.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.9797 - acc: 0.6320 - val_loss: 0.9985 - val_acc: 0.6320\n",
            "Epoch 101/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.9812 - acc: 0.6285\n",
            "Epoch 101: val_loss improved from 0.99846 to 0.99653, saving model to Best_model_epoch_101_val_loss_0.9965.keras\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.9778 - acc: 0.6347 - val_loss: 0.9965 - val_acc: 0.6360\n",
            "Epoch 102/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.9720 - acc: 0.6302\n",
            "Epoch 102: val_loss improved from 0.99653 to 0.99450, saving model to Best_model_epoch_102_val_loss_0.9945.keras\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.9764 - acc: 0.6387 - val_loss: 0.9945 - val_acc: 0.6160\n",
            "Epoch 103/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.9667 - acc: 0.6422\n",
            "Epoch 103: val_loss improved from 0.99450 to 0.99314, saving model to Best_model_epoch_103_val_loss_0.9931.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.9743 - acc: 0.6360 - val_loss: 0.9931 - val_acc: 0.6360\n",
            "Epoch 104/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.9801 - acc: 0.6297\n",
            "Epoch 104: val_loss improved from 0.99314 to 0.99124, saving model to Best_model_epoch_104_val_loss_0.9912.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.9721 - acc: 0.6373 - val_loss: 0.9912 - val_acc: 0.6320\n",
            "Epoch 105/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.9696 - acc: 0.6328\n",
            "Epoch 105: val_loss improved from 0.99124 to 0.98974, saving model to Best_model_epoch_105_val_loss_0.9897.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.9720 - acc: 0.6360 - val_loss: 0.9897 - val_acc: 0.6280\n",
            "Epoch 106/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.9747 - acc: 0.6378\n",
            "Epoch 106: val_loss improved from 0.98974 to 0.98726, saving model to Best_model_epoch_106_val_loss_0.9873.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9676 - acc: 0.6400 - val_loss: 0.9873 - val_acc: 0.6120\n",
            "Epoch 107/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9514 - acc: 0.6250\n",
            "Epoch 107: val_loss improved from 0.98726 to 0.98590, saving model to Best_model_epoch_107_val_loss_0.9859.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9664 - acc: 0.6413 - val_loss: 0.9859 - val_acc: 0.6160\n",
            "Epoch 108/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9665 - acc: 0.6094\n",
            "Epoch 108: val_loss improved from 0.98590 to 0.98440, saving model to Best_model_epoch_108_val_loss_0.9844.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.9650 - acc: 0.6373 - val_loss: 0.9844 - val_acc: 0.6320\n",
            "Epoch 109/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8600 - acc: 0.7031\n",
            "Epoch 109: val_loss improved from 0.98440 to 0.98192, saving model to Best_model_epoch_109_val_loss_0.9819.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9627 - acc: 0.6387 - val_loss: 0.9819 - val_acc: 0.6280\n",
            "Epoch 110/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0480 - acc: 0.5781\n",
            "Epoch 110: val_loss improved from 0.98192 to 0.98020, saving model to Best_model_epoch_110_val_loss_0.9802.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9607 - acc: 0.6427 - val_loss: 0.9802 - val_acc: 0.6240\n",
            "Epoch 111/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0188 - acc: 0.5781\n",
            "Epoch 111: val_loss improved from 0.98020 to 0.97830, saving model to Best_model_epoch_111_val_loss_0.9783.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9589 - acc: 0.6427 - val_loss: 0.9783 - val_acc: 0.6320\n",
            "Epoch 112/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9345 - acc: 0.7188\n",
            "Epoch 112: val_loss improved from 0.97830 to 0.97672, saving model to Best_model_epoch_112_val_loss_0.9767.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9569 - acc: 0.6440 - val_loss: 0.9767 - val_acc: 0.6200\n",
            "Epoch 113/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9616 - acc: 0.6562\n",
            "Epoch 113: val_loss improved from 0.97672 to 0.97503, saving model to Best_model_epoch_113_val_loss_0.9750.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9553 - acc: 0.6480 - val_loss: 0.9750 - val_acc: 0.6240\n",
            "Epoch 114/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0268 - acc: 0.6406\n",
            "Epoch 114: val_loss improved from 0.97503 to 0.97372, saving model to Best_model_epoch_114_val_loss_0.9737.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9539 - acc: 0.6520 - val_loss: 0.9737 - val_acc: 0.6320\n",
            "Epoch 115/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9618 - acc: 0.6719\n",
            "Epoch 115: val_loss improved from 0.97372 to 0.97140, saving model to Best_model_epoch_115_val_loss_0.9714.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9523 - acc: 0.6467 - val_loss: 0.9714 - val_acc: 0.6280\n",
            "Epoch 116/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9504 - acc: 0.6480\n",
            "Epoch 116: val_loss improved from 0.97140 to 0.97078, saving model to Best_model_epoch_116_val_loss_0.9708.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9504 - acc: 0.6480 - val_loss: 0.9708 - val_acc: 0.6320\n",
            "Epoch 117/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8438 - acc: 0.6719\n",
            "Epoch 117: val_loss improved from 0.97078 to 0.96862, saving model to Best_model_epoch_117_val_loss_0.9686.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9495 - acc: 0.6520 - val_loss: 0.9686 - val_acc: 0.6320\n",
            "Epoch 118/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8625 - acc: 0.6875\n",
            "Epoch 118: val_loss improved from 0.96862 to 0.96684, saving model to Best_model_epoch_118_val_loss_0.9668.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9466 - acc: 0.6507 - val_loss: 0.9668 - val_acc: 0.6240\n",
            "Epoch 119/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9216 - acc: 0.6250\n",
            "Epoch 119: val_loss improved from 0.96684 to 0.96521, saving model to Best_model_epoch_119_val_loss_0.9652.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9449 - acc: 0.6560 - val_loss: 0.9652 - val_acc: 0.6240\n",
            "Epoch 120/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0336 - acc: 0.6250\n",
            "Epoch 120: val_loss improved from 0.96521 to 0.96396, saving model to Best_model_epoch_120_val_loss_0.9640.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9432 - acc: 0.6507 - val_loss: 0.9640 - val_acc: 0.6240\n",
            "Epoch 121/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9413 - acc: 0.6547\n",
            "Epoch 121: val_loss improved from 0.96396 to 0.96200, saving model to Best_model_epoch_121_val_loss_0.9620.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9413 - acc: 0.6547 - val_loss: 0.9620 - val_acc: 0.6240\n",
            "Epoch 122/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9048 - acc: 0.7188\n",
            "Epoch 122: val_loss improved from 0.96200 to 0.96027, saving model to Best_model_epoch_122_val_loss_0.9603.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9404 - acc: 0.6533 - val_loss: 0.9603 - val_acc: 0.6280\n",
            "Epoch 123/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9973 - acc: 0.5938\n",
            "Epoch 123: val_loss improved from 0.96027 to 0.95921, saving model to Best_model_epoch_123_val_loss_0.9592.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9396 - acc: 0.6560 - val_loss: 0.9592 - val_acc: 0.6280\n",
            "Epoch 124/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8131 - acc: 0.6719\n",
            "Epoch 124: val_loss improved from 0.95921 to 0.95733, saving model to Best_model_epoch_124_val_loss_0.9573.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9364 - acc: 0.6520 - val_loss: 0.9573 - val_acc: 0.6320\n",
            "Epoch 125/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1506 - acc: 0.5312\n",
            "Epoch 125: val_loss improved from 0.95733 to 0.95576, saving model to Best_model_epoch_125_val_loss_0.9558.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9352 - acc: 0.6560 - val_loss: 0.9558 - val_acc: 0.6240\n",
            "Epoch 126/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7584 - acc: 0.7812\n",
            "Epoch 126: val_loss improved from 0.95576 to 0.95417, saving model to Best_model_epoch_126_val_loss_0.9542.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9335 - acc: 0.6627 - val_loss: 0.9542 - val_acc: 0.6280\n",
            "Epoch 127/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9229 - acc: 0.6562\n",
            "Epoch 127: val_loss improved from 0.95417 to 0.95254, saving model to Best_model_epoch_127_val_loss_0.9525.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9324 - acc: 0.6613 - val_loss: 0.9525 - val_acc: 0.6280\n",
            "Epoch 128/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.1112 - acc: 0.5938\n",
            "Epoch 128: val_loss improved from 0.95254 to 0.95121, saving model to Best_model_epoch_128_val_loss_0.9512.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9301 - acc: 0.6600 - val_loss: 0.9512 - val_acc: 0.6320\n",
            "Epoch 129/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9287 - acc: 0.6627\n",
            "Epoch 129: val_loss improved from 0.95121 to 0.95045, saving model to Best_model_epoch_129_val_loss_0.9505.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9287 - acc: 0.6627 - val_loss: 0.9505 - val_acc: 0.6240\n",
            "Epoch 130/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9379 - acc: 0.6406\n",
            "Epoch 130: val_loss improved from 0.95045 to 0.94847, saving model to Best_model_epoch_130_val_loss_0.9485.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9269 - acc: 0.6600 - val_loss: 0.9485 - val_acc: 0.6240\n",
            "Epoch 131/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8887 - acc: 0.7188\n",
            "Epoch 131: val_loss improved from 0.94847 to 0.94657, saving model to Best_model_epoch_131_val_loss_0.9466.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9261 - acc: 0.6613 - val_loss: 0.9466 - val_acc: 0.6360\n",
            "Epoch 132/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9844 - acc: 0.6406\n",
            "Epoch 132: val_loss improved from 0.94657 to 0.94563, saving model to Best_model_epoch_132_val_loss_0.9456.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9247 - acc: 0.6600 - val_loss: 0.9456 - val_acc: 0.6280\n",
            "Epoch 133/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9576 - acc: 0.5938\n",
            "Epoch 133: val_loss improved from 0.94563 to 0.94467, saving model to Best_model_epoch_133_val_loss_0.9447.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9232 - acc: 0.6640 - val_loss: 0.9447 - val_acc: 0.6320\n",
            "Epoch 134/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9203 - acc: 0.6719\n",
            "Epoch 134: val_loss improved from 0.94467 to 0.94278, saving model to Best_model_epoch_134_val_loss_0.9428.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9211 - acc: 0.6653 - val_loss: 0.9428 - val_acc: 0.6360\n",
            "Epoch 135/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0160 - acc: 0.6250\n",
            "Epoch 135: val_loss improved from 0.94278 to 0.94012, saving model to Best_model_epoch_135_val_loss_0.9401.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.9200 - acc: 0.6640 - val_loss: 0.9401 - val_acc: 0.6320\n",
            "Epoch 136/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8044 - acc: 0.6875\n",
            "Epoch 136: val_loss improved from 0.94012 to 0.94011, saving model to Best_model_epoch_136_val_loss_0.9401.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9177 - acc: 0.6613 - val_loss: 0.9401 - val_acc: 0.6360\n",
            "Epoch 137/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9459 - acc: 0.6562\n",
            "Epoch 137: val_loss improved from 0.94011 to 0.93778, saving model to Best_model_epoch_137_val_loss_0.9378.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9161 - acc: 0.6653 - val_loss: 0.9378 - val_acc: 0.6320\n",
            "Epoch 138/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9109 - acc: 0.6562\n",
            "Epoch 138: val_loss improved from 0.93778 to 0.93559, saving model to Best_model_epoch_138_val_loss_0.9356.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9150 - acc: 0.6680 - val_loss: 0.9356 - val_acc: 0.6360\n",
            "Epoch 139/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8768 - acc: 0.7031\n",
            "Epoch 139: val_loss improved from 0.93559 to 0.93530, saving model to Best_model_epoch_139_val_loss_0.9353.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9124 - acc: 0.6680 - val_loss: 0.9353 - val_acc: 0.6400\n",
            "Epoch 140/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8724 - acc: 0.6406\n",
            "Epoch 140: val_loss improved from 0.93530 to 0.93343, saving model to Best_model_epoch_140_val_loss_0.9334.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9114 - acc: 0.6693 - val_loss: 0.9334 - val_acc: 0.6360\n",
            "Epoch 141/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8484 - acc: 0.6719\n",
            "Epoch 141: val_loss improved from 0.93343 to 0.93229, saving model to Best_model_epoch_141_val_loss_0.9323.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9099 - acc: 0.6720 - val_loss: 0.9323 - val_acc: 0.6400\n",
            "Epoch 142/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8436 - acc: 0.6719\n",
            "Epoch 142: val_loss improved from 0.93229 to 0.92962, saving model to Best_model_epoch_142_val_loss_0.9296.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9080 - acc: 0.6707 - val_loss: 0.9296 - val_acc: 0.6360\n",
            "Epoch 143/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.9011 - acc: 0.6676\n",
            "Epoch 143: val_loss improved from 0.92962 to 0.92869, saving model to Best_model_epoch_143_val_loss_0.9287.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.9063 - acc: 0.6720 - val_loss: 0.9287 - val_acc: 0.6360\n",
            "Epoch 144/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8209 - acc: 0.7188\n",
            "Epoch 144: val_loss improved from 0.92869 to 0.92708, saving model to Best_model_epoch_144_val_loss_0.9271.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.9049 - acc: 0.6707 - val_loss: 0.9271 - val_acc: 0.6360\n",
            "Epoch 145/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9665 - acc: 0.6094\n",
            "Epoch 145: val_loss improved from 0.92708 to 0.92537, saving model to Best_model_epoch_145_val_loss_0.9254.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.9029 - acc: 0.6773 - val_loss: 0.9254 - val_acc: 0.6360\n",
            "Epoch 146/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8863 - acc: 0.7656\n",
            "Epoch 146: val_loss improved from 0.92537 to 0.92322, saving model to Best_model_epoch_146_val_loss_0.9232.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.9012 - acc: 0.6827 - val_loss: 0.9232 - val_acc: 0.6320\n",
            "Epoch 147/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0063 - acc: 0.5469\n",
            "Epoch 147: val_loss improved from 0.92322 to 0.92199, saving model to Best_model_epoch_147_val_loss_0.9220.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8993 - acc: 0.6800 - val_loss: 0.9220 - val_acc: 0.6360\n",
            "Epoch 148/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9658 - acc: 0.6250\n",
            "Epoch 148: val_loss improved from 0.92199 to 0.92070, saving model to Best_model_epoch_148_val_loss_0.9207.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.8978 - acc: 0.6853 - val_loss: 0.9207 - val_acc: 0.6320\n",
            "Epoch 149/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8347 - acc: 0.7188\n",
            "Epoch 149: val_loss improved from 0.92070 to 0.91976, saving model to Best_model_epoch_149_val_loss_0.9198.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8960 - acc: 0.6827 - val_loss: 0.9198 - val_acc: 0.6360\n",
            "Epoch 150/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8800 - acc: 0.6562\n",
            "Epoch 150: val_loss improved from 0.91976 to 0.91739, saving model to Best_model_epoch_150_val_loss_0.9174.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8943 - acc: 0.6893 - val_loss: 0.9174 - val_acc: 0.6320\n",
            "Epoch 151/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9049 - acc: 0.6562\n",
            "Epoch 151: val_loss improved from 0.91739 to 0.91574, saving model to Best_model_epoch_151_val_loss_0.9157.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.8928 - acc: 0.6840 - val_loss: 0.9157 - val_acc: 0.6320\n",
            "Epoch 152/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9530 - acc: 0.5781\n",
            "Epoch 152: val_loss improved from 0.91574 to 0.91438, saving model to Best_model_epoch_152_val_loss_0.9144.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8916 - acc: 0.6880 - val_loss: 0.9144 - val_acc: 0.6440\n",
            "Epoch 153/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0524 - acc: 0.5469\n",
            "Epoch 153: val_loss improved from 0.91438 to 0.91299, saving model to Best_model_epoch_153_val_loss_0.9130.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8899 - acc: 0.6840 - val_loss: 0.9130 - val_acc: 0.6360\n",
            "Epoch 154/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8887 - acc: 0.6867\n",
            "Epoch 154: val_loss improved from 0.91299 to 0.91087, saving model to Best_model_epoch_154_val_loss_0.9109.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8887 - acc: 0.6867 - val_loss: 0.9109 - val_acc: 0.6320\n",
            "Epoch 155/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8802 - acc: 0.6984\n",
            "Epoch 155: val_loss improved from 0.91087 to 0.91020, saving model to Best_model_epoch_155_val_loss_0.9102.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8860 - acc: 0.6880 - val_loss: 0.9102 - val_acc: 0.6400\n",
            "Epoch 156/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.8777 - acc: 0.7031\n",
            "Epoch 156: val_loss improved from 0.91020 to 0.90832, saving model to Best_model_epoch_156_val_loss_0.9083.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8843 - acc: 0.6920 - val_loss: 0.9083 - val_acc: 0.6400\n",
            "Epoch 157/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.8778 - acc: 0.6875\n",
            "Epoch 157: val_loss improved from 0.90832 to 0.90632, saving model to Best_model_epoch_157_val_loss_0.9063.keras\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.8826 - acc: 0.6893 - val_loss: 0.9063 - val_acc: 0.6360\n",
            "Epoch 158/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8810 - acc: 0.6880\n",
            "Epoch 158: val_loss improved from 0.90632 to 0.90445, saving model to Best_model_epoch_158_val_loss_0.9045.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8810 - acc: 0.6880 - val_loss: 0.9045 - val_acc: 0.6400\n",
            "Epoch 159/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.8867 - acc: 0.6832\n",
            "Epoch 159: val_loss improved from 0.90445 to 0.90287, saving model to Best_model_epoch_159_val_loss_0.9029.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.8824 - acc: 0.6867 - val_loss: 0.9029 - val_acc: 0.6400\n",
            "Epoch 160/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.8860 - acc: 0.6892\n",
            "Epoch 160: val_loss improved from 0.90287 to 0.90121, saving model to Best_model_epoch_160_val_loss_0.9012.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8769 - acc: 0.6947 - val_loss: 0.9012 - val_acc: 0.6520\n",
            "Epoch 161/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8643 - acc: 0.6891\n",
            "Epoch 161: val_loss improved from 0.90121 to 0.89870, saving model to Best_model_epoch_161_val_loss_0.8987.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8753 - acc: 0.6907 - val_loss: 0.8987 - val_acc: 0.6480\n",
            "Epoch 162/1000\n",
            " 7/12 [================>.............] - ETA: 0s - loss: 0.8680 - acc: 0.6964\n",
            "Epoch 162: val_loss improved from 0.89870 to 0.89682, saving model to Best_model_epoch_162_val_loss_0.8968.keras\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.8741 - acc: 0.6933 - val_loss: 0.8968 - val_acc: 0.6480\n",
            "Epoch 163/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.8760 - acc: 0.6889\n",
            "Epoch 163: val_loss improved from 0.89682 to 0.89613, saving model to Best_model_epoch_163_val_loss_0.8961.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.8724 - acc: 0.6920 - val_loss: 0.8961 - val_acc: 0.6400\n",
            "Epoch 164/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8675 - acc: 0.7016\n",
            "Epoch 164: val_loss improved from 0.89613 to 0.89381, saving model to Best_model_epoch_164_val_loss_0.8938.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.8698 - acc: 0.6960 - val_loss: 0.8938 - val_acc: 0.6480\n",
            "Epoch 165/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.8768 - acc: 0.6840\n",
            "Epoch 165: val_loss improved from 0.89381 to 0.89279, saving model to Best_model_epoch_165_val_loss_0.8928.keras\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.8687 - acc: 0.6893 - val_loss: 0.8928 - val_acc: 0.6480\n",
            "Epoch 166/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8690 - acc: 0.6922\n",
            "Epoch 166: val_loss improved from 0.89279 to 0.89126, saving model to Best_model_epoch_166_val_loss_0.8913.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.8667 - acc: 0.6933 - val_loss: 0.8913 - val_acc: 0.6480\n",
            "Epoch 167/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.8630 - acc: 0.6962\n",
            "Epoch 167: val_loss improved from 0.89126 to 0.88988, saving model to Best_model_epoch_167_val_loss_0.8899.keras\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.8648 - acc: 0.6973 - val_loss: 0.8899 - val_acc: 0.6520\n",
            "Epoch 168/1000\n",
            " 7/12 [================>.............] - ETA: 0s - loss: 0.8663 - acc: 0.6897\n",
            "Epoch 168: val_loss improved from 0.88988 to 0.88702, saving model to Best_model_epoch_168_val_loss_0.8870.keras\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 0.8632 - acc: 0.6973 - val_loss: 0.8870 - val_acc: 0.6520\n",
            "Epoch 169/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8789 - acc: 0.6953\n",
            "Epoch 169: val_loss improved from 0.88702 to 0.88598, saving model to Best_model_epoch_169_val_loss_0.8860.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.8616 - acc: 0.7013 - val_loss: 0.8860 - val_acc: 0.6680\n",
            "Epoch 170/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.8739 - acc: 0.6836\n",
            "Epoch 170: val_loss improved from 0.88598 to 0.88572, saving model to Best_model_epoch_170_val_loss_0.8857.keras\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.8597 - acc: 0.6960 - val_loss: 0.8857 - val_acc: 0.6520\n",
            "Epoch 171/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8485 - acc: 0.7344\n",
            "Epoch 171: val_loss improved from 0.88572 to 0.88283, saving model to Best_model_epoch_171_val_loss_0.8828.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.8587 - acc: 0.6987 - val_loss: 0.8828 - val_acc: 0.6560\n",
            "Epoch 172/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8339 - acc: 0.7078\n",
            "Epoch 172: val_loss improved from 0.88283 to 0.87994, saving model to Best_model_epoch_172_val_loss_0.8799.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.8563 - acc: 0.6947 - val_loss: 0.8799 - val_acc: 0.6640\n",
            "Epoch 173/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8712 - acc: 0.6562\n",
            "Epoch 173: val_loss improved from 0.87994 to 0.87861, saving model to Best_model_epoch_173_val_loss_0.8786.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.8543 - acc: 0.6987 - val_loss: 0.8786 - val_acc: 0.6640\n",
            "Epoch 174/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8541 - acc: 0.7013\n",
            "Epoch 174: val_loss improved from 0.87861 to 0.87858, saving model to Best_model_epoch_174_val_loss_0.8786.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.8541 - acc: 0.7013 - val_loss: 0.8786 - val_acc: 0.6560\n",
            "Epoch 175/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7441 - acc: 0.7188\n",
            "Epoch 175: val_loss improved from 0.87858 to 0.87551, saving model to Best_model_epoch_175_val_loss_0.8755.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8509 - acc: 0.7053 - val_loss: 0.8755 - val_acc: 0.6720\n",
            "Epoch 176/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8508 - acc: 0.6987\n",
            "Epoch 176: val_loss improved from 0.87551 to 0.87330, saving model to Best_model_epoch_176_val_loss_0.8733.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.8508 - acc: 0.6987 - val_loss: 0.8733 - val_acc: 0.6680\n",
            "Epoch 177/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8475 - acc: 0.7040\n",
            "Epoch 177: val_loss did not improve from 0.87330\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.8475 - acc: 0.7040 - val_loss: 0.8734 - val_acc: 0.6640\n",
            "Epoch 178/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7814 - acc: 0.7500\n",
            "Epoch 178: val_loss improved from 0.87330 to 0.87075, saving model to Best_model_epoch_178_val_loss_0.8707.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.8463 - acc: 0.7013 - val_loss: 0.8707 - val_acc: 0.6680\n",
            "Epoch 179/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7582 - acc: 0.7656\n",
            "Epoch 179: val_loss improved from 0.87075 to 0.86885, saving model to Best_model_epoch_179_val_loss_0.8689.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8444 - acc: 0.7053 - val_loss: 0.8689 - val_acc: 0.6680\n",
            "Epoch 180/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7489 - acc: 0.7812\n",
            "Epoch 180: val_loss improved from 0.86885 to 0.86688, saving model to Best_model_epoch_180_val_loss_0.8669.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.8421 - acc: 0.7067 - val_loss: 0.8669 - val_acc: 0.6720\n",
            "Epoch 181/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9007 - acc: 0.6875\n",
            "Epoch 181: val_loss improved from 0.86688 to 0.86536, saving model to Best_model_epoch_181_val_loss_0.8654.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8409 - acc: 0.7080 - val_loss: 0.8654 - val_acc: 0.6720\n",
            "Epoch 182/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.8478 - acc: 0.6962\n",
            "Epoch 182: val_loss improved from 0.86536 to 0.86401, saving model to Best_model_epoch_182_val_loss_0.8640.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.8390 - acc: 0.7067 - val_loss: 0.8640 - val_acc: 0.6720\n",
            "Epoch 183/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7350 - acc: 0.7812\n",
            "Epoch 183: val_loss improved from 0.86401 to 0.86310, saving model to Best_model_epoch_183_val_loss_0.8631.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8373 - acc: 0.7067 - val_loss: 0.8631 - val_acc: 0.6680\n",
            "Epoch 184/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8439 - acc: 0.7344\n",
            "Epoch 184: val_loss improved from 0.86310 to 0.86097, saving model to Best_model_epoch_184_val_loss_0.8610.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.8357 - acc: 0.7107 - val_loss: 0.8610 - val_acc: 0.6760\n",
            "Epoch 185/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9518 - acc: 0.6875\n",
            "Epoch 185: val_loss improved from 0.86097 to 0.85859, saving model to Best_model_epoch_185_val_loss_0.8586.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8343 - acc: 0.7080 - val_loss: 0.8586 - val_acc: 0.6680\n",
            "Epoch 186/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7906 - acc: 0.6719\n",
            "Epoch 186: val_loss improved from 0.85859 to 0.85835, saving model to Best_model_epoch_186_val_loss_0.8584.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8332 - acc: 0.7040 - val_loss: 0.8584 - val_acc: 0.6720\n",
            "Epoch 187/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8314 - acc: 0.7141\n",
            "Epoch 187: val_loss improved from 0.85835 to 0.85606, saving model to Best_model_epoch_187_val_loss_0.8561.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.8314 - acc: 0.7107 - val_loss: 0.8561 - val_acc: 0.6760\n",
            "Epoch 188/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9293 - acc: 0.7031\n",
            "Epoch 188: val_loss improved from 0.85606 to 0.85388, saving model to Best_model_epoch_188_val_loss_0.8539.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8297 - acc: 0.7080 - val_loss: 0.8539 - val_acc: 0.6720\n",
            "Epoch 189/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7349 - acc: 0.7812\n",
            "Epoch 189: val_loss improved from 0.85388 to 0.85341, saving model to Best_model_epoch_189_val_loss_0.8534.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.8276 - acc: 0.7120 - val_loss: 0.8534 - val_acc: 0.6760\n",
            "Epoch 190/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9368 - acc: 0.5938\n",
            "Epoch 190: val_loss improved from 0.85341 to 0.85234, saving model to Best_model_epoch_190_val_loss_0.8523.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.8266 - acc: 0.7147 - val_loss: 0.8523 - val_acc: 0.6680\n",
            "Epoch 191/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8287 - acc: 0.7500\n",
            "Epoch 191: val_loss improved from 0.85234 to 0.84969, saving model to Best_model_epoch_191_val_loss_0.8497.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8251 - acc: 0.7160 - val_loss: 0.8497 - val_acc: 0.6760\n",
            "Epoch 192/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8144 - acc: 0.7219\n",
            "Epoch 192: val_loss did not improve from 0.84969\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.8237 - acc: 0.7133 - val_loss: 0.8500 - val_acc: 0.6720\n",
            "Epoch 193/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9664 - acc: 0.6719\n",
            "Epoch 193: val_loss improved from 0.84969 to 0.84699, saving model to Best_model_epoch_193_val_loss_0.8470.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.8220 - acc: 0.7120 - val_loss: 0.8470 - val_acc: 0.6720\n",
            "Epoch 194/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7897 - acc: 0.6719\n",
            "Epoch 194: val_loss improved from 0.84699 to 0.84460, saving model to Best_model_epoch_194_val_loss_0.8446.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8214 - acc: 0.7093 - val_loss: 0.8446 - val_acc: 0.6720\n",
            "Epoch 195/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7654 - acc: 0.8125\n",
            "Epoch 195: val_loss improved from 0.84460 to 0.84375, saving model to Best_model_epoch_195_val_loss_0.8437.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8187 - acc: 0.7133 - val_loss: 0.8437 - val_acc: 0.6720\n",
            "Epoch 196/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7576 - acc: 0.7656\n",
            "Epoch 196: val_loss did not improve from 0.84375\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.8192 - acc: 0.7133 - val_loss: 0.8439 - val_acc: 0.6800\n",
            "Epoch 197/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8166 - acc: 0.7160\n",
            "Epoch 197: val_loss improved from 0.84375 to 0.84176, saving model to Best_model_epoch_197_val_loss_0.8418.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.8166 - acc: 0.7160 - val_loss: 0.8418 - val_acc: 0.6680\n",
            "Epoch 198/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.8120 - acc: 0.7173\n",
            "Epoch 198: val_loss improved from 0.84176 to 0.83931, saving model to Best_model_epoch_198_val_loss_0.8393.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.8157 - acc: 0.7133 - val_loss: 0.8393 - val_acc: 0.6880\n",
            "Epoch 199/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8144 - acc: 0.7141\n",
            "Epoch 199: val_loss improved from 0.83931 to 0.83834, saving model to Best_model_epoch_199_val_loss_0.8383.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.8128 - acc: 0.7160 - val_loss: 0.8383 - val_acc: 0.6800\n",
            "Epoch 200/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0300 - acc: 0.5625\n",
            "Epoch 200: val_loss did not improve from 0.83834\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.8121 - acc: 0.7080 - val_loss: 0.8385 - val_acc: 0.6760\n",
            "Epoch 201/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7896 - acc: 0.7344\n",
            "Epoch 201: val_loss improved from 0.83834 to 0.83762, saving model to Best_model_epoch_201_val_loss_0.8376.keras\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.8104 - acc: 0.7200 - val_loss: 0.8376 - val_acc: 0.6840\n",
            "Epoch 202/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7450 - acc: 0.7344\n",
            "Epoch 202: val_loss improved from 0.83762 to 0.83404, saving model to Best_model_epoch_202_val_loss_0.8340.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8096 - acc: 0.7133 - val_loss: 0.8340 - val_acc: 0.6840\n",
            "Epoch 203/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8870 - acc: 0.6719\n",
            "Epoch 203: val_loss improved from 0.83404 to 0.83379, saving model to Best_model_epoch_203_val_loss_0.8338.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8092 - acc: 0.7187 - val_loss: 0.8338 - val_acc: 0.6960\n",
            "Epoch 204/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6868 - acc: 0.7656\n",
            "Epoch 204: val_loss did not improve from 0.83379\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.8068 - acc: 0.7160 - val_loss: 0.8342 - val_acc: 0.6880\n",
            "Epoch 205/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.8052 - acc: 0.7202\n",
            "Epoch 205: val_loss improved from 0.83379 to 0.83128, saving model to Best_model_epoch_205_val_loss_0.8313.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.8063 - acc: 0.7187 - val_loss: 0.8313 - val_acc: 0.6840\n",
            "Epoch 206/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6950 - acc: 0.7656\n",
            "Epoch 206: val_loss improved from 0.83128 to 0.83024, saving model to Best_model_epoch_206_val_loss_0.8302.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8036 - acc: 0.7173 - val_loss: 0.8302 - val_acc: 0.6960\n",
            "Epoch 207/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.8027 - acc: 0.7200\n",
            "Epoch 207: val_loss improved from 0.83024 to 0.82846, saving model to Best_model_epoch_207_val_loss_0.8285.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.8027 - acc: 0.7200 - val_loss: 0.8285 - val_acc: 0.6920\n",
            "Epoch 208/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9225 - acc: 0.6094\n",
            "Epoch 208: val_loss did not improve from 0.82846\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.8021 - acc: 0.7187 - val_loss: 0.8287 - val_acc: 0.6920\n",
            "Epoch 209/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.8163 - acc: 0.7031\n",
            "Epoch 209: val_loss improved from 0.82846 to 0.82580, saving model to Best_model_epoch_209_val_loss_0.8258.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.8000 - acc: 0.7187 - val_loss: 0.8258 - val_acc: 0.6880\n",
            "Epoch 210/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8212 - acc: 0.7188\n",
            "Epoch 210: val_loss improved from 0.82580 to 0.82390, saving model to Best_model_epoch_210_val_loss_0.8239.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.7997 - acc: 0.7160 - val_loss: 0.8239 - val_acc: 0.6840\n",
            "Epoch 211/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7589 - acc: 0.7344\n",
            "Epoch 211: val_loss did not improve from 0.82390\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7981 - acc: 0.7147 - val_loss: 0.8250 - val_acc: 0.6880\n",
            "Epoch 212/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7969 - acc: 0.7187\n",
            "Epoch 212: val_loss improved from 0.82390 to 0.82316, saving model to Best_model_epoch_212_val_loss_0.8232.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7969 - acc: 0.7187 - val_loss: 0.8232 - val_acc: 0.6920\n",
            "Epoch 213/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7953 - acc: 0.7200\n",
            "Epoch 213: val_loss improved from 0.82316 to 0.82102, saving model to Best_model_epoch_213_val_loss_0.8210.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7953 - acc: 0.7200 - val_loss: 0.8210 - val_acc: 0.6920\n",
            "Epoch 214/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8114 - acc: 0.7344\n",
            "Epoch 214: val_loss improved from 0.82102 to 0.82012, saving model to Best_model_epoch_214_val_loss_0.8201.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7945 - acc: 0.7200 - val_loss: 0.8201 - val_acc: 0.6840\n",
            "Epoch 215/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6975 - acc: 0.7344\n",
            "Epoch 215: val_loss did not improve from 0.82012\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7934 - acc: 0.7173 - val_loss: 0.8202 - val_acc: 0.6920\n",
            "Epoch 216/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6361 - acc: 0.7656\n",
            "Epoch 216: val_loss improved from 0.82012 to 0.81837, saving model to Best_model_epoch_216_val_loss_0.8184.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7947 - acc: 0.7213 - val_loss: 0.8184 - val_acc: 0.7040\n",
            "Epoch 217/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7433 - acc: 0.7500\n",
            "Epoch 217: val_loss improved from 0.81837 to 0.81756, saving model to Best_model_epoch_217_val_loss_0.8176.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7904 - acc: 0.7173 - val_loss: 0.8176 - val_acc: 0.6920\n",
            "Epoch 218/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6430 - acc: 0.8438\n",
            "Epoch 218: val_loss improved from 0.81756 to 0.81614, saving model to Best_model_epoch_218_val_loss_0.8161.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7897 - acc: 0.7253 - val_loss: 0.8161 - val_acc: 0.6880\n",
            "Epoch 219/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8077 - acc: 0.7344\n",
            "Epoch 219: val_loss improved from 0.81614 to 0.81575, saving model to Best_model_epoch_219_val_loss_0.8158.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7888 - acc: 0.7200 - val_loss: 0.8158 - val_acc: 0.7080\n",
            "Epoch 220/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6868 - acc: 0.7969\n",
            "Epoch 220: val_loss improved from 0.81575 to 0.81420, saving model to Best_model_epoch_220_val_loss_0.8142.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7875 - acc: 0.7213 - val_loss: 0.8142 - val_acc: 0.7000\n",
            "Epoch 221/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7772 - acc: 0.7219\n",
            "Epoch 221: val_loss improved from 0.81420 to 0.81332, saving model to Best_model_epoch_221_val_loss_0.8133.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7862 - acc: 0.7227 - val_loss: 0.8133 - val_acc: 0.6880\n",
            "Epoch 222/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7851 - acc: 0.7253\n",
            "Epoch 222: val_loss improved from 0.81332 to 0.81302, saving model to Best_model_epoch_222_val_loss_0.8130.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.7851 - acc: 0.7253 - val_loss: 0.8130 - val_acc: 0.7040\n",
            "Epoch 223/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7837 - acc: 0.7330\n",
            "Epoch 223: val_loss improved from 0.81302 to 0.81181, saving model to Best_model_epoch_223_val_loss_0.8118.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7838 - acc: 0.7280 - val_loss: 0.8118 - val_acc: 0.7040\n",
            "Epoch 224/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7861 - acc: 0.7135\n",
            "Epoch 224: val_loss improved from 0.81181 to 0.81047, saving model to Best_model_epoch_224_val_loss_0.8105.keras\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.7830 - acc: 0.7253 - val_loss: 0.8105 - val_acc: 0.6960\n",
            "Epoch 225/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7889 - acc: 0.7250\n",
            "Epoch 225: val_loss improved from 0.81047 to 0.80968, saving model to Best_model_epoch_225_val_loss_0.8097.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7818 - acc: 0.7280 - val_loss: 0.8097 - val_acc: 0.7080\n",
            "Epoch 226/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.8055 - acc: 0.7090\n",
            "Epoch 226: val_loss did not improve from 0.80968\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7809 - acc: 0.7227 - val_loss: 0.8098 - val_acc: 0.7040\n",
            "Epoch 227/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7758 - acc: 0.7297\n",
            "Epoch 227: val_loss improved from 0.80968 to 0.80822, saving model to Best_model_epoch_227_val_loss_0.8082.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.7799 - acc: 0.7267 - val_loss: 0.8082 - val_acc: 0.7000\n",
            "Epoch 228/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7797 - acc: 0.7219\n",
            "Epoch 228: val_loss did not improve from 0.80822\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7787 - acc: 0.7267 - val_loss: 0.8083 - val_acc: 0.7040\n",
            "Epoch 229/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7803 - acc: 0.7326\n",
            "Epoch 229: val_loss improved from 0.80822 to 0.80491, saving model to Best_model_epoch_229_val_loss_0.8049.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.7777 - acc: 0.7280 - val_loss: 0.8049 - val_acc: 0.7040\n",
            "Epoch 230/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7768 - acc: 0.7240\n",
            "Epoch 230: val_loss improved from 0.80491 to 0.80419, saving model to Best_model_epoch_230_val_loss_0.8042.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7768 - acc: 0.7240 - val_loss: 0.8042 - val_acc: 0.7040\n",
            "Epoch 231/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7806 - acc: 0.7301\n",
            "Epoch 231: val_loss improved from 0.80419 to 0.80396, saving model to Best_model_epoch_231_val_loss_0.8040.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.7762 - acc: 0.7293 - val_loss: 0.8040 - val_acc: 0.7040\n",
            "Epoch 232/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7673 - acc: 0.7274\n",
            "Epoch 232: val_loss did not improve from 0.80396\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7750 - acc: 0.7253 - val_loss: 0.8047 - val_acc: 0.6960\n",
            "Epoch 233/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7748 - acc: 0.7274\n",
            "Epoch 233: val_loss improved from 0.80396 to 0.80272, saving model to Best_model_epoch_233_val_loss_0.8027.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.7742 - acc: 0.7213 - val_loss: 0.8027 - val_acc: 0.7160\n",
            "Epoch 234/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7631 - acc: 0.7257\n",
            "Epoch 234: val_loss improved from 0.80272 to 0.80251, saving model to Best_model_epoch_234_val_loss_0.8025.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.7758 - acc: 0.7240 - val_loss: 0.8025 - val_acc: 0.6960\n",
            "Epoch 235/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7567 - acc: 0.7344\n",
            "Epoch 235: val_loss improved from 0.80251 to 0.80089, saving model to Best_model_epoch_235_val_loss_0.8009.keras\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.7720 - acc: 0.7253 - val_loss: 0.8009 - val_acc: 0.7040\n",
            "Epoch 236/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7406 - acc: 0.7396\n",
            "Epoch 236: val_loss improved from 0.80089 to 0.79906, saving model to Best_model_epoch_236_val_loss_0.7991.keras\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.7710 - acc: 0.7267 - val_loss: 0.7991 - val_acc: 0.7120\n",
            "Epoch 237/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7504 - acc: 0.7344\n",
            "Epoch 237: val_loss did not improve from 0.79906\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7698 - acc: 0.7267 - val_loss: 0.7991 - val_acc: 0.7120\n",
            "Epoch 238/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.7619 - acc: 0.7227\n",
            "Epoch 238: val_loss improved from 0.79906 to 0.79760, saving model to Best_model_epoch_238_val_loss_0.7976.keras\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.7695 - acc: 0.7227 - val_loss: 0.7976 - val_acc: 0.7040\n",
            "Epoch 239/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7644 - acc: 0.7266\n",
            "Epoch 239: val_loss did not improve from 0.79760\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7684 - acc: 0.7253 - val_loss: 0.7976 - val_acc: 0.7120\n",
            "Epoch 240/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7658 - acc: 0.7344\n",
            "Epoch 240: val_loss did not improve from 0.79760\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7679 - acc: 0.7280 - val_loss: 0.7984 - val_acc: 0.7120\n",
            "Epoch 241/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7554 - acc: 0.7358\n",
            "Epoch 241: val_loss improved from 0.79760 to 0.79649, saving model to Best_model_epoch_241_val_loss_0.7965.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.7665 - acc: 0.7293 - val_loss: 0.7965 - val_acc: 0.7160\n",
            "Epoch 242/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7635 - acc: 0.7188\n",
            "Epoch 242: val_loss improved from 0.79649 to 0.79523, saving model to Best_model_epoch_242_val_loss_0.7952.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.7656 - acc: 0.7227 - val_loss: 0.7952 - val_acc: 0.7160\n",
            "Epoch 243/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7600 - acc: 0.7281\n",
            "Epoch 243: val_loss improved from 0.79523 to 0.79392, saving model to Best_model_epoch_243_val_loss_0.7939.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7646 - acc: 0.7280 - val_loss: 0.7939 - val_acc: 0.7200\n",
            "Epoch 244/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7640 - acc: 0.7293\n",
            "Epoch 244: val_loss improved from 0.79392 to 0.79347, saving model to Best_model_epoch_244_val_loss_0.7935.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7640 - acc: 0.7293 - val_loss: 0.7935 - val_acc: 0.7160\n",
            "Epoch 245/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9225 - acc: 0.7031\n",
            "Epoch 245: val_loss improved from 0.79347 to 0.79180, saving model to Best_model_epoch_245_val_loss_0.7918.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7626 - acc: 0.7240 - val_loss: 0.7918 - val_acc: 0.7240\n",
            "Epoch 246/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7627 - acc: 0.7333\n",
            "Epoch 246: val_loss did not improve from 0.79180\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7627 - acc: 0.7333 - val_loss: 0.7935 - val_acc: 0.7120\n",
            "Epoch 247/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7594 - acc: 0.7259\n",
            "Epoch 247: val_loss did not improve from 0.79180\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7622 - acc: 0.7293 - val_loss: 0.7920 - val_acc: 0.7240\n",
            "Epoch 248/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7608 - acc: 0.7320\n",
            "Epoch 248: val_loss improved from 0.79180 to 0.79110, saving model to Best_model_epoch_248_val_loss_0.7911.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7608 - acc: 0.7320 - val_loss: 0.7911 - val_acc: 0.7160\n",
            "Epoch 249/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7593 - acc: 0.7267\n",
            "Epoch 249: val_loss improved from 0.79110 to 0.78962, saving model to Best_model_epoch_249_val_loss_0.7896.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7593 - acc: 0.7267 - val_loss: 0.7896 - val_acc: 0.7160\n",
            "Epoch 250/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7700 - acc: 0.7656\n",
            "Epoch 250: val_loss improved from 0.78962 to 0.78865, saving model to Best_model_epoch_250_val_loss_0.7887.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7585 - acc: 0.7267 - val_loss: 0.7887 - val_acc: 0.7200\n",
            "Epoch 251/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8316 - acc: 0.7031\n",
            "Epoch 251: val_loss did not improve from 0.78865\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7581 - acc: 0.7307 - val_loss: 0.7892 - val_acc: 0.7200\n",
            "Epoch 252/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7567 - acc: 0.7307\n",
            "Epoch 252: val_loss improved from 0.78865 to 0.78751, saving model to Best_model_epoch_252_val_loss_0.7875.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7567 - acc: 0.7307 - val_loss: 0.7875 - val_acc: 0.7240\n",
            "Epoch 253/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7533 - acc: 0.7244\n",
            "Epoch 253: val_loss improved from 0.78751 to 0.78686, saving model to Best_model_epoch_253_val_loss_0.7869.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7563 - acc: 0.7253 - val_loss: 0.7869 - val_acc: 0.7240\n",
            "Epoch 254/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7582 - acc: 0.7344\n",
            "Epoch 254: val_loss did not improve from 0.78686\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7556 - acc: 0.7293 - val_loss: 0.7872 - val_acc: 0.7240\n",
            "Epoch 255/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7542 - acc: 0.7320\n",
            "Epoch 255: val_loss improved from 0.78686 to 0.78605, saving model to Best_model_epoch_255_val_loss_0.7861.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7542 - acc: 0.7320 - val_loss: 0.7861 - val_acc: 0.7280\n",
            "Epoch 256/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7530 - acc: 0.7307\n",
            "Epoch 256: val_loss improved from 0.78605 to 0.78460, saving model to Best_model_epoch_256_val_loss_0.7846.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7530 - acc: 0.7307 - val_loss: 0.7846 - val_acc: 0.7240\n",
            "Epoch 257/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7528 - acc: 0.7293\n",
            "Epoch 257: val_loss improved from 0.78460 to 0.78358, saving model to Best_model_epoch_257_val_loss_0.7836.keras\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.7528 - acc: 0.7293 - val_loss: 0.7836 - val_acc: 0.7240\n",
            "Epoch 258/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7460 - acc: 0.7386\n",
            "Epoch 258: val_loss improved from 0.78358 to 0.78285, saving model to Best_model_epoch_258_val_loss_0.7828.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7519 - acc: 0.7360 - val_loss: 0.7828 - val_acc: 0.7280\n",
            "Epoch 259/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7638 - acc: 0.7312\n",
            "Epoch 259: val_loss did not improve from 0.78285\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7515 - acc: 0.7333 - val_loss: 0.7837 - val_acc: 0.7240\n",
            "Epoch 260/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7574 - acc: 0.7244\n",
            "Epoch 260: val_loss did not improve from 0.78285\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7506 - acc: 0.7293 - val_loss: 0.7833 - val_acc: 0.7240\n",
            "Epoch 261/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7494 - acc: 0.7333\n",
            "Epoch 261: val_loss improved from 0.78285 to 0.78165, saving model to Best_model_epoch_261_val_loss_0.7816.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7494 - acc: 0.7333 - val_loss: 0.7816 - val_acc: 0.7320\n",
            "Epoch 262/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7489 - acc: 0.7320\n",
            "Epoch 262: val_loss did not improve from 0.78165\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7489 - acc: 0.7320 - val_loss: 0.7825 - val_acc: 0.7280\n",
            "Epoch 263/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7485 - acc: 0.7320\n",
            "Epoch 263: val_loss improved from 0.78165 to 0.77924, saving model to Best_model_epoch_263_val_loss_0.7792.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7485 - acc: 0.7320 - val_loss: 0.7792 - val_acc: 0.7320\n",
            "Epoch 264/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7615 - acc: 0.7188\n",
            "Epoch 264: val_loss did not improve from 0.77924\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7470 - acc: 0.7267 - val_loss: 0.7794 - val_acc: 0.7280\n",
            "Epoch 265/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7457 - acc: 0.7406\n",
            "Epoch 265: val_loss did not improve from 0.77924\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7464 - acc: 0.7333 - val_loss: 0.7809 - val_acc: 0.7280\n",
            "Epoch 266/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7536 - acc: 0.7301\n",
            "Epoch 266: val_loss did not improve from 0.77924\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7467 - acc: 0.7320 - val_loss: 0.7800 - val_acc: 0.7200\n",
            "Epoch 267/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8729 - acc: 0.6562\n",
            "Epoch 267: val_loss improved from 0.77924 to 0.77818, saving model to Best_model_epoch_267_val_loss_0.7782.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7456 - acc: 0.7320 - val_loss: 0.7782 - val_acc: 0.7280\n",
            "Epoch 268/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7446 - acc: 0.7360\n",
            "Epoch 268: val_loss improved from 0.77818 to 0.77684, saving model to Best_model_epoch_268_val_loss_0.7768.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7446 - acc: 0.7360 - val_loss: 0.7768 - val_acc: 0.7280\n",
            "Epoch 269/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6163 - acc: 0.6875\n",
            "Epoch 269: val_loss improved from 0.77684 to 0.77577, saving model to Best_model_epoch_269_val_loss_0.7758.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7437 - acc: 0.7320 - val_loss: 0.7758 - val_acc: 0.7240\n",
            "Epoch 270/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7957 - acc: 0.7500\n",
            "Epoch 270: val_loss did not improve from 0.77577\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7430 - acc: 0.7320 - val_loss: 0.7769 - val_acc: 0.7280\n",
            "Epoch 271/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5565 - acc: 0.8281\n",
            "Epoch 271: val_loss improved from 0.77577 to 0.77493, saving model to Best_model_epoch_271_val_loss_0.7749.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.7424 - acc: 0.7320 - val_loss: 0.7749 - val_acc: 0.7280\n",
            "Epoch 272/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5823 - acc: 0.8438\n",
            "Epoch 272: val_loss improved from 0.77493 to 0.77392, saving model to Best_model_epoch_272_val_loss_0.7739.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7427 - acc: 0.7360 - val_loss: 0.7739 - val_acc: 0.7320\n",
            "Epoch 273/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7406 - acc: 0.7373\n",
            "Epoch 273: val_loss did not improve from 0.77392\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7406 - acc: 0.7373 - val_loss: 0.7744 - val_acc: 0.7280\n",
            "Epoch 274/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5825 - acc: 0.8125\n",
            "Epoch 274: val_loss improved from 0.77392 to 0.77380, saving model to Best_model_epoch_274_val_loss_0.7738.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7406 - acc: 0.7427 - val_loss: 0.7738 - val_acc: 0.7320\n",
            "Epoch 275/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7431 - acc: 0.7358\n",
            "Epoch 275: val_loss did not improve from 0.77380\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7400 - acc: 0.7347 - val_loss: 0.7748 - val_acc: 0.7280\n",
            "Epoch 276/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7347 - acc: 0.7422\n",
            "Epoch 276: val_loss did not improve from 0.77380\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.7388 - acc: 0.7413 - val_loss: 0.7745 - val_acc: 0.7240\n",
            "Epoch 277/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6963 - acc: 0.7812\n",
            "Epoch 277: val_loss improved from 0.77380 to 0.77305, saving model to Best_model_epoch_277_val_loss_0.7730.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7381 - acc: 0.7373 - val_loss: 0.7730 - val_acc: 0.7280\n",
            "Epoch 278/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7229 - acc: 0.7528\n",
            "Epoch 278: val_loss improved from 0.77305 to 0.77235, saving model to Best_model_epoch_278_val_loss_0.7723.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7377 - acc: 0.7413 - val_loss: 0.7723 - val_acc: 0.7280\n",
            "Epoch 279/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6962 - acc: 0.8125\n",
            "Epoch 279: val_loss did not improve from 0.77235\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7379 - acc: 0.7440 - val_loss: 0.7730 - val_acc: 0.7320\n",
            "Epoch 280/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7502 - acc: 0.7344\n",
            "Epoch 280: val_loss improved from 0.77235 to 0.77132, saving model to Best_model_epoch_280_val_loss_0.7713.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7356 - acc: 0.7400 - val_loss: 0.7713 - val_acc: 0.7280\n",
            "Epoch 281/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7356 - acc: 0.7427\n",
            "Epoch 281: val_loss improved from 0.77132 to 0.77062, saving model to Best_model_epoch_281_val_loss_0.7706.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7356 - acc: 0.7427 - val_loss: 0.7706 - val_acc: 0.7280\n",
            "Epoch 282/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9392 - acc: 0.6875\n",
            "Epoch 282: val_loss did not improve from 0.77062\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7358 - acc: 0.7440 - val_loss: 0.7712 - val_acc: 0.7240\n",
            "Epoch 283/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5593 - acc: 0.8125\n",
            "Epoch 283: val_loss improved from 0.77062 to 0.76903, saving model to Best_model_epoch_283_val_loss_0.7690.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7342 - acc: 0.7373 - val_loss: 0.7690 - val_acc: 0.7240\n",
            "Epoch 284/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7499 - acc: 0.7031\n",
            "Epoch 284: val_loss did not improve from 0.76903\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7331 - acc: 0.7413 - val_loss: 0.7699 - val_acc: 0.7280\n",
            "Epoch 285/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7552 - acc: 0.7375\n",
            "Epoch 285: val_loss did not improve from 0.76903\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7321 - acc: 0.7467 - val_loss: 0.7692 - val_acc: 0.7280\n",
            "Epoch 286/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8112 - acc: 0.6562\n",
            "Epoch 286: val_loss improved from 0.76903 to 0.76760, saving model to Best_model_epoch_286_val_loss_0.7676.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.7318 - acc: 0.7440 - val_loss: 0.7676 - val_acc: 0.7280\n",
            "Epoch 287/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7342 - acc: 0.7457\n",
            "Epoch 287: val_loss improved from 0.76760 to 0.76749, saving model to Best_model_epoch_287_val_loss_0.7675.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.7309 - acc: 0.7427 - val_loss: 0.7675 - val_acc: 0.7280\n",
            "Epoch 288/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7306 - acc: 0.7400\n",
            "Epoch 288: val_loss improved from 0.76749 to 0.76615, saving model to Best_model_epoch_288_val_loss_0.7662.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7306 - acc: 0.7400 - val_loss: 0.7662 - val_acc: 0.7320\n",
            "Epoch 289/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7169 - acc: 0.7500\n",
            "Epoch 289: val_loss did not improve from 0.76615\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7297 - acc: 0.7440 - val_loss: 0.7674 - val_acc: 0.7280\n",
            "Epoch 290/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6271 - acc: 0.7812\n",
            "Epoch 290: val_loss did not improve from 0.76615\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7304 - acc: 0.7427 - val_loss: 0.7673 - val_acc: 0.7280\n",
            "Epoch 291/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8313 - acc: 0.6875\n",
            "Epoch 291: val_loss improved from 0.76615 to 0.76545, saving model to Best_model_epoch_291_val_loss_0.7654.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7293 - acc: 0.7453 - val_loss: 0.7654 - val_acc: 0.7280\n",
            "Epoch 292/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7279 - acc: 0.7480\n",
            "Epoch 292: val_loss improved from 0.76545 to 0.76443, saving model to Best_model_epoch_292_val_loss_0.7644.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.7279 - acc: 0.7480 - val_loss: 0.7644 - val_acc: 0.7320\n",
            "Epoch 293/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7274 - acc: 0.7427\n",
            "Epoch 293: val_loss did not improve from 0.76443\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7274 - acc: 0.7427 - val_loss: 0.7646 - val_acc: 0.7240\n",
            "Epoch 294/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7219 - acc: 0.7457\n",
            "Epoch 294: val_loss improved from 0.76443 to 0.76423, saving model to Best_model_epoch_294_val_loss_0.7642.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7269 - acc: 0.7440 - val_loss: 0.7642 - val_acc: 0.7280\n",
            "Epoch 295/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7262 - acc: 0.7453\n",
            "Epoch 295: val_loss improved from 0.76423 to 0.76196, saving model to Best_model_epoch_295_val_loss_0.7620.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7262 - acc: 0.7453 - val_loss: 0.7620 - val_acc: 0.7280\n",
            "Epoch 296/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7362 - acc: 0.7358\n",
            "Epoch 296: val_loss did not improve from 0.76196\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7265 - acc: 0.7400 - val_loss: 0.7626 - val_acc: 0.7240\n",
            "Epoch 297/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7150 - acc: 0.7457\n",
            "Epoch 297: val_loss did not improve from 0.76196\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7246 - acc: 0.7453 - val_loss: 0.7623 - val_acc: 0.7320\n",
            "Epoch 298/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7247 - acc: 0.7453\n",
            "Epoch 298: val_loss did not improve from 0.76196\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.7247 - acc: 0.7453 - val_loss: 0.7638 - val_acc: 0.7240\n",
            "Epoch 299/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7165 - acc: 0.7517\n",
            "Epoch 299: val_loss improved from 0.76196 to 0.76147, saving model to Best_model_epoch_299_val_loss_0.7615.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.7233 - acc: 0.7507 - val_loss: 0.7615 - val_acc: 0.7280\n",
            "Epoch 300/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7265 - acc: 0.7457\n",
            "Epoch 300: val_loss improved from 0.76147 to 0.76027, saving model to Best_model_epoch_300_val_loss_0.7603.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.7229 - acc: 0.7480 - val_loss: 0.7603 - val_acc: 0.7320\n",
            "Epoch 301/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7272 - acc: 0.7500\n",
            "Epoch 301: val_loss did not improve from 0.76027\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7229 - acc: 0.7480 - val_loss: 0.7611 - val_acc: 0.7280\n",
            "Epoch 302/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7163 - acc: 0.7516\n",
            "Epoch 302: val_loss did not improve from 0.76027\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7224 - acc: 0.7467 - val_loss: 0.7614 - val_acc: 0.7240\n",
            "Epoch 303/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7250 - acc: 0.7437\n",
            "Epoch 303: val_loss did not improve from 0.76027\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.7216 - acc: 0.7453 - val_loss: 0.7604 - val_acc: 0.7280\n",
            "Epoch 304/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7215 - acc: 0.7543\n",
            "Epoch 304: val_loss improved from 0.76027 to 0.75846, saving model to Best_model_epoch_304_val_loss_0.7585.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.7215 - acc: 0.7520 - val_loss: 0.7585 - val_acc: 0.7320\n",
            "Epoch 305/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7229 - acc: 0.7429\n",
            "Epoch 305: val_loss did not improve from 0.75846\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7198 - acc: 0.7493 - val_loss: 0.7592 - val_acc: 0.7320\n",
            "Epoch 306/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7193 - acc: 0.7453\n",
            "Epoch 306: val_loss did not improve from 0.75846\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.7196 - acc: 0.7467 - val_loss: 0.7591 - val_acc: 0.7280\n",
            "Epoch 307/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7370 - acc: 0.7500\n",
            "Epoch 307: val_loss did not improve from 0.75846\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.7191 - acc: 0.7480 - val_loss: 0.7599 - val_acc: 0.7240\n",
            "Epoch 308/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7220 - acc: 0.7500\n",
            "Epoch 308: val_loss did not improve from 0.75846\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.7192 - acc: 0.7480 - val_loss: 0.7589 - val_acc: 0.7280\n",
            "Epoch 309/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.7204 - acc: 0.7578\n",
            "Epoch 309: val_loss improved from 0.75846 to 0.75736, saving model to Best_model_epoch_309_val_loss_0.7574.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.7183 - acc: 0.7520 - val_loss: 0.7574 - val_acc: 0.7280\n",
            "Epoch 310/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7181 - acc: 0.7440\n",
            "Epoch 310: val_loss improved from 0.75736 to 0.75690, saving model to Best_model_epoch_310_val_loss_0.7569.keras\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.7181 - acc: 0.7440 - val_loss: 0.7569 - val_acc: 0.7240\n",
            "Epoch 311/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7165 - acc: 0.7467\n",
            "Epoch 311: val_loss did not improve from 0.75690\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7165 - acc: 0.7467 - val_loss: 0.7570 - val_acc: 0.7280\n",
            "Epoch 312/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7340 - acc: 0.7483\n",
            "Epoch 312: val_loss did not improve from 0.75690\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.7170 - acc: 0.7493 - val_loss: 0.7577 - val_acc: 0.7240\n",
            "Epoch 313/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6965 - acc: 0.7622\n",
            "Epoch 313: val_loss improved from 0.75690 to 0.75486, saving model to Best_model_epoch_313_val_loss_0.7549.keras\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.7157 - acc: 0.7467 - val_loss: 0.7549 - val_acc: 0.7280\n",
            "Epoch 314/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7020 - acc: 0.7552\n",
            "Epoch 314: val_loss improved from 0.75486 to 0.75470, saving model to Best_model_epoch_314_val_loss_0.7547.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.7155 - acc: 0.7467 - val_loss: 0.7547 - val_acc: 0.7240\n",
            "Epoch 315/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7180 - acc: 0.7457\n",
            "Epoch 315: val_loss did not improve from 0.75470\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7148 - acc: 0.7467 - val_loss: 0.7552 - val_acc: 0.7280\n",
            "Epoch 316/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7160 - acc: 0.7500\n",
            "Epoch 316: val_loss improved from 0.75470 to 0.75432, saving model to Best_model_epoch_316_val_loss_0.7543.keras\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.7140 - acc: 0.7493 - val_loss: 0.7543 - val_acc: 0.7280\n",
            "Epoch 317/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.7039 - acc: 0.7539\n",
            "Epoch 317: val_loss did not improve from 0.75432\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.7135 - acc: 0.7507 - val_loss: 0.7550 - val_acc: 0.7280\n",
            "Epoch 318/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6796 - acc: 0.7569\n",
            "Epoch 318: val_loss did not improve from 0.75432\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.7128 - acc: 0.7507 - val_loss: 0.7545 - val_acc: 0.7240\n",
            "Epoch 319/1000\n",
            " 7/12 [================>.............] - ETA: 0s - loss: 0.7221 - acc: 0.7321\n",
            "Epoch 319: val_loss improved from 0.75432 to 0.75240, saving model to Best_model_epoch_319_val_loss_0.7524.keras\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.7125 - acc: 0.7480 - val_loss: 0.7524 - val_acc: 0.7320\n",
            "Epoch 320/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.7003 - acc: 0.7520\n",
            "Epoch 320: val_loss improved from 0.75240 to 0.75178, saving model to Best_model_epoch_320_val_loss_0.7518.keras\n",
            "12/12 [==============================] - 0s 31ms/step - loss: 0.7121 - acc: 0.7493 - val_loss: 0.7518 - val_acc: 0.7320\n",
            "Epoch 321/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7143 - acc: 0.7531\n",
            "Epoch 321: val_loss did not improve from 0.75178\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7113 - acc: 0.7507 - val_loss: 0.7530 - val_acc: 0.7280\n",
            "Epoch 322/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7241 - acc: 0.7391\n",
            "Epoch 322: val_loss did not improve from 0.75178\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.7104 - acc: 0.7520 - val_loss: 0.7523 - val_acc: 0.7240\n",
            "Epoch 323/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.7135 - acc: 0.7441\n",
            "Epoch 323: val_loss did not improve from 0.75178\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.7107 - acc: 0.7467 - val_loss: 0.7527 - val_acc: 0.7240\n",
            "Epoch 324/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6875 - acc: 0.7569\n",
            "Epoch 324: val_loss improved from 0.75178 to 0.75020, saving model to Best_model_epoch_324_val_loss_0.7502.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.7096 - acc: 0.7507 - val_loss: 0.7502 - val_acc: 0.7240\n",
            "Epoch 325/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7008 - acc: 0.7547\n",
            "Epoch 325: val_loss improved from 0.75020 to 0.74920, saving model to Best_model_epoch_325_val_loss_0.7492.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.7091 - acc: 0.7507 - val_loss: 0.7492 - val_acc: 0.7280\n",
            "Epoch 326/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7090 - acc: 0.7480\n",
            "Epoch 326: val_loss did not improve from 0.74920\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7090 - acc: 0.7480 - val_loss: 0.7506 - val_acc: 0.7320\n",
            "Epoch 327/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6885 - acc: 0.7656\n",
            "Epoch 327: val_loss did not improve from 0.74920\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7077 - acc: 0.7533 - val_loss: 0.7506 - val_acc: 0.7280\n",
            "Epoch 328/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7854 - acc: 0.7656\n",
            "Epoch 328: val_loss improved from 0.74920 to 0.74866, saving model to Best_model_epoch_328_val_loss_0.7487.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7076 - acc: 0.7533 - val_loss: 0.7487 - val_acc: 0.7280\n",
            "Epoch 329/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7469 - acc: 0.6875\n",
            "Epoch 329: val_loss did not improve from 0.74866\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7069 - acc: 0.7507 - val_loss: 0.7489 - val_acc: 0.7320\n",
            "Epoch 330/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7069 - acc: 0.7493\n",
            "Epoch 330: val_loss improved from 0.74866 to 0.74757, saving model to Best_model_epoch_330_val_loss_0.7476.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.7069 - acc: 0.7493 - val_loss: 0.7476 - val_acc: 0.7280\n",
            "Epoch 331/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6913 - acc: 0.7578\n",
            "Epoch 331: val_loss did not improve from 0.74757\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7070 - acc: 0.7520 - val_loss: 0.7498 - val_acc: 0.7240\n",
            "Epoch 332/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7013 - acc: 0.7543\n",
            "Epoch 332: val_loss improved from 0.74757 to 0.74715, saving model to Best_model_epoch_332_val_loss_0.7471.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7068 - acc: 0.7507 - val_loss: 0.7471 - val_acc: 0.7320\n",
            "Epoch 333/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7089 - acc: 0.7514\n",
            "Epoch 333: val_loss did not improve from 0.74715\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7053 - acc: 0.7560 - val_loss: 0.7473 - val_acc: 0.7200\n",
            "Epoch 334/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7155 - acc: 0.7472\n",
            "Epoch 334: val_loss did not improve from 0.74715\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7049 - acc: 0.7507 - val_loss: 0.7475 - val_acc: 0.7280\n",
            "Epoch 335/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7039 - acc: 0.7543\n",
            "Epoch 335: val_loss improved from 0.74715 to 0.74687, saving model to Best_model_epoch_335_val_loss_0.7469.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7044 - acc: 0.7547 - val_loss: 0.7469 - val_acc: 0.7240\n",
            "Epoch 336/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6929 - acc: 0.7528\n",
            "Epoch 336: val_loss did not improve from 0.74687\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7034 - acc: 0.7533 - val_loss: 0.7473 - val_acc: 0.7280\n",
            "Epoch 337/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7170 - acc: 0.7448\n",
            "Epoch 337: val_loss improved from 0.74687 to 0.74669, saving model to Best_model_epoch_337_val_loss_0.7467.keras\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.7033 - acc: 0.7520 - val_loss: 0.7467 - val_acc: 0.7200\n",
            "Epoch 338/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7006 - acc: 0.7557\n",
            "Epoch 338: val_loss improved from 0.74669 to 0.74503, saving model to Best_model_epoch_338_val_loss_0.7450.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7041 - acc: 0.7520 - val_loss: 0.7450 - val_acc: 0.7320\n",
            "Epoch 339/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7023 - acc: 0.7560\n",
            "Epoch 339: val_loss did not improve from 0.74503\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7023 - acc: 0.7560 - val_loss: 0.7462 - val_acc: 0.7280\n",
            "Epoch 340/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7045 - acc: 0.7486\n",
            "Epoch 340: val_loss improved from 0.74503 to 0.74465, saving model to Best_model_epoch_340_val_loss_0.7446.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.7016 - acc: 0.7547 - val_loss: 0.7446 - val_acc: 0.7240\n",
            "Epoch 341/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7016 - acc: 0.7533\n",
            "Epoch 341: val_loss improved from 0.74465 to 0.74409, saving model to Best_model_epoch_341_val_loss_0.7441.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7016 - acc: 0.7533 - val_loss: 0.7441 - val_acc: 0.7280\n",
            "Epoch 342/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7014 - acc: 0.7547\n",
            "Epoch 342: val_loss improved from 0.74409 to 0.74404, saving model to Best_model_epoch_342_val_loss_0.7440.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7014 - acc: 0.7547 - val_loss: 0.7440 - val_acc: 0.7280\n",
            "Epoch 343/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7007 - acc: 0.7560\n",
            "Epoch 343: val_loss improved from 0.74404 to 0.74362, saving model to Best_model_epoch_343_val_loss_0.7436.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7007 - acc: 0.7560 - val_loss: 0.7436 - val_acc: 0.7280\n",
            "Epoch 344/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5568 - acc: 0.7812\n",
            "Epoch 344: val_loss did not improve from 0.74362\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6999 - acc: 0.7520 - val_loss: 0.7441 - val_acc: 0.7240\n",
            "Epoch 345/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.7005 - acc: 0.7547\n",
            "Epoch 345: val_loss improved from 0.74362 to 0.74142, saving model to Best_model_epoch_345_val_loss_0.7414.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.7005 - acc: 0.7547 - val_loss: 0.7414 - val_acc: 0.7240\n",
            "Epoch 346/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6926 - acc: 0.7614\n",
            "Epoch 346: val_loss did not improve from 0.74142\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6993 - acc: 0.7560 - val_loss: 0.7445 - val_acc: 0.7280\n",
            "Epoch 347/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6960 - acc: 0.7609\n",
            "Epoch 347: val_loss did not improve from 0.74142\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6986 - acc: 0.7560 - val_loss: 0.7438 - val_acc: 0.7240\n",
            "Epoch 348/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7023 - acc: 0.7543\n",
            "Epoch 348: val_loss did not improve from 0.74142\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6987 - acc: 0.7560 - val_loss: 0.7425 - val_acc: 0.7280\n",
            "Epoch 349/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7050 - acc: 0.7625\n",
            "Epoch 349: val_loss did not improve from 0.74142\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6983 - acc: 0.7573 - val_loss: 0.7436 - val_acc: 0.7240\n",
            "Epoch 350/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.7137 - acc: 0.7453\n",
            "Epoch 350: val_loss did not improve from 0.74142\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6976 - acc: 0.7573 - val_loss: 0.7433 - val_acc: 0.7280\n",
            "Epoch 351/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7094 - acc: 0.7552\n",
            "Epoch 351: val_loss improved from 0.74142 to 0.74011, saving model to Best_model_epoch_351_val_loss_0.7401.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6968 - acc: 0.7547 - val_loss: 0.7401 - val_acc: 0.7280\n",
            "Epoch 352/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6974 - acc: 0.7560\n",
            "Epoch 352: val_loss improved from 0.74011 to 0.73915, saving model to Best_model_epoch_352_val_loss_0.7392.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6974 - acc: 0.7560 - val_loss: 0.7392 - val_acc: 0.7320\n",
            "Epoch 353/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6881 - acc: 0.7656\n",
            "Epoch 353: val_loss did not improve from 0.73915\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6958 - acc: 0.7587 - val_loss: 0.7411 - val_acc: 0.7240\n",
            "Epoch 354/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6919 - acc: 0.7594\n",
            "Epoch 354: val_loss did not improve from 0.73915\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6958 - acc: 0.7547 - val_loss: 0.7422 - val_acc: 0.7200\n",
            "Epoch 355/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6953 - acc: 0.7560\n",
            "Epoch 355: val_loss did not improve from 0.73915\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6953 - acc: 0.7560 - val_loss: 0.7406 - val_acc: 0.7280\n",
            "Epoch 356/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7077 - acc: 0.7514\n",
            "Epoch 356: val_loss improved from 0.73915 to 0.73875, saving model to Best_model_epoch_356_val_loss_0.7387.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6948 - acc: 0.7587 - val_loss: 0.7387 - val_acc: 0.7320\n",
            "Epoch 357/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6901 - acc: 0.7599\n",
            "Epoch 357: val_loss did not improve from 0.73875\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6936 - acc: 0.7613 - val_loss: 0.7392 - val_acc: 0.7280\n",
            "Epoch 358/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6950 - acc: 0.7585\n",
            "Epoch 358: val_loss improved from 0.73875 to 0.73851, saving model to Best_model_epoch_358_val_loss_0.7385.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6938 - acc: 0.7573 - val_loss: 0.7385 - val_acc: 0.7280\n",
            "Epoch 359/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6937 - acc: 0.7573\n",
            "Epoch 359: val_loss did not improve from 0.73851\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6937 - acc: 0.7573 - val_loss: 0.7410 - val_acc: 0.7320\n",
            "Epoch 360/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6934 - acc: 0.7573\n",
            "Epoch 360: val_loss did not improve from 0.73851\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6934 - acc: 0.7573 - val_loss: 0.7400 - val_acc: 0.7240\n",
            "Epoch 361/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6781 - acc: 0.7628\n",
            "Epoch 361: val_loss improved from 0.73851 to 0.73793, saving model to Best_model_epoch_361_val_loss_0.7379.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6927 - acc: 0.7587 - val_loss: 0.7379 - val_acc: 0.7320\n",
            "Epoch 362/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6921 - acc: 0.7587\n",
            "Epoch 362: val_loss did not improve from 0.73793\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6921 - acc: 0.7587 - val_loss: 0.7389 - val_acc: 0.7280\n",
            "Epoch 363/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6881 - acc: 0.7656\n",
            "Epoch 363: val_loss improved from 0.73793 to 0.73699, saving model to Best_model_epoch_363_val_loss_0.7370.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6917 - acc: 0.7587 - val_loss: 0.7370 - val_acc: 0.7240\n",
            "Epoch 364/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6853 - acc: 0.7609\n",
            "Epoch 364: val_loss did not improve from 0.73699\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6911 - acc: 0.7600 - val_loss: 0.7371 - val_acc: 0.7280\n",
            "Epoch 365/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6882 - acc: 0.7594\n",
            "Epoch 365: val_loss did not improve from 0.73699\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6909 - acc: 0.7600 - val_loss: 0.7375 - val_acc: 0.7280\n",
            "Epoch 366/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6815 - acc: 0.7628\n",
            "Epoch 366: val_loss improved from 0.73699 to 0.73674, saving model to Best_model_epoch_366_val_loss_0.7367.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6913 - acc: 0.7587 - val_loss: 0.7367 - val_acc: 0.7320\n",
            "Epoch 367/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6929 - acc: 0.7585\n",
            "Epoch 367: val_loss improved from 0.73674 to 0.73580, saving model to Best_model_epoch_367_val_loss_0.7358.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6894 - acc: 0.7613 - val_loss: 0.7358 - val_acc: 0.7280\n",
            "Epoch 368/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6947 - acc: 0.7528\n",
            "Epoch 368: val_loss did not improve from 0.73580\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6907 - acc: 0.7573 - val_loss: 0.7368 - val_acc: 0.7320\n",
            "Epoch 369/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6892 - acc: 0.7627\n",
            "Epoch 369: val_loss improved from 0.73580 to 0.73417, saving model to Best_model_epoch_369_val_loss_0.7342.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6892 - acc: 0.7627 - val_loss: 0.7342 - val_acc: 0.7320\n",
            "Epoch 370/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6890 - acc: 0.7587\n",
            "Epoch 370: val_loss did not improve from 0.73417\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6890 - acc: 0.7587 - val_loss: 0.7345 - val_acc: 0.7280\n",
            "Epoch 371/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6828 - acc: 0.7642\n",
            "Epoch 371: val_loss did not improve from 0.73417\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6890 - acc: 0.7600 - val_loss: 0.7362 - val_acc: 0.7280\n",
            "Epoch 372/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6940 - acc: 0.7604\n",
            "Epoch 372: val_loss improved from 0.73417 to 0.73274, saving model to Best_model_epoch_372_val_loss_0.7327.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6881 - acc: 0.7573 - val_loss: 0.7327 - val_acc: 0.7360\n",
            "Epoch 373/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6963 - acc: 0.7571\n",
            "Epoch 373: val_loss did not improve from 0.73274\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6881 - acc: 0.7587 - val_loss: 0.7334 - val_acc: 0.7320\n",
            "Epoch 374/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6817 - acc: 0.7685\n",
            "Epoch 374: val_loss did not improve from 0.73274\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6868 - acc: 0.7640 - val_loss: 0.7353 - val_acc: 0.7320\n",
            "Epoch 375/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6867 - acc: 0.7642\n",
            "Epoch 375: val_loss did not improve from 0.73274\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6873 - acc: 0.7587 - val_loss: 0.7357 - val_acc: 0.7280\n",
            "Epoch 376/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6710 - acc: 0.7628\n",
            "Epoch 376: val_loss did not improve from 0.73274\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6858 - acc: 0.7587 - val_loss: 0.7333 - val_acc: 0.7320\n",
            "Epoch 377/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6986 - acc: 0.7599\n",
            "Epoch 377: val_loss improved from 0.73274 to 0.73199, saving model to Best_model_epoch_377_val_loss_0.7320.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6859 - acc: 0.7627 - val_loss: 0.7320 - val_acc: 0.7360\n",
            "Epoch 378/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6931 - acc: 0.7514\n",
            "Epoch 378: val_loss improved from 0.73199 to 0.73184, saving model to Best_model_epoch_378_val_loss_0.7318.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6856 - acc: 0.7587 - val_loss: 0.7318 - val_acc: 0.7360\n",
            "Epoch 379/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6895 - acc: 0.7543\n",
            "Epoch 379: val_loss improved from 0.73184 to 0.73121, saving model to Best_model_epoch_379_val_loss_0.7312.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6864 - acc: 0.7600 - val_loss: 0.7312 - val_acc: 0.7400\n",
            "Epoch 380/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6843 - acc: 0.7627\n",
            "Epoch 380: val_loss did not improve from 0.73121\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6843 - acc: 0.7627 - val_loss: 0.7321 - val_acc: 0.7400\n",
            "Epoch 381/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6774 - acc: 0.7642\n",
            "Epoch 381: val_loss did not improve from 0.73121\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6843 - acc: 0.7613 - val_loss: 0.7332 - val_acc: 0.7320\n",
            "Epoch 382/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6796 - acc: 0.7628\n",
            "Epoch 382: val_loss did not improve from 0.73121\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6842 - acc: 0.7600 - val_loss: 0.7326 - val_acc: 0.7400\n",
            "Epoch 383/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6896 - acc: 0.7569\n",
            "Epoch 383: val_loss improved from 0.73121 to 0.72988, saving model to Best_model_epoch_383_val_loss_0.7299.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6840 - acc: 0.7587 - val_loss: 0.7299 - val_acc: 0.7440\n",
            "Epoch 384/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6839 - acc: 0.7613\n",
            "Epoch 384: val_loss improved from 0.72988 to 0.72981, saving model to Best_model_epoch_384_val_loss_0.7298.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6839 - acc: 0.7613 - val_loss: 0.7298 - val_acc: 0.7400\n",
            "Epoch 385/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6823 - acc: 0.7670\n",
            "Epoch 385: val_loss did not improve from 0.72981\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6827 - acc: 0.7613 - val_loss: 0.7319 - val_acc: 0.7360\n",
            "Epoch 386/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6830 - acc: 0.7625\n",
            "Epoch 386: val_loss improved from 0.72981 to 0.72939, saving model to Best_model_epoch_386_val_loss_0.7294.keras\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.6841 - acc: 0.7627 - val_loss: 0.7294 - val_acc: 0.7480\n",
            "Epoch 387/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6165 - acc: 0.8125\n",
            "Epoch 387: val_loss did not improve from 0.72939\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6821 - acc: 0.7600 - val_loss: 0.7327 - val_acc: 0.7320\n",
            "Epoch 388/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6617 - acc: 0.7691\n",
            "Epoch 388: val_loss did not improve from 0.72939\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6824 - acc: 0.7613 - val_loss: 0.7307 - val_acc: 0.7360\n",
            "Epoch 389/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6823 - acc: 0.7653\n",
            "Epoch 389: val_loss improved from 0.72939 to 0.72926, saving model to Best_model_epoch_389_val_loss_0.7293.keras\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.6823 - acc: 0.7653 - val_loss: 0.7293 - val_acc: 0.7440\n",
            "Epoch 390/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6795 - acc: 0.7585\n",
            "Epoch 390: val_loss did not improve from 0.72926\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6810 - acc: 0.7600 - val_loss: 0.7311 - val_acc: 0.7360\n",
            "Epoch 391/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6992 - acc: 0.7604\n",
            "Epoch 391: val_loss did not improve from 0.72926\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6811 - acc: 0.7640 - val_loss: 0.7321 - val_acc: 0.7320\n",
            "Epoch 392/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6808 - acc: 0.7587\n",
            "Epoch 392: val_loss improved from 0.72926 to 0.72711, saving model to Best_model_epoch_392_val_loss_0.7271.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6808 - acc: 0.7587 - val_loss: 0.7271 - val_acc: 0.7440\n",
            "Epoch 393/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6671 - acc: 0.7656\n",
            "Epoch 393: val_loss did not improve from 0.72711\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6802 - acc: 0.7640 - val_loss: 0.7278 - val_acc: 0.7400\n",
            "Epoch 394/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6839 - acc: 0.7571\n",
            "Epoch 394: val_loss did not improve from 0.72711\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6800 - acc: 0.7587 - val_loss: 0.7292 - val_acc: 0.7400\n",
            "Epoch 395/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6784 - acc: 0.7614\n",
            "Epoch 395: val_loss did not improve from 0.72711\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6790 - acc: 0.7600 - val_loss: 0.7280 - val_acc: 0.7400\n",
            "Epoch 396/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6920 - acc: 0.7641\n",
            "Epoch 396: val_loss did not improve from 0.72711\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6785 - acc: 0.7613 - val_loss: 0.7272 - val_acc: 0.7480\n",
            "Epoch 397/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6649 - acc: 0.7703\n",
            "Epoch 397: val_loss did not improve from 0.72711\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6787 - acc: 0.7613 - val_loss: 0.7290 - val_acc: 0.7360\n",
            "Epoch 398/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6879 - acc: 0.7535\n",
            "Epoch 398: val_loss did not improve from 0.72711\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6781 - acc: 0.7613 - val_loss: 0.7284 - val_acc: 0.7360\n",
            "Epoch 399/1000\n",
            " 7/12 [================>.............] - ETA: 0s - loss: 0.6594 - acc: 0.7545\n",
            "Epoch 399: val_loss improved from 0.72711 to 0.72621, saving model to Best_model_epoch_399_val_loss_0.7262.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.6781 - acc: 0.7627 - val_loss: 0.7262 - val_acc: 0.7480\n",
            "Epoch 400/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6858 - acc: 0.7656\n",
            "Epoch 400: val_loss improved from 0.72621 to 0.72534, saving model to Best_model_epoch_400_val_loss_0.7253.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6775 - acc: 0.7640 - val_loss: 0.7253 - val_acc: 0.7440\n",
            "Epoch 401/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6512 - acc: 0.7695\n",
            "Epoch 401: val_loss did not improve from 0.72534\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6771 - acc: 0.7653 - val_loss: 0.7280 - val_acc: 0.7360\n",
            "Epoch 402/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6936 - acc: 0.7516\n",
            "Epoch 402: val_loss did not improve from 0.72534\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6765 - acc: 0.7600 - val_loss: 0.7267 - val_acc: 0.7480\n",
            "Epoch 403/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6784 - acc: 0.7500\n",
            "Epoch 403: val_loss improved from 0.72534 to 0.72523, saving model to Best_model_epoch_403_val_loss_0.7252.keras\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.6763 - acc: 0.7613 - val_loss: 0.7252 - val_acc: 0.7480\n",
            "Epoch 404/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6681 - acc: 0.7609\n",
            "Epoch 404: val_loss improved from 0.72523 to 0.72467, saving model to Best_model_epoch_404_val_loss_0.7247.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6761 - acc: 0.7613 - val_loss: 0.7247 - val_acc: 0.7480\n",
            "Epoch 405/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.7022 - acc: 0.7448\n",
            "Epoch 405: val_loss did not improve from 0.72467\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6764 - acc: 0.7627 - val_loss: 0.7275 - val_acc: 0.7360\n",
            "Epoch 406/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6793 - acc: 0.7614\n",
            "Epoch 406: val_loss did not improve from 0.72467\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6750 - acc: 0.7640 - val_loss: 0.7262 - val_acc: 0.7440\n",
            "Epoch 407/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6713 - acc: 0.7670\n",
            "Epoch 407: val_loss improved from 0.72467 to 0.72429, saving model to Best_model_epoch_407_val_loss_0.7243.keras\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.6755 - acc: 0.7627 - val_loss: 0.7243 - val_acc: 0.7440\n",
            "Epoch 408/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6805 - acc: 0.7535\n",
            "Epoch 408: val_loss improved from 0.72429 to 0.72417, saving model to Best_model_epoch_408_val_loss_0.7242.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.6742 - acc: 0.7613 - val_loss: 0.7242 - val_acc: 0.7480\n",
            "Epoch 409/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6728 - acc: 0.7685\n",
            "Epoch 409: val_loss did not improve from 0.72417\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6744 - acc: 0.7640 - val_loss: 0.7247 - val_acc: 0.7440\n",
            "Epoch 410/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6802 - acc: 0.7594\n",
            "Epoch 410: val_loss improved from 0.72417 to 0.72376, saving model to Best_model_epoch_410_val_loss_0.7238.keras\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.6741 - acc: 0.7640 - val_loss: 0.7238 - val_acc: 0.7480\n",
            "Epoch 411/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6731 - acc: 0.7627\n",
            "Epoch 411: val_loss did not improve from 0.72376\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6731 - acc: 0.7627 - val_loss: 0.7245 - val_acc: 0.7440\n",
            "Epoch 412/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6762 - acc: 0.7628\n",
            "Epoch 412: val_loss improved from 0.72376 to 0.72342, saving model to Best_model_epoch_412_val_loss_0.7234.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6733 - acc: 0.7653 - val_loss: 0.7234 - val_acc: 0.7480\n",
            "Epoch 413/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6862 - acc: 0.7557\n",
            "Epoch 413: val_loss improved from 0.72342 to 0.72212, saving model to Best_model_epoch_413_val_loss_0.7221.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6732 - acc: 0.7627 - val_loss: 0.7221 - val_acc: 0.7440\n",
            "Epoch 414/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6830 - acc: 0.7614\n",
            "Epoch 414: val_loss did not improve from 0.72212\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6726 - acc: 0.7640 - val_loss: 0.7233 - val_acc: 0.7480\n",
            "Epoch 415/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6751 - acc: 0.7703\n",
            "Epoch 415: val_loss did not improve from 0.72212\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6722 - acc: 0.7640 - val_loss: 0.7234 - val_acc: 0.7480\n",
            "Epoch 416/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6804 - acc: 0.7614\n",
            "Epoch 416: val_loss did not improve from 0.72212\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6718 - acc: 0.7613 - val_loss: 0.7254 - val_acc: 0.7400\n",
            "Epoch 417/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6725 - acc: 0.7667\n",
            "Epoch 417: val_loss did not improve from 0.72212\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6725 - acc: 0.7667 - val_loss: 0.7238 - val_acc: 0.7360\n",
            "Epoch 418/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6277 - acc: 0.7891\n",
            "Epoch 418: val_loss did not improve from 0.72212\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6715 - acc: 0.7640 - val_loss: 0.7229 - val_acc: 0.7480\n",
            "Epoch 419/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4896 - acc: 0.8438\n",
            "Epoch 419: val_loss did not improve from 0.72212\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6709 - acc: 0.7653 - val_loss: 0.7226 - val_acc: 0.7440\n",
            "Epoch 420/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7995 - acc: 0.7656\n",
            "Epoch 420: val_loss improved from 0.72212 to 0.72211, saving model to Best_model_epoch_420_val_loss_0.7221.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.6708 - acc: 0.7653 - val_loss: 0.7221 - val_acc: 0.7480\n",
            "Epoch 421/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6620 - acc: 0.7599\n",
            "Epoch 421: val_loss improved from 0.72211 to 0.72113, saving model to Best_model_epoch_421_val_loss_0.7211.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6714 - acc: 0.7613 - val_loss: 0.7211 - val_acc: 0.7440\n",
            "Epoch 422/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6707 - acc: 0.7627\n",
            "Epoch 422: val_loss did not improve from 0.72113\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6707 - acc: 0.7627 - val_loss: 0.7224 - val_acc: 0.7400\n",
            "Epoch 423/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6776 - acc: 0.7656\n",
            "Epoch 423: val_loss improved from 0.72113 to 0.72096, saving model to Best_model_epoch_423_val_loss_0.7210.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6693 - acc: 0.7667 - val_loss: 0.7210 - val_acc: 0.7480\n",
            "Epoch 424/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6720 - acc: 0.7642\n",
            "Epoch 424: val_loss improved from 0.72096 to 0.72043, saving model to Best_model_epoch_424_val_loss_0.7204.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6690 - acc: 0.7667 - val_loss: 0.7204 - val_acc: 0.7520\n",
            "Epoch 425/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6590 - acc: 0.7699\n",
            "Epoch 425: val_loss improved from 0.72043 to 0.72020, saving model to Best_model_epoch_425_val_loss_0.7202.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6689 - acc: 0.7653 - val_loss: 0.7202 - val_acc: 0.7520\n",
            "Epoch 426/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6692 - acc: 0.7627\n",
            "Epoch 426: val_loss did not improve from 0.72020\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6692 - acc: 0.7627 - val_loss: 0.7229 - val_acc: 0.7400\n",
            "Epoch 427/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6683 - acc: 0.7640\n",
            "Epoch 427: val_loss did not improve from 0.72020\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6683 - acc: 0.7640 - val_loss: 0.7205 - val_acc: 0.7440\n",
            "Epoch 428/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6728 - acc: 0.7639\n",
            "Epoch 428: val_loss did not improve from 0.72020\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6675 - acc: 0.7667 - val_loss: 0.7215 - val_acc: 0.7440\n",
            "Epoch 429/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6720 - acc: 0.7628\n",
            "Epoch 429: val_loss did not improve from 0.72020\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6676 - acc: 0.7627 - val_loss: 0.7205 - val_acc: 0.7440\n",
            "Epoch 430/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6744 - acc: 0.7531\n",
            "Epoch 430: val_loss improved from 0.72020 to 0.71918, saving model to Best_model_epoch_430_val_loss_0.7192.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6673 - acc: 0.7653 - val_loss: 0.7192 - val_acc: 0.7480\n",
            "Epoch 431/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6507 - acc: 0.7770\n",
            "Epoch 431: val_loss did not improve from 0.71918\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6673 - acc: 0.7667 - val_loss: 0.7205 - val_acc: 0.7440\n",
            "Epoch 432/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6789 - acc: 0.7642\n",
            "Epoch 432: val_loss did not improve from 0.71918\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6677 - acc: 0.7653 - val_loss: 0.7218 - val_acc: 0.7400\n",
            "Epoch 433/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6736 - acc: 0.7625\n",
            "Epoch 433: val_loss improved from 0.71918 to 0.71896, saving model to Best_model_epoch_433_val_loss_0.7190.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6665 - acc: 0.7653 - val_loss: 0.7190 - val_acc: 0.7480\n",
            "Epoch 434/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6670 - acc: 0.7627\n",
            "Epoch 434: val_loss did not improve from 0.71896\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6670 - acc: 0.7627 - val_loss: 0.7208 - val_acc: 0.7400\n",
            "Epoch 435/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6742 - acc: 0.7614\n",
            "Epoch 435: val_loss improved from 0.71896 to 0.71852, saving model to Best_model_epoch_435_val_loss_0.7185.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6669 - acc: 0.7640 - val_loss: 0.7185 - val_acc: 0.7360\n",
            "Epoch 436/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6747 - acc: 0.7670\n",
            "Epoch 436: val_loss did not improve from 0.71852\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6655 - acc: 0.7667 - val_loss: 0.7198 - val_acc: 0.7440\n",
            "Epoch 437/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6648 - acc: 0.7667\n",
            "Epoch 437: val_loss did not improve from 0.71852\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6648 - acc: 0.7667 - val_loss: 0.7194 - val_acc: 0.7440\n",
            "Epoch 438/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6545 - acc: 0.7734\n",
            "Epoch 438: val_loss improved from 0.71852 to 0.71786, saving model to Best_model_epoch_438_val_loss_0.7179.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6643 - acc: 0.7667 - val_loss: 0.7179 - val_acc: 0.7480\n",
            "Epoch 439/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6676 - acc: 0.7628\n",
            "Epoch 439: val_loss did not improve from 0.71786\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6643 - acc: 0.7667 - val_loss: 0.7182 - val_acc: 0.7440\n",
            "Epoch 440/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6602 - acc: 0.7614\n",
            "Epoch 440: val_loss did not improve from 0.71786\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6639 - acc: 0.7653 - val_loss: 0.7180 - val_acc: 0.7440\n",
            "Epoch 441/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6636 - acc: 0.7680\n",
            "Epoch 441: val_loss improved from 0.71786 to 0.71764, saving model to Best_model_epoch_441_val_loss_0.7176.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6636 - acc: 0.7680 - val_loss: 0.7176 - val_acc: 0.7440\n",
            "Epoch 442/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6636 - acc: 0.7653\n",
            "Epoch 442: val_loss did not improve from 0.71764\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6636 - acc: 0.7653 - val_loss: 0.7178 - val_acc: 0.7440\n",
            "Epoch 443/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6625 - acc: 0.7667\n",
            "Epoch 443: val_loss did not improve from 0.71764\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6625 - acc: 0.7667 - val_loss: 0.7183 - val_acc: 0.7440\n",
            "Epoch 444/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6680 - acc: 0.7628\n",
            "Epoch 444: val_loss did not improve from 0.71764\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6627 - acc: 0.7640 - val_loss: 0.7184 - val_acc: 0.7440\n",
            "Epoch 445/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6626 - acc: 0.7627\n",
            "Epoch 445: val_loss improved from 0.71764 to 0.71638, saving model to Best_model_epoch_445_val_loss_0.7164.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6626 - acc: 0.7627 - val_loss: 0.7164 - val_acc: 0.7480\n",
            "Epoch 446/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6617 - acc: 0.7653\n",
            "Epoch 446: val_loss did not improve from 0.71638\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6617 - acc: 0.7653 - val_loss: 0.7174 - val_acc: 0.7440\n",
            "Epoch 447/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6453 - acc: 0.7756\n",
            "Epoch 447: val_loss did not improve from 0.71638\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6625 - acc: 0.7640 - val_loss: 0.7167 - val_acc: 0.7440\n",
            "Epoch 448/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6691 - acc: 0.7578\n",
            "Epoch 448: val_loss improved from 0.71638 to 0.71570, saving model to Best_model_epoch_448_val_loss_0.7157.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6619 - acc: 0.7667 - val_loss: 0.7157 - val_acc: 0.7440\n",
            "Epoch 449/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6596 - acc: 0.7713\n",
            "Epoch 449: val_loss did not improve from 0.71570\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6609 - acc: 0.7680 - val_loss: 0.7164 - val_acc: 0.7400\n",
            "Epoch 450/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4893 - acc: 0.8438\n",
            "Epoch 450: val_loss did not improve from 0.71570\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6613 - acc: 0.7667 - val_loss: 0.7176 - val_acc: 0.7440\n",
            "Epoch 451/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6987 - acc: 0.7500\n",
            "Epoch 451: val_loss did not improve from 0.71570\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6610 - acc: 0.7667 - val_loss: 0.7160 - val_acc: 0.7400\n",
            "Epoch 452/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6567 - acc: 0.7727\n",
            "Epoch 452: val_loss improved from 0.71570 to 0.71456, saving model to Best_model_epoch_452_val_loss_0.7146.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6603 - acc: 0.7667 - val_loss: 0.7146 - val_acc: 0.7440\n",
            "Epoch 453/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6601 - acc: 0.7707\n",
            "Epoch 453: val_loss did not improve from 0.71456\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6601 - acc: 0.7707 - val_loss: 0.7155 - val_acc: 0.7440\n",
            "Epoch 454/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5248 - acc: 0.8750\n",
            "Epoch 454: val_loss did not improve from 0.71456\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6609 - acc: 0.7693 - val_loss: 0.7148 - val_acc: 0.7440\n",
            "Epoch 455/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6543 - acc: 0.7750\n",
            "Epoch 455: val_loss improved from 0.71456 to 0.71355, saving model to Best_model_epoch_455_val_loss_0.7135.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6593 - acc: 0.7680 - val_loss: 0.7135 - val_acc: 0.7440\n",
            "Epoch 456/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6682 - acc: 0.7656\n",
            "Epoch 456: val_loss did not improve from 0.71355\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6602 - acc: 0.7653 - val_loss: 0.7172 - val_acc: 0.7360\n",
            "Epoch 457/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6602 - acc: 0.7656\n",
            "Epoch 457: val_loss did not improve from 0.71355\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6588 - acc: 0.7667 - val_loss: 0.7155 - val_acc: 0.7400\n",
            "Epoch 458/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6464 - acc: 0.7727\n",
            "Epoch 458: val_loss improved from 0.71355 to 0.71290, saving model to Best_model_epoch_458_val_loss_0.7129.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6588 - acc: 0.7667 - val_loss: 0.7129 - val_acc: 0.7400\n",
            "Epoch 459/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6795 - acc: 0.7571\n",
            "Epoch 459: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6589 - acc: 0.7667 - val_loss: 0.7134 - val_acc: 0.7400\n",
            "Epoch 460/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6580 - acc: 0.7707\n",
            "Epoch 460: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6580 - acc: 0.7707 - val_loss: 0.7137 - val_acc: 0.7400\n",
            "Epoch 461/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6614 - acc: 0.7727\n",
            "Epoch 461: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6581 - acc: 0.7720 - val_loss: 0.7145 - val_acc: 0.7480\n",
            "Epoch 462/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6577 - acc: 0.7693\n",
            "Epoch 462: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6577 - acc: 0.7693 - val_loss: 0.7138 - val_acc: 0.7400\n",
            "Epoch 463/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6534 - acc: 0.7727\n",
            "Epoch 463: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6570 - acc: 0.7707 - val_loss: 0.7141 - val_acc: 0.7400\n",
            "Epoch 464/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6571 - acc: 0.7693\n",
            "Epoch 464: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6571 - acc: 0.7693 - val_loss: 0.7140 - val_acc: 0.7360\n",
            "Epoch 465/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6364 - acc: 0.7828\n",
            "Epoch 465: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6573 - acc: 0.7707 - val_loss: 0.7146 - val_acc: 0.7440\n",
            "Epoch 466/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6552 - acc: 0.7685\n",
            "Epoch 466: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6554 - acc: 0.7680 - val_loss: 0.7136 - val_acc: 0.7360\n",
            "Epoch 467/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6347 - acc: 0.7188\n",
            "Epoch 467: val_loss did not improve from 0.71290\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6562 - acc: 0.7653 - val_loss: 0.7130 - val_acc: 0.7400\n",
            "Epoch 468/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6559 - acc: 0.7667\n",
            "Epoch 468: val_loss improved from 0.71290 to 0.71268, saving model to Best_model_epoch_468_val_loss_0.7127.keras\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.6559 - acc: 0.7667 - val_loss: 0.7127 - val_acc: 0.7400\n",
            "Epoch 469/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6553 - acc: 0.7680\n",
            "Epoch 469: val_loss improved from 0.71268 to 0.71262, saving model to Best_model_epoch_469_val_loss_0.7126.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6553 - acc: 0.7680 - val_loss: 0.7126 - val_acc: 0.7440\n",
            "Epoch 470/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6555 - acc: 0.7720\n",
            "Epoch 470: val_loss did not improve from 0.71262\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6555 - acc: 0.7720 - val_loss: 0.7127 - val_acc: 0.7440\n",
            "Epoch 471/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6617 - acc: 0.7656\n",
            "Epoch 471: val_loss improved from 0.71262 to 0.70990, saving model to Best_model_epoch_471_val_loss_0.7099.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6550 - acc: 0.7747 - val_loss: 0.7099 - val_acc: 0.7400\n",
            "Epoch 472/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6485 - acc: 0.7674\n",
            "Epoch 472: val_loss did not improve from 0.70990\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6553 - acc: 0.7680 - val_loss: 0.7114 - val_acc: 0.7400\n",
            "Epoch 473/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6530 - acc: 0.7674\n",
            "Epoch 473: val_loss did not improve from 0.70990\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6537 - acc: 0.7693 - val_loss: 0.7114 - val_acc: 0.7400\n",
            "Epoch 474/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6635 - acc: 0.7656\n",
            "Epoch 474: val_loss did not improve from 0.70990\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6549 - acc: 0.7693 - val_loss: 0.7123 - val_acc: 0.7360\n",
            "Epoch 475/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6535 - acc: 0.7693\n",
            "Epoch 475: val_loss did not improve from 0.70990\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6535 - acc: 0.7693 - val_loss: 0.7123 - val_acc: 0.7440\n",
            "Epoch 476/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6591 - acc: 0.7713\n",
            "Epoch 476: val_loss did not improve from 0.70990\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6540 - acc: 0.7693 - val_loss: 0.7104 - val_acc: 0.7400\n",
            "Epoch 477/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6534 - acc: 0.7693\n",
            "Epoch 477: val_loss did not improve from 0.70990\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6534 - acc: 0.7693 - val_loss: 0.7116 - val_acc: 0.7360\n",
            "Epoch 478/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6625 - acc: 0.7656\n",
            "Epoch 478: val_loss did not improve from 0.70990\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6527 - acc: 0.7693 - val_loss: 0.7101 - val_acc: 0.7440\n",
            "Epoch 479/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6554 - acc: 0.7685\n",
            "Epoch 479: val_loss improved from 0.70990 to 0.70980, saving model to Best_model_epoch_479_val_loss_0.7098.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.6522 - acc: 0.7720 - val_loss: 0.7098 - val_acc: 0.7360\n",
            "Epoch 480/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6406 - acc: 0.7750\n",
            "Epoch 480: val_loss did not improve from 0.70980\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6518 - acc: 0.7747 - val_loss: 0.7111 - val_acc: 0.7400\n",
            "Epoch 481/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7250 - acc: 0.7812\n",
            "Epoch 481: val_loss did not improve from 0.70980\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6522 - acc: 0.7747 - val_loss: 0.7103 - val_acc: 0.7400\n",
            "Epoch 482/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6513 - acc: 0.7680\n",
            "Epoch 482: val_loss did not improve from 0.70980\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6513 - acc: 0.7680 - val_loss: 0.7104 - val_acc: 0.7360\n",
            "Epoch 483/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6449 - acc: 0.7773\n",
            "Epoch 483: val_loss improved from 0.70980 to 0.70974, saving model to Best_model_epoch_483_val_loss_0.7097.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6515 - acc: 0.7720 - val_loss: 0.7097 - val_acc: 0.7400\n",
            "Epoch 484/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6374 - acc: 0.7828\n",
            "Epoch 484: val_loss did not improve from 0.70974\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.6506 - acc: 0.7760 - val_loss: 0.7098 - val_acc: 0.7360\n",
            "Epoch 485/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6507 - acc: 0.7733\n",
            "Epoch 485: val_loss improved from 0.70974 to 0.70921, saving model to Best_model_epoch_485_val_loss_0.7092.keras\n",
            "12/12 [==============================] - 1s 128ms/step - loss: 0.6507 - acc: 0.7733 - val_loss: 0.7092 - val_acc: 0.7360\n",
            "Epoch 486/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6546 - acc: 0.7699\n",
            "Epoch 486: val_loss did not improve from 0.70921\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6509 - acc: 0.7707 - val_loss: 0.7102 - val_acc: 0.7360\n",
            "Epoch 487/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6525 - acc: 0.7676\n",
            "Epoch 487: val_loss did not improve from 0.70921\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6504 - acc: 0.7733 - val_loss: 0.7093 - val_acc: 0.7400\n",
            "Epoch 488/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6500 - acc: 0.7741\n",
            "Epoch 488: val_loss improved from 0.70921 to 0.70808, saving model to Best_model_epoch_488_val_loss_0.7081.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.6503 - acc: 0.7747 - val_loss: 0.7081 - val_acc: 0.7440\n",
            "Epoch 489/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6742 - acc: 0.7656\n",
            "Epoch 489: val_loss did not improve from 0.70808\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6499 - acc: 0.7720 - val_loss: 0.7095 - val_acc: 0.7400\n",
            "Epoch 490/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6405 - acc: 0.7832\n",
            "Epoch 490: val_loss did not improve from 0.70808\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.6490 - acc: 0.7707 - val_loss: 0.7097 - val_acc: 0.7360\n",
            "Epoch 491/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6392 - acc: 0.7770\n",
            "Epoch 491: val_loss did not improve from 0.70808\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6491 - acc: 0.7720 - val_loss: 0.7084 - val_acc: 0.7360\n",
            "Epoch 492/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6273 - acc: 0.7832\n",
            "Epoch 492: val_loss did not improve from 0.70808\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.6489 - acc: 0.7680 - val_loss: 0.7082 - val_acc: 0.7440\n",
            "Epoch 493/1000\n",
            " 7/12 [================>.............] - ETA: 0s - loss: 0.6317 - acc: 0.7746\n",
            "Epoch 493: val_loss did not improve from 0.70808\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6493 - acc: 0.7733 - val_loss: 0.7089 - val_acc: 0.7360\n",
            "Epoch 494/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6026 - acc: 0.7852\n",
            "Epoch 494: val_loss did not improve from 0.70808\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6491 - acc: 0.7733 - val_loss: 0.7094 - val_acc: 0.7400\n",
            "Epoch 495/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6588 - acc: 0.7708\n",
            "Epoch 495: val_loss improved from 0.70808 to 0.70720, saving model to Best_model_epoch_495_val_loss_0.7072.keras\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 0.6482 - acc: 0.7680 - val_loss: 0.7072 - val_acc: 0.7360\n",
            "Epoch 496/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6420 - acc: 0.7760\n",
            "Epoch 496: val_loss improved from 0.70720 to 0.70704, saving model to Best_model_epoch_496_val_loss_0.7070.keras\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 0.6490 - acc: 0.7747 - val_loss: 0.7070 - val_acc: 0.7400\n",
            "Epoch 497/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6549 - acc: 0.7781\n",
            "Epoch 497: val_loss did not improve from 0.70704\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6473 - acc: 0.7773 - val_loss: 0.7082 - val_acc: 0.7440\n",
            "Epoch 498/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6362 - acc: 0.7793\n",
            "Epoch 498: val_loss improved from 0.70704 to 0.70694, saving model to Best_model_epoch_498_val_loss_0.7069.keras\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.6471 - acc: 0.7760 - val_loss: 0.7069 - val_acc: 0.7360\n",
            "Epoch 499/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6426 - acc: 0.7734\n",
            "Epoch 499: val_loss improved from 0.70694 to 0.70653, saving model to Best_model_epoch_499_val_loss_0.7065.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6467 - acc: 0.7720 - val_loss: 0.7065 - val_acc: 0.7360\n",
            "Epoch 500/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6461 - acc: 0.7733\n",
            "Epoch 500: val_loss did not improve from 0.70653\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6461 - acc: 0.7733 - val_loss: 0.7072 - val_acc: 0.7360\n",
            "Epoch 501/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6468 - acc: 0.7707\n",
            "Epoch 501: val_loss did not improve from 0.70653\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6468 - acc: 0.7707 - val_loss: 0.7076 - val_acc: 0.7360\n",
            "Epoch 502/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6461 - acc: 0.7733\n",
            "Epoch 502: val_loss did not improve from 0.70653\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6461 - acc: 0.7733 - val_loss: 0.7083 - val_acc: 0.7360\n",
            "Epoch 503/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6459 - acc: 0.7760\n",
            "Epoch 503: val_loss did not improve from 0.70653\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6459 - acc: 0.7760 - val_loss: 0.7083 - val_acc: 0.7360\n",
            "Epoch 504/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6445 - acc: 0.7784\n",
            "Epoch 504: val_loss did not improve from 0.70653\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6456 - acc: 0.7747 - val_loss: 0.7067 - val_acc: 0.7360\n",
            "Epoch 505/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6450 - acc: 0.7733\n",
            "Epoch 505: val_loss improved from 0.70653 to 0.70591, saving model to Best_model_epoch_505_val_loss_0.7059.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6450 - acc: 0.7733 - val_loss: 0.7059 - val_acc: 0.7400\n",
            "Epoch 506/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6457 - acc: 0.7747\n",
            "Epoch 506: val_loss improved from 0.70591 to 0.70517, saving model to Best_model_epoch_506_val_loss_0.7052.keras\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.6457 - acc: 0.7747 - val_loss: 0.7052 - val_acc: 0.7360\n",
            "Epoch 507/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6445 - acc: 0.7642\n",
            "Epoch 507: val_loss did not improve from 0.70517\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6452 - acc: 0.7693 - val_loss: 0.7057 - val_acc: 0.7360\n",
            "Epoch 508/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6482 - acc: 0.7741\n",
            "Epoch 508: val_loss did not improve from 0.70517\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6447 - acc: 0.7733 - val_loss: 0.7074 - val_acc: 0.7360\n",
            "Epoch 509/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6380 - acc: 0.7766\n",
            "Epoch 509: val_loss did not improve from 0.70517\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6447 - acc: 0.7720 - val_loss: 0.7064 - val_acc: 0.7400\n",
            "Epoch 510/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6452 - acc: 0.7733\n",
            "Epoch 510: val_loss did not improve from 0.70517\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6452 - acc: 0.7733 - val_loss: 0.7063 - val_acc: 0.7400\n",
            "Epoch 511/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6408 - acc: 0.7741\n",
            "Epoch 511: val_loss did not improve from 0.70517\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6432 - acc: 0.7733 - val_loss: 0.7054 - val_acc: 0.7360\n",
            "Epoch 512/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6436 - acc: 0.7828\n",
            "Epoch 512: val_loss did not improve from 0.70517\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6433 - acc: 0.7773 - val_loss: 0.7070 - val_acc: 0.7360\n",
            "Epoch 513/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6581 - acc: 0.7708\n",
            "Epoch 513: val_loss improved from 0.70517 to 0.70442, saving model to Best_model_epoch_513_val_loss_0.7044.keras\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.6433 - acc: 0.7720 - val_loss: 0.7044 - val_acc: 0.7360\n",
            "Epoch 514/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6404 - acc: 0.7812\n",
            "Epoch 514: val_loss did not improve from 0.70442\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6433 - acc: 0.7760 - val_loss: 0.7072 - val_acc: 0.7360\n",
            "Epoch 515/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6089 - acc: 0.7847\n",
            "Epoch 515: val_loss did not improve from 0.70442\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6424 - acc: 0.7733 - val_loss: 0.7045 - val_acc: 0.7360\n",
            "Epoch 516/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6490 - acc: 0.7741\n",
            "Epoch 516: val_loss improved from 0.70442 to 0.70373, saving model to Best_model_epoch_516_val_loss_0.7037.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6420 - acc: 0.7720 - val_loss: 0.7037 - val_acc: 0.7360\n",
            "Epoch 517/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6321 - acc: 0.7656\n",
            "Epoch 517: val_loss did not improve from 0.70373\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6420 - acc: 0.7707 - val_loss: 0.7039 - val_acc: 0.7360\n",
            "Epoch 518/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6417 - acc: 0.7747\n",
            "Epoch 518: val_loss improved from 0.70373 to 0.70284, saving model to Best_model_epoch_518_val_loss_0.7028.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6417 - acc: 0.7747 - val_loss: 0.7028 - val_acc: 0.7360\n",
            "Epoch 519/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6412 - acc: 0.7760\n",
            "Epoch 519: val_loss did not improve from 0.70284\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6412 - acc: 0.7760 - val_loss: 0.7045 - val_acc: 0.7400\n",
            "Epoch 520/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6576 - acc: 0.7344\n",
            "Epoch 520: val_loss did not improve from 0.70284\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6415 - acc: 0.7733 - val_loss: 0.7037 - val_acc: 0.7360\n",
            "Epoch 521/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6419 - acc: 0.7727\n",
            "Epoch 521: val_loss did not improve from 0.70284\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6409 - acc: 0.7733 - val_loss: 0.7047 - val_acc: 0.7360\n",
            "Epoch 522/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6435 - acc: 0.7784\n",
            "Epoch 522: val_loss did not improve from 0.70284\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6412 - acc: 0.7800 - val_loss: 0.7048 - val_acc: 0.7400\n",
            "Epoch 523/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5765 - acc: 0.7969\n",
            "Epoch 523: val_loss improved from 0.70284 to 0.70280, saving model to Best_model_epoch_523_val_loss_0.7028.keras\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6412 - acc: 0.7760 - val_loss: 0.7028 - val_acc: 0.7360\n",
            "Epoch 524/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6482 - acc: 0.7699\n",
            "Epoch 524: val_loss improved from 0.70280 to 0.70243, saving model to Best_model_epoch_524_val_loss_0.7024.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6399 - acc: 0.7733 - val_loss: 0.7024 - val_acc: 0.7360\n",
            "Epoch 525/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6400 - acc: 0.7720\n",
            "Epoch 525: val_loss did not improve from 0.70243\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6400 - acc: 0.7720 - val_loss: 0.7038 - val_acc: 0.7400\n",
            "Epoch 526/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6438 - acc: 0.7713\n",
            "Epoch 526: val_loss improved from 0.70243 to 0.70214, saving model to Best_model_epoch_526_val_loss_0.7021.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6395 - acc: 0.7733 - val_loss: 0.7021 - val_acc: 0.7360\n",
            "Epoch 527/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5109 - acc: 0.8125\n",
            "Epoch 527: val_loss did not improve from 0.70214\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6397 - acc: 0.7707 - val_loss: 0.7040 - val_acc: 0.7400\n",
            "Epoch 528/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6300 - acc: 0.7798\n",
            "Epoch 528: val_loss did not improve from 0.70214\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6390 - acc: 0.7747 - val_loss: 0.7024 - val_acc: 0.7440\n",
            "Epoch 529/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6389 - acc: 0.7760\n",
            "Epoch 529: val_loss did not improve from 0.70214\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6389 - acc: 0.7760 - val_loss: 0.7039 - val_acc: 0.7400\n",
            "Epoch 530/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6409 - acc: 0.7798\n",
            "Epoch 530: val_loss improved from 0.70214 to 0.70195, saving model to Best_model_epoch_530_val_loss_0.7019.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6386 - acc: 0.7773 - val_loss: 0.7019 - val_acc: 0.7360\n",
            "Epoch 531/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6386 - acc: 0.7747\n",
            "Epoch 531: val_loss did not improve from 0.70195\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6386 - acc: 0.7747 - val_loss: 0.7028 - val_acc: 0.7320\n",
            "Epoch 532/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6380 - acc: 0.7733\n",
            "Epoch 532: val_loss did not improve from 0.70195\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6380 - acc: 0.7733 - val_loss: 0.7028 - val_acc: 0.7400\n",
            "Epoch 533/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6408 - acc: 0.7685\n",
            "Epoch 533: val_loss improved from 0.70195 to 0.70168, saving model to Best_model_epoch_533_val_loss_0.7017.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6384 - acc: 0.7733 - val_loss: 0.7017 - val_acc: 0.7400\n",
            "Epoch 534/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6441 - acc: 0.7719\n",
            "Epoch 534: val_loss improved from 0.70168 to 0.70151, saving model to Best_model_epoch_534_val_loss_0.7015.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6375 - acc: 0.7747 - val_loss: 0.7015 - val_acc: 0.7400\n",
            "Epoch 535/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6568 - acc: 0.7670\n",
            "Epoch 535: val_loss improved from 0.70151 to 0.70149, saving model to Best_model_epoch_535_val_loss_0.7015.keras\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6373 - acc: 0.7747 - val_loss: 0.7015 - val_acc: 0.7400\n",
            "Epoch 536/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6371 - acc: 0.7760\n",
            "Epoch 536: val_loss did not improve from 0.70149\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6371 - acc: 0.7760 - val_loss: 0.7039 - val_acc: 0.7400\n",
            "Epoch 537/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6398 - acc: 0.7741\n",
            "Epoch 537: val_loss improved from 0.70149 to 0.70142, saving model to Best_model_epoch_537_val_loss_0.7014.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6376 - acc: 0.7733 - val_loss: 0.7014 - val_acc: 0.7400\n",
            "Epoch 538/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8432 - acc: 0.7188\n",
            "Epoch 538: val_loss did not improve from 0.70142\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6371 - acc: 0.7760 - val_loss: 0.7026 - val_acc: 0.7400\n",
            "Epoch 539/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6202 - acc: 0.7875\n",
            "Epoch 539: val_loss did not improve from 0.70142\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6364 - acc: 0.7800 - val_loss: 0.7022 - val_acc: 0.7400\n",
            "Epoch 540/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6297 - acc: 0.7798\n",
            "Epoch 540: val_loss improved from 0.70142 to 0.69988, saving model to Best_model_epoch_540_val_loss_0.6999.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6364 - acc: 0.7773 - val_loss: 0.6999 - val_acc: 0.7400\n",
            "Epoch 541/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6472 - acc: 0.7699\n",
            "Epoch 541: val_loss improved from 0.69988 to 0.69965, saving model to Best_model_epoch_541_val_loss_0.6997.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6358 - acc: 0.7760 - val_loss: 0.6997 - val_acc: 0.7400\n",
            "Epoch 542/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6246 - acc: 0.7827\n",
            "Epoch 542: val_loss improved from 0.69965 to 0.69963, saving model to Best_model_epoch_542_val_loss_0.6996.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6358 - acc: 0.7773 - val_loss: 0.6996 - val_acc: 0.7400\n",
            "Epoch 543/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6261 - acc: 0.8281\n",
            "Epoch 543: val_loss did not improve from 0.69963\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6349 - acc: 0.7773 - val_loss: 0.7011 - val_acc: 0.7400\n",
            "Epoch 544/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6508 - acc: 0.7656\n",
            "Epoch 544: val_loss did not improve from 0.69963\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6352 - acc: 0.7773 - val_loss: 0.7023 - val_acc: 0.7400\n",
            "Epoch 545/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5758 - acc: 0.7969\n",
            "Epoch 545: val_loss did not improve from 0.69963\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6349 - acc: 0.7760 - val_loss: 0.7011 - val_acc: 0.7400\n",
            "Epoch 546/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6271 - acc: 0.7784\n",
            "Epoch 546: val_loss did not improve from 0.69963\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6340 - acc: 0.7760 - val_loss: 0.7009 - val_acc: 0.7400\n",
            "Epoch 547/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6485 - acc: 0.7656\n",
            "Epoch 547: val_loss did not improve from 0.69963\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6343 - acc: 0.7693 - val_loss: 0.6996 - val_acc: 0.7440\n",
            "Epoch 548/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6359 - acc: 0.7770\n",
            "Epoch 548: val_loss did not improve from 0.69963\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6339 - acc: 0.7787 - val_loss: 0.7010 - val_acc: 0.7400\n",
            "Epoch 549/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6333 - acc: 0.7812\n",
            "Epoch 549: val_loss improved from 0.69963 to 0.69923, saving model to Best_model_epoch_549_val_loss_0.6992.keras\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6338 - acc: 0.7827 - val_loss: 0.6992 - val_acc: 0.7400\n",
            "Epoch 550/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6224 - acc: 0.7828\n",
            "Epoch 550: val_loss improved from 0.69923 to 0.69885, saving model to Best_model_epoch_550_val_loss_0.6989.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6336 - acc: 0.7773 - val_loss: 0.6989 - val_acc: 0.7440\n",
            "Epoch 551/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5615 - acc: 0.7812\n",
            "Epoch 551: val_loss did not improve from 0.69885\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6334 - acc: 0.7773 - val_loss: 0.6992 - val_acc: 0.7400\n",
            "Epoch 552/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6547 - acc: 0.7676\n",
            "Epoch 552: val_loss did not improve from 0.69885\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6328 - acc: 0.7787 - val_loss: 0.7016 - val_acc: 0.7440\n",
            "Epoch 553/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6291 - acc: 0.7770\n",
            "Epoch 553: val_loss did not improve from 0.69885\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6333 - acc: 0.7720 - val_loss: 0.7002 - val_acc: 0.7400\n",
            "Epoch 554/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.4767 - acc: 0.8438\n",
            "Epoch 554: val_loss did not improve from 0.69885\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6323 - acc: 0.7760 - val_loss: 0.6999 - val_acc: 0.7400\n",
            "Epoch 555/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6290 - acc: 0.7812\n",
            "Epoch 555: val_loss improved from 0.69885 to 0.69873, saving model to Best_model_epoch_555_val_loss_0.6987.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6325 - acc: 0.7800 - val_loss: 0.6987 - val_acc: 0.7400\n",
            "Epoch 556/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6329 - acc: 0.7800\n",
            "Epoch 556: val_loss did not improve from 0.69873\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6329 - acc: 0.7800 - val_loss: 0.7009 - val_acc: 0.7400\n",
            "Epoch 557/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6319 - acc: 0.7787\n",
            "Epoch 557: val_loss did not improve from 0.69873\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6319 - acc: 0.7787 - val_loss: 0.6988 - val_acc: 0.7400\n",
            "Epoch 558/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6331 - acc: 0.7784\n",
            "Epoch 558: val_loss did not improve from 0.69873\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6319 - acc: 0.7787 - val_loss: 0.6993 - val_acc: 0.7400\n",
            "Epoch 559/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7609 - acc: 0.7656\n",
            "Epoch 559: val_loss did not improve from 0.69873\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6317 - acc: 0.7747 - val_loss: 0.6996 - val_acc: 0.7360\n",
            "Epoch 560/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6362 - acc: 0.7784\n",
            "Epoch 560: val_loss improved from 0.69873 to 0.69706, saving model to Best_model_epoch_560_val_loss_0.6971.keras\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.6315 - acc: 0.7760 - val_loss: 0.6971 - val_acc: 0.7400\n",
            "Epoch 561/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6312 - acc: 0.7812\n",
            "Epoch 561: val_loss did not improve from 0.69706\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6313 - acc: 0.7813 - val_loss: 0.6990 - val_acc: 0.7440\n",
            "Epoch 562/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6286 - acc: 0.7812\n",
            "Epoch 562: val_loss did not improve from 0.69706\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6304 - acc: 0.7800 - val_loss: 0.7007 - val_acc: 0.7360\n",
            "Epoch 563/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6299 - acc: 0.7773\n",
            "Epoch 563: val_loss did not improve from 0.69706\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6299 - acc: 0.7773 - val_loss: 0.6987 - val_acc: 0.7400\n",
            "Epoch 564/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7221 - acc: 0.7500\n",
            "Epoch 564: val_loss did not improve from 0.69706\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6299 - acc: 0.7853 - val_loss: 0.6981 - val_acc: 0.7400\n",
            "Epoch 565/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3696 - acc: 0.8750\n",
            "Epoch 565: val_loss did not improve from 0.69706\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6304 - acc: 0.7760 - val_loss: 0.7010 - val_acc: 0.7360\n",
            "Epoch 566/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6248 - acc: 0.7798\n",
            "Epoch 566: val_loss did not improve from 0.69706\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6291 - acc: 0.7800 - val_loss: 0.6990 - val_acc: 0.7400\n",
            "Epoch 567/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6306 - acc: 0.7798\n",
            "Epoch 567: val_loss did not improve from 0.69706\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6299 - acc: 0.7800 - val_loss: 0.6976 - val_acc: 0.7400\n",
            "Epoch 568/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6208 - acc: 0.7656\n",
            "Epoch 568: val_loss improved from 0.69706 to 0.69650, saving model to Best_model_epoch_568_val_loss_0.6965.keras\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 0.6292 - acc: 0.7760 - val_loss: 0.6965 - val_acc: 0.7400\n",
            "Epoch 569/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6297 - acc: 0.7747\n",
            "Epoch 569: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6297 - acc: 0.7747 - val_loss: 0.6983 - val_acc: 0.7400\n",
            "Epoch 570/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6512 - acc: 0.7812\n",
            "Epoch 570: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6292 - acc: 0.7773 - val_loss: 0.6980 - val_acc: 0.7440\n",
            "Epoch 571/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6518 - acc: 0.7743\n",
            "Epoch 571: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6282 - acc: 0.7813 - val_loss: 0.6968 - val_acc: 0.7440\n",
            "Epoch 572/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6286 - acc: 0.7770\n",
            "Epoch 572: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6284 - acc: 0.7747 - val_loss: 0.6990 - val_acc: 0.7360\n",
            "Epoch 573/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6179 - acc: 0.7882\n",
            "Epoch 573: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6299 - acc: 0.7827 - val_loss: 0.6979 - val_acc: 0.7400\n",
            "Epoch 574/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6410 - acc: 0.7882\n",
            "Epoch 574: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6273 - acc: 0.7853 - val_loss: 0.6972 - val_acc: 0.7400\n",
            "Epoch 575/1000\n",
            " 7/12 [================>.............] - ETA: 0s - loss: 0.6118 - acc: 0.7924\n",
            "Epoch 575: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6273 - acc: 0.7760 - val_loss: 0.6977 - val_acc: 0.7400\n",
            "Epoch 576/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6464 - acc: 0.7756\n",
            "Epoch 576: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6263 - acc: 0.7840 - val_loss: 0.6979 - val_acc: 0.7400\n",
            "Epoch 577/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6121 - acc: 0.7917\n",
            "Epoch 577: val_loss did not improve from 0.69650\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6268 - acc: 0.7813 - val_loss: 0.6996 - val_acc: 0.7480\n",
            "Epoch 578/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6390 - acc: 0.7812\n",
            "Epoch 578: val_loss improved from 0.69650 to 0.69610, saving model to Best_model_epoch_578_val_loss_0.6961.keras\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.6266 - acc: 0.7813 - val_loss: 0.6961 - val_acc: 0.7400\n",
            "Epoch 579/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6421 - acc: 0.7734\n",
            "Epoch 579: val_loss improved from 0.69610 to 0.69568, saving model to Best_model_epoch_579_val_loss_0.6957.keras\n",
            "12/12 [==============================] - 0s 33ms/step - loss: 0.6262 - acc: 0.7787 - val_loss: 0.6957 - val_acc: 0.7440\n",
            "Epoch 580/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7794 - acc: 0.7188\n",
            "Epoch 580: val_loss did not improve from 0.69568\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6258 - acc: 0.7867 - val_loss: 0.6977 - val_acc: 0.7440\n",
            "Epoch 581/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6360 - acc: 0.7726\n",
            "Epoch 581: val_loss did not improve from 0.69568\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6255 - acc: 0.7787 - val_loss: 0.6975 - val_acc: 0.7440\n",
            "Epoch 582/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6214 - acc: 0.7812\n",
            "Epoch 582: val_loss did not improve from 0.69568\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6255 - acc: 0.7787 - val_loss: 0.6986 - val_acc: 0.7440\n",
            "Epoch 583/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.5874 - acc: 0.8016\n",
            "Epoch 583: val_loss did not improve from 0.69568\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6250 - acc: 0.7867 - val_loss: 0.6970 - val_acc: 0.7440\n",
            "Epoch 584/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6304 - acc: 0.7750\n",
            "Epoch 584: val_loss improved from 0.69568 to 0.69489, saving model to Best_model_epoch_584_val_loss_0.6949.keras\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 0.6252 - acc: 0.7800 - val_loss: 0.6949 - val_acc: 0.7480\n",
            "Epoch 585/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6304 - acc: 0.7841\n",
            "Epoch 585: val_loss did not improve from 0.69489\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6273 - acc: 0.7840 - val_loss: 0.6977 - val_acc: 0.7440\n",
            "Epoch 586/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6241 - acc: 0.7847\n",
            "Epoch 586: val_loss did not improve from 0.69489\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6235 - acc: 0.7827 - val_loss: 0.6959 - val_acc: 0.7440\n",
            "Epoch 587/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6129 - acc: 0.7827\n",
            "Epoch 587: val_loss did not improve from 0.69489\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6241 - acc: 0.7800 - val_loss: 0.6952 - val_acc: 0.7480\n",
            "Epoch 588/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6296 - acc: 0.7812\n",
            "Epoch 588: val_loss did not improve from 0.69489\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6243 - acc: 0.7840 - val_loss: 0.6956 - val_acc: 0.7480\n",
            "Epoch 589/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6332 - acc: 0.7847\n",
            "Epoch 589: val_loss did not improve from 0.69489\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6236 - acc: 0.7840 - val_loss: 0.6974 - val_acc: 0.7440\n",
            "Epoch 590/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6200 - acc: 0.7770\n",
            "Epoch 590: val_loss did not improve from 0.69489\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6231 - acc: 0.7813 - val_loss: 0.6963 - val_acc: 0.7480\n",
            "Epoch 591/1000\n",
            " 7/12 [================>.............] - ETA: 0s - loss: 0.6351 - acc: 0.7746\n",
            "Epoch 591: val_loss improved from 0.69489 to 0.69457, saving model to Best_model_epoch_591_val_loss_0.6946.keras\n",
            "12/12 [==============================] - 0s 37ms/step - loss: 0.6239 - acc: 0.7800 - val_loss: 0.6946 - val_acc: 0.7440\n",
            "Epoch 592/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6223 - acc: 0.7847\n",
            "Epoch 592: val_loss improved from 0.69457 to 0.69431, saving model to Best_model_epoch_592_val_loss_0.6943.keras\n",
            "12/12 [==============================] - 0s 34ms/step - loss: 0.6229 - acc: 0.7853 - val_loss: 0.6943 - val_acc: 0.7440\n",
            "Epoch 593/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6036 - acc: 0.7847\n",
            "Epoch 593: val_loss did not improve from 0.69431\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6221 - acc: 0.7867 - val_loss: 0.6962 - val_acc: 0.7440\n",
            "Epoch 594/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.6214 - acc: 0.7754\n",
            "Epoch 594: val_loss did not improve from 0.69431\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6225 - acc: 0.7813 - val_loss: 0.6974 - val_acc: 0.7440\n",
            "Epoch 595/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6229 - acc: 0.7813\n",
            "Epoch 595: val_loss did not improve from 0.69431\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6229 - acc: 0.7813 - val_loss: 0.6969 - val_acc: 0.7480\n",
            "Epoch 596/1000\n",
            " 8/12 [===================>..........] - ETA: 0s - loss: 0.5876 - acc: 0.7949\n",
            "Epoch 596: val_loss did not improve from 0.69431\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6222 - acc: 0.7840 - val_loss: 0.6963 - val_acc: 0.7480\n",
            "Epoch 597/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6158 - acc: 0.7922\n",
            "Epoch 597: val_loss did not improve from 0.69431\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6217 - acc: 0.7840 - val_loss: 0.6950 - val_acc: 0.7480\n",
            "Epoch 598/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6211 - acc: 0.7853\n",
            "Epoch 598: val_loss improved from 0.69431 to 0.69377, saving model to Best_model_epoch_598_val_loss_0.6938.keras\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.6211 - acc: 0.7853 - val_loss: 0.6938 - val_acc: 0.7520\n",
            "Epoch 599/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6218 - acc: 0.7853\n",
            "Epoch 599: val_loss did not improve from 0.69377\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6218 - acc: 0.7853 - val_loss: 0.6958 - val_acc: 0.7480\n",
            "Epoch 600/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6144 - acc: 0.7812\n",
            "Epoch 600: val_loss did not improve from 0.69377\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6207 - acc: 0.7840 - val_loss: 0.6950 - val_acc: 0.7480\n",
            "Epoch 601/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6157 - acc: 0.7884\n",
            "Epoch 601: val_loss did not improve from 0.69377\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6202 - acc: 0.7853 - val_loss: 0.6958 - val_acc: 0.7480\n",
            "Epoch 602/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6340 - acc: 0.7756\n",
            "Epoch 602: val_loss did not improve from 0.69377\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6201 - acc: 0.7787 - val_loss: 0.6967 - val_acc: 0.7480\n",
            "Epoch 603/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.5945 - acc: 0.7847\n",
            "Epoch 603: val_loss did not improve from 0.69377\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6200 - acc: 0.7800 - val_loss: 0.6963 - val_acc: 0.7480\n",
            "Epoch 604/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6039 - acc: 0.7844\n",
            "Epoch 604: val_loss did not improve from 0.69377\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6200 - acc: 0.7853 - val_loss: 0.6965 - val_acc: 0.7440\n",
            "Epoch 605/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6202 - acc: 0.7867\n",
            "Epoch 605: val_loss did not improve from 0.69377\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6202 - acc: 0.7867 - val_loss: 0.6938 - val_acc: 0.7520\n",
            "Epoch 606/1000\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.6422 - acc: 0.7847\n",
            "Epoch 606: val_loss improved from 0.69377 to 0.69277, saving model to Best_model_epoch_606_val_loss_0.6928.keras\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.6199 - acc: 0.7907 - val_loss: 0.6928 - val_acc: 0.7480\n",
            "Epoch 607/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6196 - acc: 0.7827\n",
            "Epoch 607: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6199 - acc: 0.7840 - val_loss: 0.6949 - val_acc: 0.7480\n",
            "Epoch 608/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6329 - acc: 0.7812\n",
            "Epoch 608: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6203 - acc: 0.7813 - val_loss: 0.6977 - val_acc: 0.7480\n",
            "Epoch 609/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6211 - acc: 0.7884\n",
            "Epoch 609: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6189 - acc: 0.7880 - val_loss: 0.6938 - val_acc: 0.7520\n",
            "Epoch 610/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6117 - acc: 0.7940\n",
            "Epoch 610: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6182 - acc: 0.7907 - val_loss: 0.6942 - val_acc: 0.7480\n",
            "Epoch 611/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6189 - acc: 0.7827\n",
            "Epoch 611: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6189 - acc: 0.7827 - val_loss: 0.6958 - val_acc: 0.7520\n",
            "Epoch 612/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6155 - acc: 0.7827\n",
            "Epoch 612: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6182 - acc: 0.7840 - val_loss: 0.6962 - val_acc: 0.7480\n",
            "Epoch 613/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6257 - acc: 0.7812\n",
            "Epoch 613: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6177 - acc: 0.7827 - val_loss: 0.6929 - val_acc: 0.7560\n",
            "Epoch 614/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6121 - acc: 0.7969\n",
            "Epoch 614: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6174 - acc: 0.7907 - val_loss: 0.6935 - val_acc: 0.7480\n",
            "Epoch 615/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6247 - acc: 0.7841\n",
            "Epoch 615: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6172 - acc: 0.7867 - val_loss: 0.6949 - val_acc: 0.7480\n",
            "Epoch 616/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6180 - acc: 0.7840\n",
            "Epoch 616: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6180 - acc: 0.7840 - val_loss: 0.6943 - val_acc: 0.7440\n",
            "Epoch 617/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5975 - acc: 0.7869\n",
            "Epoch 617: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6166 - acc: 0.7800 - val_loss: 0.6937 - val_acc: 0.7440\n",
            "Epoch 618/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6135 - acc: 0.7926\n",
            "Epoch 618: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6181 - acc: 0.7893 - val_loss: 0.6932 - val_acc: 0.7520\n",
            "Epoch 619/1000\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6249 - acc: 0.7884\n",
            "Epoch 619: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6159 - acc: 0.7880 - val_loss: 0.6938 - val_acc: 0.7440\n",
            "Epoch 620/1000\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.6370 - acc: 0.7688\n",
            "Epoch 620: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6163 - acc: 0.7813 - val_loss: 0.6941 - val_acc: 0.7520\n",
            "Epoch 621/1000\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6168 - acc: 0.7880\n",
            "Epoch 621: val_loss did not improve from 0.69277\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6168 - acc: 0.7880 - val_loss: 0.6940 - val_acc: 0.7440\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curve(history_wt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "PsPsg-A9gNza",
        "outputId": "f6f2cf31-ed30-4574-c608-bb869afc7ed7"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test = np.argmax(Fine_tunable_model.predict(X_test),axis=1)\n",
        "y_pred_train = np.argmax(Fine_tunable_model.predict(X_train),axis=1)\n",
        "\n",
        "Confusion_matrix(Y_train,y_pred_train, \"training\")\n",
        "Confusion_matrix(Y_test,y_pred_test, \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tzSyEHOQgQEn",
        "outputId": "709300b3-bef4-4afe-8aa2-12986976de24"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 2ms/step\n",
            "24/24 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the file\n",
        "files.download('Conf_training.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7frNFnNVpxJT",
        "outputId": "d989d3ad-ca26-4a03-f5c6-1679502d3c71"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2cb01785-faf6-4b9c-835c-66b5a2abf92b\", \"Conf_training.png\", 34346)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}